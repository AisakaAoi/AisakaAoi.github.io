<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="该论文发表于IEEE Journal of Translational Engineering in Health and Medicine (中科院二区，IF&#x3D;3.8)，题目为《Fusion of Multi-Task Neurophysiological Data to Enhance the Detection of AttentionDeficit&#x2F;Hyperact"><meta property="og:type" content="article"><meta property="og:title" content="IEEE JTEHM | 多任务神经生理数据融合以增强注意力缺陷多动障碍的检测"><meta property="og:url" content="https://aisakaaoi.github.io/6afbdec.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="该论文发表于IEEE Journal of Translational Engineering in Health and Medicine (中科院二区，IF&#x3D;3.8)，题目为《Fusion of Multi-Task Neurophysiological Data to Enhance the Detection of AttentionDeficit&#x2F;Hyperact"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/6afbdec/7.webp"><meta property="article:published_time" content="2025-07-27T22:47:55.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:57.753Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/6afbdec/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/6afbdec.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>IEEE JTEHM | 多任务神经生理数据融合以增强注意力缺陷多动障碍的检测 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1011</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/6afbdec.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">IEEE JTEHM | 多任务神经生理数据融合以增强注意力缺陷多动障碍的检测</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2025-07-28 06:47:55" itemprop="dateCreated datePublished" datetime="2025-07-28T06:47:55+08:00">2025-07-28</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/6afbdec.html" class="post-meta-item leancloud_visitors" data-flag-title="IEEE JTEHM | 多任务神经生理数据融合以增强注意力缺陷多动障碍的检测" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/6afbdec.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/6afbdec.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.6k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>11 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><img src="/6afbdec/1.webp"><p>该论文发表于IEEE Journal of Translational Engineering in Health and Medicine (中科院二区，IF&#x3D;3.8)，题目为《Fusion of Multi-Task Neurophysiological Data to Enhance the Detection of AttentionDeficit&#x2F;Hyperactivity Disorder》。</p><p>复旦大学附属儿科医院儿童保健科的张凯峰为此论文的第一作者，国立中央大学生物医学科学与工程系的陈春川为此论文的通讯作者。</p><p>论文链接：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10614196">https://ieeexplore.ieee.org/document/10614196</a></p><span id="more"></span><hr><h3 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h3><p>注意力缺陷&#x2F;多动障碍（ADHD）是一种常见的儿童期神经发育障碍，症状主要为注意力不集中、多动和冲动行为，可能对学习和社交产生长期负面影响。目前，ADHD 的诊断主要依赖于行为观察和主观评分，这种方法存在诊断延迟和误诊率高的问题。因此，开发一种客观、高效的早期诊断方法显得尤为重要。本文的核心内容是探索如何通过融合多任务神经生理数据来提高ADHD的检测准确率。研究招募了49名ADHD儿童和32名典型发育儿童，年龄在6至12岁之间。实验中使用虚拟现实（VR）技术，开发了一个模拟虚拟教室的环境，并设计了三种不同难度的任务（视觉CPT、听觉CPT和视觉+听觉CPT），以评估儿童在任务执行过程中的注意力表现。在任务执行过程中，同时采集了脑电图（EEG）、眼动（EM）、头部旋转（HR）以及任务表现（TP）数据。通过这些多任务设计，研究旨在更全面地评估儿童在不同条件下的大脑资源调动情况。研究使用了t检验对各个特征进行统计分析，以确定其在区分ADHD儿童和典型发育儿童中的重要性。然后，采用深度神经网络（DNN）方法对数据进行分类，并比较了单模态数据和多模态数据融合的分类效果。结果显示，多任务神经生理数据的融合显著提高了ADHD的检测准确率，表明通过结合多种神经生理数据，可以更全面地捕捉ADHD的特征，从而提高诊断的准确性。此外，研究还发现，在视觉CPT中，典型发育儿童在有无干扰的情况下任务表现相似，而ADHD儿童在有干扰时表现更差。这表明ADHD儿童在复杂任务中更容易超出其大脑资源的限制，不同任务类型对儿童注意力的影响存在差异。综上所述，本研究结果表明，多任务神经生理数据的融合可以为 ADHD 的早期诊断提供重要支持。</p><hr><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>早期诊断和治疗对于减轻 ADHD 症状、改善患者的生活质量具有关键作用。近年来，随着神经生理学和人工智能技术的发展，研究者开始探索利用多模态数据来辅助 ADHD 的诊断。这些方法通过分析儿童在特定任务中的神经生理反应，为诊断提供了更客观的依据。本研究旨在通过融合多任务神经生理数据，设计不同难度的任务，利用深度神经网络（DNN）模型，更全面地评估儿童在任务执行过程中的大脑资源调动情况，进一步提高 ADHD 的检测准确率，从而为 ADHD 的早期诊断提供更有力的支持。</p><hr><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>共有 81 名 6 至 12 岁的儿童参与了本研究。其中，49名被诊断为 ADHD患者（平均年龄 &#x3D; 7.4 ± 1.6；22 名男孩），32名为正常发育儿童（平均年龄 &#x3D; 8.0 ± 1.5；16 名男孩）。</p><p>本研究通过虚拟现实连续性能测试（CPT）任务，探究受试者的注意力表现。参与者佩戴HTC 头盔，在实验过程中持续注视前方黑板上出现的数字。这些数字范围从0到9，并以2秒的刺激间隔随机出现。参与者被指示在看到数字顺序出现时（即‘0’之后出现‘1’时）做出反应（GO条件），否则需要抑制反应（NOGO 条件）。GO 和 NOGO 刺激随机出现。此外，25 次试验（12次GO刺激和13次 NOGO 刺激）包括各种与任务无关的听觉和视觉干扰。出现干扰事件的实验被标记为D试验，而没有任何干扰的试验则被标记为ND试验。在本研究中，根据干扰类型的不同，主要包括了视觉CPT、听觉CPT和视觉+听觉CPT三种任务形式。</p><p>受试者使用 HTC Vive TM 控制器完成任务。除了测试任务外，HTC Vive TM 头盔还提供有关受试者头部旋转（HR）和眼动（EM）的信息。此外，六个 EEG 通道（Af3、Af4、Af7、Af8、Fp1 和 Fp2）与 HTC Vive 集成，并以 512Hz 的采样率记录任务期间的脑电活动。</p><hr><h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><p>（1）特征提取</p><p>任务表现（TP）：在TP数据中，将主动准确率、被动准确率和反应时间作为特征。实验中存在有三种 CPT 任务和三种条件（干扰、无干扰和整体平均）。</p><p>脑电图（EEG）：首先对 EEG 数据进行带通滤波（2-56 Hz），以去除电源线干扰和头部运动伪迹。将处理后的 EEG 数据通过快速傅里叶变换转换到频域，然后将每个通道的频谱密度按预定义的五个频率带（δ（1-3 Hz）、θ（4-7 Hz）、α（8-14 Hz）、β（15-24 Hz）和 γ（25-48 Hz））进行平均。对每个频率带，计算每个通道在相应条件下的最大值、最小值、平均值和方差。</p><p>眼动（EM）：使用 HTC Vive TM 内置的眼动追踪设备收集眼动数据。该系统对数据进行预处理，并报告诸如注视点坐标、瞳孔位置、瞳孔直径以及左眼和右眼的开合信息等数据。这些数据作为眼动（EM）特征。</p><p>头部旋转（HR）：头部旋转幅度被认为是评估受试者注意力的一个重要特征。任务期间的 HR 由 HTC Vive TM 计算。</p><p>（2）统计分析</p><p>对提取的 TP、EEG、EM 和 HR 特征数据使用 t 检验，分析各个特征在区分 ADHD 儿童和典型发育儿童中的显著性，筛选出在统计上显著的特征，减少特征维度，提高后续分类模型的效率和准确性。提取的剩余特征数量分别为 TP 27 个、EEG 180 个、EM 99 个和 HR 9 个（总共 315 个特征）。</p><p>（3）深度神经网络模型</p><p>采用深度神经网络（DNN）方法进行 ADHD 评估分类。DNN 模型的基本结构包括三个连接层：输入层、隐藏层和输出层（见图 1）。为了适应不同数据集的复杂性，作者系统地调整了模态特定 DNN 模型的超参数，以实现最佳分类准确率，包括隐藏层的数量和神经元的数量。设计递减的节点旨在减少特征抽象和复杂性，同时保留关键特征。所有隐藏层均使用ReLU激活函数。对于所有输出层，由于其适用于二分类问题，因此使用了 Sigmoid 激活函数。具体来说，TP 数据集使用了 2 个隐藏层，神经元数量分别为 128 和 32。HR数据集有 3 个隐藏层，神经元数量分别为 64、32 和 8。EM数据集使用了7层神经网络结构，神经元数量分别为 1024、512、256、128、32、16 和 4。对于 EEG 数据集，首先，通过处理一个包含 8 个隐藏层的 DNN 模型（神经元数量分别为 512、512、256、128、128、32、16 和 4）来测试每个通道的重要性。然后，所有 6 个通道的EEG数据作为一个整体输入到一个包含 6 个隐藏层的 DNN 模型（神经元数量分别为 1024、1024、512、512、256 和 128）。</p><img src="/6afbdec/2.webp"><div align="center">图1. DNN模型的架构</div><p>（4）融合模型架构</p><p>在获得了不同模态的特征后，实现了两种融合模型：早期融合和晚期融合（见图 2）。在早期融合中，EEG、EM 和 HR 被合并为一个特征集，用于训练预测模型。由于数据集值的范围不同，使用了 min-max 归一化方法对特征进行重新缩放（见图 2 左侧）。在晚期融合中，所有不同的特征分别使用 DNN 模型进行处理，以获得重要的 DNN 模型参数。最后隐藏层的数据特定加权因子被提取作为特征表示。这些提取的特征被合并为融合模型的输入，用于训练（见图 2 右侧）。</p><img src="/6afbdec/3.webp"><div align="center">图2. 基于DNN的早期（左）和晚期（右）融合模型的架构</div><hr><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>（1）任务表现</p><p>表 1 总结了两组的基于任务的表现。首先，检查了干扰对组内影响，并将显著值用灰色突出显示。对于视觉 CPT，在典型发育儿童中，D 试验和 ND 试验的任务表现没有差异，但在 ADHD 儿童中，D 试验的遗漏错误显著高于 ND 试验。对于听觉 CPT，当存在干扰时，两组的正确检测率显著降低，遗漏错误显著增加。关于视觉和听觉 CPT，干扰仅在典型发育儿童中降低了任务表现（正确检测率、反应时间和遗漏错误），而在 ADHD 儿童中，D 试验和 ND 试验的任务表现相似。在视觉和听觉 CPT 中，ADHD 儿童的反应时间在两组之间存在显著差异。此外，在无干扰试验和有干扰试验中，所有三种 CPT 的正确检测率、遗漏错误和错误反应在两组之间均存在显著差异。</p><img src="/6afbdec/4.webp"><div align="center">表1. 行为结果任务表现</div><p>（2）个体特征的统计检验</p><p>使用 t 检验对每个个体特征的组间差异进行统计检验，表 2 总结了结果。可以看到，在经过多重比较校正后，TP、EM、HR 和 EEG 中分别有 22、39、8 和 34 个特征被统计识别为重要特征。TP、EM、HR 和 EEG 中重要特征与总特征的比例分别为 61%、40%、89% 和 3.1%。</p><img src="/6afbdec/5.webp"><div align="center">表2. 模态特异性显著特征</div><p>（3）单模态数据集的分类结果</p><p>表 3 列出了使用不同分类器的模态特定的分类结果。通过 DNN 使用 TP 或 HR 数据可以获得最佳性能，其次是使用 HR 的 LGB。当使用 XGB 时，使用 FP1 EEG 数据可以获得最佳准确率 76%。使用 HR 数据时，SVM 的最佳性能为 74%。此外，与其他 EEG 通道相比，FP1 在 4 个分类器中的 3 个中表现优于其他通道。因此，选择 FP1 作为进一步研究的代表性通道。</p><img src="/6afbdec/6.webp"><div align="center">表3. 每个数据集的分类结果</div><p>（4）融合模型的分类结果</p><p>为了验证多任务神经生理数据的融合是否可以提高分离率，使用 DNN 测试了不同模型的组合。表 4 列出了融合结果。当使用 2 个、3 个和 4 个神经生理数据集进行融合时，融合模型的准确率分别为 83%、84% 和 89%。为了检验更多特征是否总是导致更高的准确率，作者测试了融合所有可能特征（即所有通道的 EEG 特征）的情况，准确率下降到了 75%（见表 4）。</p><img src="/6afbdec/7.webp"><div align="center">表4. 融合模型的分类结果</div><hr><h3 id="思考与总结"><a href="#思考与总结" class="headerlink" title="思考与总结"></a>思考与总结</h3><p>本文开发了一个基于 DNN 架构，使用多任务神经生理数据（EEG、EM、HR 和 TP）的融合模型，以增强对健ADHD患者的检测。在本研究中，设计了三种不同难度（视觉干扰、听觉干扰、视觉+听觉干扰）的任务，以操纵受试者在任务表现期间所调动的大脑资源水平。分析结果证实，多任务神经生理数据的融合，可以将 ADHD 检测准确率提高到 89%，而单一数据集的最高准确率仅为 81%。研究结果表明，来自多种任务的不同神经生理模型可以提供有助于 ADHD 筛查的重要信息，提高识别ADHD 患者的准确率。总之，基于 DNN 的多任务融合模型，为 ADHD 患者的早期临床诊断和管理提供了一种更高效、更准确的替代方法。</p><hr><h3 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=17209">https://www.scholat.com/teamwork/showPostMessage.html?id=17209</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/dc18036b.html" rel="prev" title="IEEE TNNLS | EEGMatch：基于半监督脑电的跨被试情感识别的不完整标签学习"><i class="fa fa-chevron-left"></i> IEEE TNNLS | EEGMatch：基于半监督脑电的跨被试情感识别的不完整标签学习</a></div><div class="post-nav-item"><a href="/97983d58.html" rel="next" title="CSIG云上微表情-第66期-微表情智能分析@IEEE T-PAMI">CSIG云上微表情-第66期-微表情智能分析@IEEE T-PAMI <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E6%A6%82%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">论文概要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="nav-number">2.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-number">3.</span> <span class="nav-text">数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">研究方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">5.</span> <span class="nav-text">实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E8%80%83%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">思考与总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87%E9%93%BE%E6%8E%A5"><span class="nav-number">7.</span> <span class="nav-text">原文链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1011</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">148:14</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>