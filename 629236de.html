<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇学习报告基于论文《Inverting Generative Adversarial Renderer for Face Reconstruction》。此论文提出一种新颖的生成对抗渲染器（Generative Adversarial Renderer，GAR），用于取代基于简单图形规则的可微分渲染器来完成三维人脸模型的重建任务。相关成果发表在2021 CVPR(Oral)。原文链接与代码地址见"><meta property="og:type" content="article"><meta property="og:title" content="学习报告：用于人脸重建的反向生成对抗渲染器"><meta property="og:url" content="https://aisakaaoi.top/629236de.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本篇学习报告基于论文《Inverting Generative Adversarial Renderer for Face Reconstruction》。此论文提出一种新颖的生成对抗渲染器（Generative Adversarial Renderer，GAR），用于取代基于简单图形规则的可微分渲染器来完成三维人脸模型的重建任务。相关成果发表在2021 CVPR(Oral)。原文链接与代码地址见"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.top/629236de/1.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/2.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/3.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/4.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/5.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/6.webp"><meta property="og:image" content="https://aisakaaoi.top/629236de/7.webp"><meta property="article:published_time" content="2021-05-27T10:14:24.000Z"><meta property="article:modified_time" content="2023-11-22T10:25:07.885Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.top/629236de/1.webp"><link rel="canonical" href="https://aisakaaoi.top/629236de.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>学习报告：用于人脸重建的反向生成对抗渲染器 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">53</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">688</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.top/629236de.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">学习报告：用于人脸重建的反向生成对抗渲染器</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-05-27 18:14:24" itemprop="dateCreated datePublished" datetime="2021-05-27T18:14:24+08:00">2021-05-27</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/629236de.html" class="post-meta-item leancloud_visitors" data-flag-title="学习报告：用于人脸重建的反向生成对抗渲染器" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/629236de.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/629236de.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.7k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇学习报告基于论文《Inverting Generative Adversarial Renderer for Face Reconstruction》。此论文提出一种新颖的<strong>生成对抗渲染器（Generative Adversarial Renderer，GAR）</strong>，用于取代基于简单图形规则的可微分渲染器来完成三维人脸模型的重建任务。相关成果发表在2021 CVPR(Oral)。原文链接与代码地址见文末。</p><img src="/629236de/1.webp"><div align="center">图1 论文截图</div><span id="more"></span><hr><h3 id="研究背景和内容"><a href="#研究背景和内容" class="headerlink" title="研究背景和内容"></a>研究背景和内容</h3><p>从无约束的单目人脸图像中准确恢复人脸的三维形状是一项具有挑战性的任务。目前最先进的三维人脸重建方法大致可以分为两大类：</p><ul><li>基于深度学习的方法<br>基于深度学习的方法通常采用回归的方式，将人脸图像作为输入，学习回归相应的三维形变模型（3D Morphable Models，3DMM）参数。然而，这些方法通常需要大量的标记数据，而正确标注的3DMM参数很难获取。</li><li>基于优化的方法<br>基于优化的方法一般将人脸的成像视为一个生成过程。它将一系列几何系数(如反照率、纹理、光照、视角等)作为输入与输出，按照一定的图形规则渲染图像。然后通过优化方法最小化渲染图像和目标图像之间的距离。然而，由于图形规则通常采用简化的模型来表征人脸图像采集的物理过程，导致成像过程中的许多细节无法建模，这给人脸重建的优化带来了困难。</li></ul><p>在最近的研究中，可微分渲染器[1]给两类方法都提供了一个有效的工具。在基于学习的方法中，回归的参数可以通过可微分渲染器渲染到二维图像中，然后添加光度损失进行优化。通过这种方式，可以在不需要输入图像标注的情况下训练模型。对于基于优化的方法，可微分渲染器引入了基于梯度的优化，允许使用更复杂的损失函数，并能够稳定训练过程。 然而，主要基于图形学基础的可微分渲染器简化了真实世界中光照、反射等真实机制，因而难以生成足够真实的图像，如图2的(b)、(c)所示。</p><img src="/629236de/2.webp"><div align="center">图2 不同渲染器的渲染效果对比</div><div align="center">（第二行是输入的几何图形，第一行是相应的渲染图像）</div><p>为了解决上述的问题，作者提出了一种基于对抗生成网络的自监督方式进行三维模型的渲染，以取代传统的基于图形的可微分渲染器，同时保持使用渲染器进行训练的优势。总的来说，该方法的主要贡献有三方面：</p><ol><li>第一个提出使用<strong>条件神经渲染器（conditional neural renderer）</strong>用于人脸重建，而不是基于图像的可微分渲染器。</li><li>提出的条件神经渲染器<strong>以人脸法线贴图和隐变量作为输入</strong>，然后<strong>产生生动逼真的人脸图像</strong>。</li><li>基于该渲染器提出了<strong>新的人脸重建算法</strong>，并在多个人脸数据集上实现了最先进的性能。</li></ol><hr><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="基于生成对抗的渲染器"><a href="#基于生成对抗的渲染器" class="headerlink" title="基于生成对抗的渲染器"></a>基于生成对抗的渲染器</h4><p>为了实现人脸几何参数受控的生成器，文章提出了基于<strong>StyleGan v2[2]<strong>的一系列渲染模块组成的</strong>生成式对抗渲染器（GAR）</strong>。这些基于三维人脸几何信息的渲染模块，在保持使用随机隐变量生成纹理的同时显式地加入了人脸的几何信息。</p><img src="/629236de/3.webp"><div align="center">图3 整体网络结构</div><p>具体而言，对于每个模块的4维输入特征（批次、通道数、长、宽），StyleGAN v2提出在通道数上进行风格注入，以调控生成的结果。而在长宽的图像维度上，文章提出同时注入与人脸几何相关的法线信息，以监督整张人脸在图像中的位置和五官的分布。该注入以乘积的方式添加，保持特征图在不同几何信息的位置具有针对性的响应。</p><p>为了规范化生成网络得到的结果以匹配输入条件，文章还提出了<strong>法线一致性损失（Normal Consistency Loss）</strong>（见图3(b)左侧）。利用几何学方法渲染一批合成人脸训练了一个人脸法线估计网络（SFSNet），然后使用该网络预测生成输出图像的人脸法线贴图作为弱监督保持法线的近似一致性。</p><p>另外，文章还提出为了控制生成图像的纹理信息和人脸的几何信息能够解耦，在训练过程中采用<strong>交换几何信息</strong>和<strong>注入变量信息</strong>的方式，并利用预训练的关键点检测保持人脸几何信息的一致性以及人脸识别网络提取的特征保持纹理信息的一致性。</p><img src="/629236de/4.webp"><div align="center">图4 不同隐变量渲染生成的图像</div><p>通过GAR生成图像的例子可以在图4中看到，这表明提出的方法比传统的基于图形的渲染器生成的人脸图像具有更高的视觉质量，甚至发型，眼镜和其他属性都能很好地生成。</p><hr><h4 id="基于反向渲染的人脸三维重建"><a href="#基于反向渲染的人脸三维重建" class="headerlink" title="基于反向渲染的人脸三维重建"></a>基于反向渲染的人脸三维重建</h4><p>GAR经过训练后，可以代替可微分渲染器进行基于优化的人脸三维重建任务。</p><p>在此，人脸重建的目标是给定一张自然环境下的人脸图像，通过优化3DMM的参数以及输入的隐变量和噪声重建其人脸几何结构。反向渲染技术可以充分利用GAR的优势，以生成的与给定的图像相似的图片为目标，反推出输出的人脸形状与纹理。</p><p>文章中设计了与生成器结构对称的反向网络，用于估计隐变量的初始优化值，以解决随机初始化导致基于梯度的优化可能会卡在损失函数的局部最小值的问题。反向网络生成的特征图与GAR对应层的特征图具有相同的空间大小。将各层统计的均值和标准差串联起来，然后进行最大似然估计（MLP），得到重构的隐变量。另外，为了获得良好的3DMM初始人脸参数，文章中采用了传统的基于二维人脸关键点的3DMM拟合算法。GAR反向网络和3DMM参数预求解方法为优化提供了良好的初始化，通过最小化渲染图像和输入图像之间的光度损失，可以实现对三维人脸形状的优化。</p><hr><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="人脸图像生成的评价"><a href="#人脸图像生成的评价" class="headerlink" title="人脸图像生成的评价"></a>人脸图像生成的评价</h4><p>为了定量分析生成的图像质量，文章中使用Progressive GAN[3]、StyleGAN[4]和提出的GAR随机生成50000张图像，并计算生成的图像与真实图像数据之间的FID(Frechet Inception Distance)分数。如表1所示，结果表明，提出的GAR在更多的条件约束下训练，并且网络的可控性得到了提升。</p><div align="center">表1 不同人脸生成方法的FID分数</div><img src="/629236de/5.webp"><hr><h4 id="人脸重建的评价"><a href="#人脸重建的评价" class="headerlink" title="人脸重建的评价"></a>人脸重建的评价</h4><ul><li>定性比较</li></ul><p>作者将提出的重建算法与几种最先进的方法在MoFA-Test数据集[5]上进行了定性比较，如图5所示。第2-5行是渲染的图像，可见文章提出的方法渲染的图像非常接近输入图像。第6-9行是重建的人脸网格，该重建方法的结果在形状和表情方面显然更准确，具有更多高保真的细节。</p><img src="/629236de/6.webp"><div align="center">图5 人脸重建效果对比</div><ul><li>定量比较</li></ul><p>为了定量评估该重建算法的性能，作者使用扫描的人脸来测试准确性。具体而言，使用Florence数据集[6]中每个视频的5帧，并将重建结果与真实标注的三维扫描网格进行比较。结果如表2所示，文章的方法在平均误差方面有较好的结果。</p><div align="center">表2 基于Florence数据集的点-面距离网格重建误差</div><img src="/629236de/7.webp"><hr><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>下面介绍针对所提出的人脸重建框架的不同组成部分的消融研究的结果。</p><p><strong>法线一致性损失（Normal Consistency Loss）的影响。</strong>如表2的第5行和第7行所示，当训练没有该损失的GAR(“Ours w&#x2F;o norm-cycle”)时，训练不能保证生成器在输入的法线贴图上处于良好的状态。这说明所提出的法线一致性损失对于提高输入法线贴图的可控性具有重要意义。</p><p><strong>渲染器反向网络初始化的效果。</strong>如表2的第6行和第7行所示，当隐变量没有被反向网络初始化(“our w&#x2F;o initial”)时，重建误差严重增加，并且对于特定的人脸可能不会收敛。</p><p><strong>条件的选择。</strong>以往的工作倾向于采用3DMM参数或深度作为条件输入。然而，作者发现以人脸法线贴图作为条件对提出的GAR是一种更有效的形式。以人脸法线贴图作为输入，可以更稳定、更快地减少损失，并能更好地收敛到最优状态。</p><p><strong>法线信息注入模块的作用。</strong>与简单地将法线贴图和特征映射连接相比，该方法在进一步减小损失方面是有效的，说明了该方法的有效性。</p><hr><h3 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h3><p>文章提出了一个新颖的生成对抗渲染器（GAR）与一种反向渲染网络以实现人脸重建。GAR比基于传统图形学的渲染器效果更加真实，大大减小了渲染图像与真实图像之间的差异。而基于GAR的反向网络以生成与给定图像相似的图像为目标，反推出人脸的形状与纹理进而完成三维人脸的重建。GAR不仅解决了基于图形学的可微分渲染器存在的本质问题，而且达到了很好的效果，这为我们解决三维人脸重建问题提供了新的思路。</p><hr><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=9918">https://www.scholat.com/teamwork/showPostMessage.html?id=9918</a><br>[1] Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, et al. Unsupervised training for 3D morphable model regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8377–8386, 2018.<br>[2] Tero Karras, Samuli Laine, Miika Aittala, et al. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020.<br>[3] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.<br>[4] Tero Karras, Samuli Laine, Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4401–4410, 2019.<br>[5] Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, et al. Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 1274–1283, 2017.<br>[6] Andrew D Bagdanov, Alberto Del Bimbo, Iacopo Masi. The florence 2d&#x2F;3d hybrid face dataset. In Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding, pages 79–80. ACM, 2011.</p></blockquote><p>原文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.02431">https://arxiv.org/abs/2105.02431</a><br>Github地址：<a target="_blank" rel="noopener" href="https://github.com/WestlyPark/StyleRenderer">https://github.com/WestlyPark/StyleRenderer</a><br>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/370573907">https://zhuanlan.zhihu.com/p/370573907</a></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/c5453de8.html" rel="prev" title="《头文字D》里最经典的十款赛车"><i class="fa fa-chevron-left"></i> 《头文字D》里最经典的十款赛车</a></div><div class="post-nav-item"><a href="/2d26f426.html" rel="next" title="跟李沐学AI-动手学深度学习 PyTorch版-26 网络中的网络 NiN">跟李沐学AI-动手学深度学习 PyTorch版-26 网络中的网络 NiN <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E5%92%8C%E5%86%85%E5%AE%B9"><span class="nav-number">1.</span> <span class="nav-text">研究背景和内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%9A%84%E6%B8%B2%E6%9F%93%E5%99%A8"><span class="nav-number">2.1.</span> <span class="nav-text">基于生成对抗的渲染器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8F%8D%E5%90%91%E6%B8%B2%E6%9F%93%E7%9A%84%E4%BA%BA%E8%84%B8%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA"><span class="nav-number">2.2.</span> <span class="nav-text">基于反向渲染的人脸三维重建</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">3.</span> <span class="nav-text">实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E8%84%B8%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E8%AF%84%E4%BB%B7"><span class="nav-number">3.1.</span> <span class="nav-text">人脸图像生成的评价</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E8%84%B8%E9%87%8D%E5%BB%BA%E7%9A%84%E8%AF%84%E4%BB%B7"><span class="nav-number">3.2.</span> <span class="nav-text">人脸重建的评价</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.3.</span> <span class="nav-text">消融实验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83"><span class="nav-number">4.</span> <span class="nav-text">总结与思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">5.</span> <span class="nav-text">参考链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">688</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">53</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">96:57</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>