<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇学习报告基于ACL2021（Annual Meeting of the Association for Computational Linguistics， CCF A类会议）的论文：《ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learni"><meta property="og:type" content="article"><meta property="og:title" content="ERICA:通过对比学习提高预训练语言模型的实体和关系理解"><meta property="og:url" content="https://aisakaaoi.github.io/a312c491.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本篇学习报告基于ACL2021（Annual Meeting of the Association for Computational Linguistics， CCF A类会议）的论文：《ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learni"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a312c491/8.webp"><meta property="article:published_time" content="2021-08-20T12:43:40.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:36.725Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/a312c491/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/a312c491.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>ERICA:通过对比学习提高预训练语言模型的实体和关系理解 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1006</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/a312c491.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ERICA:通过对比学习提高预训练语言模型的实体和关系理解</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-08-20 20:43:40" itemprop="dateCreated datePublished" datetime="2021-08-20T20:43:40+08:00">2021-08-20</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/a312c491.html" class="post-meta-item leancloud_visitors" data-flag-title="ERICA:通过对比学习提高预训练语言模型的实体和关系理解" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/a312c491.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/a312c491.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.9k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇学习报告基于ACL2021（Annual Meeting of the Association for Computational Linguistics， CCF A类会议）的论文：《ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning》，由清华大学与腾讯Wechat AI平台合作研究。</p><span id="more"></span><hr><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>近几年，预训练语言模型(Pre-trained Language Model，简称PLM) 在自然语言处理领域的各种下游任务上表现得地极为出色，如文本分类、命名实体识别和智能问答。PLM可以在各种自监督学习方法中有效地捕捉文本中的语法和语义，为下游自然语言处理任务生成信息语言表示。然而，传统的预训练目标并没有针对相关事实去建立模型的，而这些事实往往分布在文本中且对理解整个文本至关重要。为了解决这个问题，最近的一些研究试图通过改进PLM以更好地理解实体之间的关系。但是这些方法主要是关注孤立的句内关系，这不仅忽略了对实体本身的理解，也忽略了在文档级中多个实体之间的相互作用。这些实体之间的关系理解涉及复杂的推理模式，有研究证明至少40.7%的关系事实是需要从多个句子中才能提取到的。图1所示的例子是要识别“瓜达拉哈在哪里？”的答案，我们可以很容易就可以发现，答案是无法从独立的一个句子中得到的，而是要在整篇文档中才能推理出与“瓜达拉哈”相关的国家。从这个例子我们可以看到想要捕获文本内的关系事实目前面临着两个主要挑战：（1）要想理解一个实体，要综合考虑它和其他实体的关系；（2）要理解一个关系，就要考虑在文本中复杂的推理模式。</p><img src="/a312c491/1.webp"><div align="center">图1: 在文档级中发现并突出重要的实体和关系，以找出“瓜达拉哈拉在哪里”的答案</div><p>在这篇论文中，作者提出了一个新的对比学习框架ERICA来提高PLM对实体和关系的理解能力，旨在更好地捕捉文本中实体和关系事实。具体来说，本篇文章在PLM上主要做出了以下两个贡献：（1)实体识别任务，通过给定的头部实体及其关系来区分哪些尾部实体可以被推断出来。它通过考虑文本中每个实体与其他实体的关系来提高对每个实体的理解；（2)关系判别任务，从语义上区分两种关系是否相近。通过构建具有文档级远程监督的实体对，隐式地考虑了复杂的关系推理链，从而提高了对关系理解。我们在一套语言理解任务上进行了实验，包括关系抽取、实体分类和问答。实验结果表明，ERICA提高了典型PLM (BERT和RoBERTa等)的性能，并优于基线，特别是在低资源设置下，这表明ERICA有效地提高了PLM对实体和关系的理解，并捕获了文本内的关系事实。</p><hr><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="实体与关系表示（Entity-amp-Relation-Representation）"><a href="#实体与关系表示（Entity-amp-Relation-Representation）" class="headerlink" title="实体与关系表示（Entity &amp; Relation Representation）"></a>实体与关系表示（Entity &amp; Relation Representation）</h4><p>作者提出的ERICA是利用外部知识图谱在一个大规模无标记语料库上进行训练的，ERICA对一个文档中的所有实体都枚举成一组实体对。如果实体对在外部知识图谱中本身存在关系，则顺理成章生成三元组（正例），反之两个实体之间的关系被赋值为“no_relation”（反例）。ERICA使用一个PLM对每个文档进行编码生成对应的隐藏态，再经过平均池化机制去获得局部实体表示。由于一个实体在一个文档是可以出现多次的，作者就把这多次的表示求平均以得到该实体在文档中的全局表示，紧接着把最终的全局实体表示进行向链接来得到它们的关系表示。</p><hr><h4 id="实体识别任务（Entity-Discrimination-Task）"><a href="#实体识别任务（Entity-Discrimination-Task）" class="headerlink" title="实体识别任务（Entity Discrimination Task）"></a>实体识别任务（Entity Discrimination Task）</h4><p>两个实体之间的关系可以用一个三元组表示，即（头实体，关系，尾实体）。实体识别任务的目标是要求在给定的头实体和关系的情况下去推断出文档中所存在的尾实体。PLM在这部分的实质工作是根据已知的正例头实体和关系的约束，将尾部实体与其他不相干的实体区分开来。论文在该部分的创新点在于使用了对比学习的方法使得PLM去考虑实体之间的关系来进行区分，而不再是像传统的方法一样单纯地将后验概率最大化。对比式学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。与生成式学习（如常见的对抗网络）相比，对比式学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强。</p><img src="/a312c491/2.webp"><div align="center">图2: 实体识别任务示例。对于文本中具有远监督关系的实体对，实体识别任务要求真实尾实体比其他实体更接近头实体</div><hr><h4 id="关系识别任务（Relation-Discrimination-Task）"><a href="#关系识别任务（Relation-Discrimination-Task）" class="headerlink" title="关系识别任务（Relation Discrimination Task）"></a>关系识别任务（Relation Discrimination Task）</h4><p>关系识别任务旨在通过语义上的判别来区分两种关系是否相近。与现有的PLM关系增强方法相比，论文的创新点在于使用了文档级的远程监督方法而不再是一般的独立句子，这进一步地帮助PLM理解现实场景中复杂的推理链，从而提高PLM对关系的理解能力。在该部分，文章训练一个基于文本的关系表示的预训练语言模型，它将正例且具有相同关系的实体对文本进行训练，使它们在语义上相近。值得注意的是，这部分的工作中也使用到对比学习的方法去区分正例关系和反例关系。</p><img src="/a312c491/3.webp"><div align="center">图3: 关系识别任务示例。对于属于相同关系的实体对，关系识别任务要求它们的关系表示更接近</div><hr><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>在实验部分论文包含了构建远程监督的数据集构建，ERICA在自然语言处理的下游任务（关系抽取、实体分类、问答）上的表现。论文提出的ERICA是为了提升传统PLM的效果，因此论文设计了ERICA在BERT和RoBERT两个经典的PLM上做了实验。</p><h4 id="远程监督数据集构建"><a href="#远程监督数据集构建" class="headerlink" title="远程监督数据集构建"></a>远程监督数据集构建</h4><p>论文中利用英语维基百科和Wikidata来构建了远程监督的预训练数据集。首先使用spaCy来进行命名实体识别，然后将这些识别出来的实体且在维基百科也出现的实体与Wikidata条目的超链接链接起来，从而获得每个实体的Wikidata ID。这里提到的spaCy是一种python的开源库，可以完成大部分nlp的相关人物。紧接着，通过查询Wikidata，可以对不同实体之间的关系进行远程标注。论文约束每个文档都至少包含128个单词、4个实体和4个关系三元组。最后，论文共收集了100万个文档(约1G存储空间)，其中有4000多个远程标注的关系。平均而言，每个文档包含186.9个token、12.9个实体和7.2个关系三元组，一个实体在每个文档中出现1.3次。基于人工对数据集随机抽样的评估，命名实体识别的F1-score为84.7%，关系提取的F1-score为25.4%。</p><hr><h4 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h4><p>关系抽取的目的是从预定义的关系集中提取两个被识别实体之间的关系，文章在文档级和句子级的训练集上分别进行了实验，同时还把训练集所占原数据集的比例做了1%,10%,100%三种划分。表1展示了在句子级上的实验结果。句子级上的关系抽取是在两个广泛使用的数据集:TACRED和SemEval-2010 Task 8上，数据集被插入额外的标记以指明每个句子中的头实体和尾实体。BERT[2]、RoBERTa[3]和MTBandCP被作为baseline方法，从结果来看，我们可以发现ERICA与CP在句子级关系抽取任务上取得了几乎相当的结果，这意味着在文档级的预训练ERICA不影响PLM在句子层面里在关系理解上的表现。</p><div align="center">表1：ERICA在句子级关系抽取的micro-F1表现</div><img src="/a312c491/4.webp"><p>文档级的关系抽取，在该部分的评价指标使用了micro-F1和micro-ignore-F1，其中micro-ignore-F1是指忽略了训练集和测试集之间共享关系事实下的评估。对于文档级关系抽取论文选择了DocRED[1]的方法，它需要阅读文档中的多个句子以综合所有信息来识别两个实体之间的关系，CNN、BILSTM、BERT、HINBERT和CorefBERT被作为baseline方法。从表2的结果可以看出:(1)ERICA在每个数据量上都显著优于所有基线，说明ERICA在训练前通过隐式考虑文档中实体的复杂推理模式，能够更好地理解文档中实体之间的关系;(2)MTB[4]、CP[5]两种方法的效果都不如BERT，这意味着句子级的预训练缺乏对复杂推理模式的考虑，在一定程度上影响了PLM在文档级RE任务中的表现;(3)ERICA在较小的训练集上比baseline表现出更大的优势，这意味着ERICA在对比学习中获得了良好的文档级关系推理能力，从而在低资源设置下获得改进。</p><div align="center">表2：ERICA在文档级关系抽取的表现</div><img src="/a312c491/5.webp"><hr><h4 id="实体分类"><a href="#实体分类" class="headerlink" title="实体分类"></a>实体分类</h4><p>实体分类的目的是将提取出来的实体分类为预定义的实体类型。论文在FIGER这个带有远程监督标记的句子级实体类型数据集进行测试。选取BERT、RoBERTa、MTB、CP、ERNIE作为基线。从表3的结果中我们可以看出，ERICA的表现优于所有baseline，说明ERICA无论是通过实体级还是关系级对比学习，都能更好地表示实体并区分实体。</p><div align="center">表3：ERICA在实体分类上的表现</div><img src="/a312c491/6.webp"><hr><h4 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h4><p>问答的目的是在给定问题的文本中提取出一个特定的回答范围。该部分选择了在Multi-choice QA和Extractive QA两种模式上进行实验。我们测试训练集的多个分区。</p><p>Multi-choice QA要求模型在阅读多个文档并进行多跳推理后回答实体的特定属性。首先，将问题和文档连接成一个长序列，然后找出文档中出现的所有实体将它们进行编码，编码后应用平均池来获得全局实体表示。最后，在实体表示的基础上使用分类器进行预测。FastQA[6]、BiDAF[7]两个广泛的问答系统被作为baseline，从表4所列的结果中，我们可以观察到ERICA在两种设置下都优于基线，这表明ERICA可以更好地理解文档中的实体及其关系，并根据查询提取真实答案。掩蔽设置的显著改进也表明ERICA可以更好地执行多跳推理，从上下文合成和分析信息。</p><div align="center">表4：ERICA在Multi-choice QA上的表现</div><img src="/a312c491/7.webp"><p>Extractive QA采用了三个广泛使用的数据集:SQuAD、TriviaQA和NaturalQA来评估各个领域的ERICA。该部分实验从数据集中设置测试集和训练集并遵循BERT的QA设置，即把给定的问题和段落连接成一个长序列，通过PLM对序列进行编码并采用两个分类器来预测答案的开始和结束索引。从表5所列的结果中，我们可以观察到ERICA优于所有基线，这表明通过实体和关系理解的增强ERICA更能够捕获文本中的关系事实和合成实体信息。</p><div align="center">表5：ERICA在Extractive QA上的表现</div><img src="/a312c491/8.webp"><hr><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在这篇文章中，我们提出了一个通用的PLM框架ERICA，这个框架通过对比学习提高实体和关系的理解。论文还证明了ERICA在几个自然语言处理任务中的有效性，包括关系抽取，实体类型和问题回答。实验结果表明，ERICA优于所有基线，特别是在低资源设置下，这意味着ERICA可以帮助PLM更好地捕获文本中的关系事实，并合成实体及其关系的信息。</p><hr><h3 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h3><ol><li>对比学习是自监督的一种学习方式，也就是意味着它会减少对标注数据的依赖。本篇论文使用的对比学习方法是在基于开源数据集维基百科上进行训练的，也就是说拥有较为充足的已标注的数据来对对比学习的结果进行评估。但是在垂直领域上（例如医疗、教育等）往往缺乏专业标注数据，这就会导致无法对学习结果进行评估。虽然PLM已经被广泛应用，但是在我们使用在特定领域数据上时，就不得不去考虑有关领域迁移的相关影响。</li><li>在远程监督数据构建的工作中，论文明确提到构建的数据的评估结果是由人工评估的。如果在现实资源比较欠缺的情况下，这也会消耗大量的人力。但是对于已拥有行业的外部知识图谱作为指导的情况下，那么该部分的工作量将会有一定的减少。</li><li>本篇论文可以帮助我们实现在文档级的文本中进行实体识别和关系推理，但是论文方法目前是面向英文文本实现的，如果想要面向中文文本识别我们就要考虑输入的变化，同时也可以增加针对中文的基于文字特征方法来作为实体识别创新点进行切入，也可以引入中文的逻辑推理方法来对关系识别来进行改进或者创新。</li></ol><hr><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=10315">https://www.scholat.com/teamwork/showPostMessage.html?id=10315</a><br>[1]Y uan Y ao, Deming Y e, Peng Li,and et al. 2019.DocRED: A large-scale document-level relation extraction dataset. InProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, V olume 1: Long Papers, pages 764–777.<br>[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee, ,and et al.2018.BERT: Pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, V olume 1 (Long and Short Papers), pages 4171–4186.<br>[3]Yinhan Liu, Myle Ott, Naman Goyal,and et al. 2019.RoBERTa: A robustly optimized BERT pretraining approach.CoRR, abs&#x2F;1907.11692.<br>[4]Michael McCloskey and Neal J Cohen. 1989.Catastrophic interference in connectionist networks: the sequential learning problem. InPsychology of learning and motivation, volume 24, pages 109–165. Elsevier.<br>[5]Hao Peng, Tianyu Gao, Xu Han,and et al. 2020.Learning from context or names? an empirical study on neural relation extraction. InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3661–3672,Online. Association for Computational Linguistics.<br>[6]Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017.Making neural QA as simple as possible but not simpler. InProceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 271–280. Association for Computational Linguistics.<br>[7]Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi,and et al. 2016.Bidirectional attention flow for machine comprehension. InProceedings of 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24, 2017,Conference Track Proceedings.</p></blockquote><p>论文下载地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.15022">https://arxiv.org/abs/2012.15022</a><br>论文源码地址：<a target="_blank" rel="noopener" href="https://github.com/thunlp/ERICA">https://github.com/thunlp/ERICA</a></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/f1832120.html" rel="prev" title="Android-RecyclerView-缓存复用与数据更新"><i class="fa fa-chevron-left"></i> Android-RecyclerView-缓存复用与数据更新</a></div><div class="post-nav-item"><a href="/612fee56.html" rel="next" title="SFFAI 119 | 事件关系抽取专题《左新宇：知识融合的事件因果关系数据增强方法》">SFFAI 119 | 事件关系抽取专题《左新宇：知识融合的事件因果关系数据增强方法》 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">背景介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BD%93%E4%B8%8E%E5%85%B3%E7%B3%BB%E8%A1%A8%E7%A4%BA%EF%BC%88Entity-amp-Relation-Representation%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">实体与关系表示（Entity &amp; Relation Representation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%EF%BC%88Entity-Discrimination-Task%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">实体识别任务（Entity Discrimination Task）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E7%B3%BB%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%EF%BC%88Relation-Discrimination-Task%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">关系识别任务（Relation Discrimination Task）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="nav-number">3.1.</span> <span class="nav-text">远程监督数据集构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="nav-number">3.2.</span> <span class="nav-text">关系抽取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BD%93%E5%88%86%E7%B1%BB"><span class="nav-number">3.3.</span> <span class="nav-text">实体分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E7%AD%94"><span class="nav-number">3.4.</span> <span class="nav-text">问答</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">4.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">总结与思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">参考链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1006</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">145:57</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>