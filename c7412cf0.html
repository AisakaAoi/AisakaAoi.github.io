<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="导读视觉注意力机制是人类视觉所特有的一种大脑信号处理机制，而深度学习中的注意力机制正是借鉴了人类视觉的注意力思维方式。一般来说，人类在观察外界环境时会迅速的扫描全景，然后根据大脑信号的处理快速的锁定重点关注的目标区域，最终形成**注意力焦点[1]**。该机制可以帮助人类在有限的资源下，从大量无关背景区域中筛选出具有重要价值信息的目标区域，帮助人类更加高效的处理视觉信息。"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-一文看尽深度学习中的各种注意力机制"><meta property="og:url" content="https://aisakaaoi.top/c7412cf0.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="导读视觉注意力机制是人类视觉所特有的一种大脑信号处理机制，而深度学习中的注意力机制正是借鉴了人类视觉的注意力思维方式。一般来说，人类在观察外界环境时会迅速的扫描全景，然后根据大脑信号的处理快速的锁定重点关注的目标区域，最终形成**注意力焦点[1]**。该机制可以帮助人类在有限的资源下，从大量无关背景区域中筛选出具有重要价值信息的目标区域，帮助人类更加高效的处理视觉信息。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/1.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/2.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/3.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/4.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/5.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/6.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/7.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/8.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/9.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/10.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/11.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/12.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/13.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/14.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/15.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/16.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/17.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/18.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/19.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/20.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/21.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/22.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/23.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/24.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/25.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/26.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/27.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/28.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/29.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/30.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/31.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/32.webp"><meta property="og:image" content="https://aisakaaoi.top/c7412cf0/33.webp"><meta property="article:published_time" content="2021-06-09T08:29:41.000Z"><meta property="article:modified_time" content="2023-12-28T03:14:53.725Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.top/c7412cf0/1.webp"><link rel="canonical" href="https://aisakaaoi.top/c7412cf0.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度学习-一文看尽深度学习中的各种注意力机制 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">50</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">840</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.top/c7412cf0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习-一文看尽深度学习中的各种注意力机制</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-06-09 16:29:41" itemprop="dateCreated datePublished" datetime="2021-06-09T16:29:41+08:00">2021-06-09</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/%F0%9F%92%AB-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-Deep-Learning-Basic-Concepts/" itemprop="url" rel="index"><span itemprop="name">💫_深度学习基本概念 Deep Learning Basic Concepts</span></a> </span></span><span id="/c7412cf0.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-一文看尽深度学习中的各种注意力机制" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/c7412cf0.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/c7412cf0.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>20k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>49 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h3><p>视觉注意力机制是人类视觉所特有的一种大脑信号处理机制，而深度学习中的注意力机制正是借鉴了人类视觉的注意力思维方式。一般来说，人类在观察外界环境时会迅速的扫描全景，然后根据大脑信号的处理快速的锁定重点关注的目标区域，最终形成**注意力焦点[1]**。该机制可以帮助人类在有限的资源下，从大量无关背景区域中筛选出具有重要价值信息的目标区域，帮助人类更加高效的处理视觉信息。</p><img src="/c7412cf0/1.webp"> <span id="more"></span><hr><h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p>注意力机制在计算机视觉领域的应用主要使用于捕捉图像上的respective field，而在自然语言处理领域中的应用主要使用于定位关键的token。下面简单介绍下注意力机制在早期的几个经典应用。</p><img src="/c7412cf0/2.webp"><hr><h4 id="《A-Model-of-Saliency-Based-Visual-Attention-for-Rapid-Scene-Analysis》-2"><a href="#《A-Model-of-Saliency-Based-Visual-Attention-for-Rapid-Scene-Analysis》-2" class="headerlink" title="《A Model of Saliency-Based Visual Attention for Rapid Scene Analysis》[2]"></a>《A Model of Saliency-Based Visual Attention for Rapid Scene Analysis》[2]</h4><img src="/c7412cf0/3.webp"><p>这是早期将注意力机制应用于计算机视觉领域的一篇代表作，文章于1998年发表于TAPMI。作者受早期灵长目视觉系统的神经元结构启发，提出了一种视觉注意力系统，可以将多尺度的图像特征组合成单一的显著性图。最后，利用一个动态神经网络，并按照显著性的顺序来高效的选择重点区域。</p><hr><h4 id="《Recurrent-Models-of-Visual-Attention》-3"><a href="#《Recurrent-Models-of-Visual-Attention》-3" class="headerlink" title="《Recurrent Models of Visual Attention》[3]"></a>《Recurrent Models of Visual Attention》[3]</h4><img src="/c7412cf0/4.webp"><p>使注意力机制真正火起来的当属于谷歌DeepMind于2014年所提出的这篇文章，该论文首次在RNN模型上应用了注意力机制的方法进行图像分类。</p><hr><h4 id="《Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate》-4"><a href="#《Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate》-4" class="headerlink" title="《Neural Machine Translation by Jointly Learning to Align and Translate》[4]"></a>《Neural Machine Translation by Jointly Learning to Align and Translate》[4]</h4><img src="/c7412cf0/5.webp"><p>这是由深度学习三巨头之一Yoshua Bengio等人于2015年发表于ICLR上的一篇论文，该论文的最大贡献是将注意力机制首次应用到NLP领域，实现了同步的对齐和翻译，解决以往神经机器翻译(NMT)领域使用Encoder-Decoder架构的一个潜在问题，即将信息都压缩在固定长度的向量，无法对应长句子。</p><hr><h4 id="《Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention》-5"><a href="#《Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention》-5" class="headerlink" title="《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》[5]"></a>《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》[5]</h4><img src="/c7412cf0/6.webp"><p>这篇文章由Yoshua Bengio等人于2015年在ICML上所发表的，该论文将注意力机制引入到图像领域，作者提出了两种基于注意力机制的图像描述生成模型: 使用基本反向传播训练的Soft Attetnion方法和使用强化学习训练的Hard Attention方法。</p><hr><h4 id="《Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition》-6"><a href="#《Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition》-6" class="headerlink" title="《Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition》[6]"></a>《Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition》[6]</h4><img src="/c7412cf0/7.webp"><p>这是发表于CVPR 2017年的一篇文章，作者提出了一种基于CNN的注意力机制，叫做循环注意力卷积神经网络（Recurrent Attention Convolutional Neural Network, RA-CANN），该网络可以递归地分析局部信息，并从所获取的局部区域中提取细粒度信息。此外，作者还引入了一个注意力生成子网络（Attenion Proposal Sub-Network, APN），迭代的对整图操作以生成对应的子区域，最后再将各个子区域的预测记过整合起来，从而后的整张图片最终的分类预测结果。</p><hr><h4 id="《Attention-is-All-Your-Need》-7"><a href="#《Attention-is-All-Your-Need》-7" class="headerlink" title="《Attention is All Your Need》[7]"></a>《Attention is All Your Need》[7]</h4><img src="/c7412cf0/8.webp"><p>这是由谷歌机器翻译团队于2017年发表于NIPS上的一篇文章，该论文最大的贡献便是抛弃了以往机器翻译基本都会应用的RNN或CNN等传统架构，以编码器-解码器为基础，创新性的提出了一种Transformer架构。该架构可以有效的解决RNN无法并行处理以及CNN无法高效的捕捉长距离依赖的问题，近期更是被进一步地应用到了计算机视觉领域，同时在多个CV任务上取得了SOTA性能，挑战CNN在CV领域多年的霸主地位。</p><hr><h3 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h3><p>本文将重点围绕<strong>通道、空间、自注意力、类别</strong>等多个**维度[8]**介绍计算机视觉领域中较为出名的注意力机制方法,力争用最简短的语言解释得更加通俗易懂。</p><hr><h4 id="通道-amp-空间注意力"><a href="#通道-amp-空间注意力" class="headerlink" title="通道&amp;空间注意力"></a>通道&amp;空间注意力</h4><blockquote><p><strong>通道注意力</strong>旨在显示的建模出不同通道之间的相关性，通过网络学习的方式来自动获取到每个特征通道的重要程度，最后再为每个通道赋予不同的权重系数，从而来强化重要的特征抑制非重要的特征。<br><strong>空间注意力</strong>旨在提升关键区域的特征表达，本质上是将原始图片中的空间信息通过空间转换模块，变换到另一个空间中并保留关键信息，为每个位置生成权重掩膜（mask）并加权输出，从而增强感兴趣的特定目标区域同时弱化不相关的背景区域。</p></blockquote><h5 id="SE-Net-9"><a href="#SE-Net-9" class="headerlink" title="SE-Net[9]"></a>SE-Net[9]</h5><p>《Squeeze-and-Excitation Networks》发表于CVPR 2018，是CV领域将注意力机制应用到通道维度的代表作，后续大量基于通道域的工作均是基于此进行润(魔)色(改)。SE-Net是ImageNet 2017大规模图像分类任务的冠军，结构简单且效果显著，可以通过特征重标定的方式来自适应地调整通道之间的特征响应。</p><img src="/c7412cf0/9.webp"><ul><li><strong>Squeeze</strong> 利用全局平均池化(Global Average Pooling, GAP) 操作来提取全局感受野，将所有特征通道都抽象为一个点；</li><li><strong>Excitation</strong> 利用两层的多层感知机(Multi-Layer Perceptron, MLP) 网络来进行非线性的特征变换，显示地构建特征图之间的相关性；</li><li><strong>Transform</strong> 利用Sigmoid激活函数实现特征重标定，强化重要特征图，弱化非重要特征图。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SELayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br></pre></td></tr></table></figure><h5 id="GE-Net-10"><a href="#GE-Net-10" class="headerlink" title="GE-Net[10]"></a>GE-Net[10]</h5><p>《Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks》发表于NIPS 2018，从上下文建模的角度出发，提出了一种比SE-Net更一般的形式。GE-Net充分利用空间注意力来更好的挖掘特征之间的上下文信息。【代码链接可访问**github[11]**】</p><img src="/c7412cf0/10.webp"><ul><li><strong>Gather</strong> 用于从局部的空间位置上提取特征；</li><li><strong>Excite</strong> 用于将特征缩放至原始尺寸。</li></ul><h5 id="RA-Net-12"><a href="#RA-Net-12" class="headerlink" title="RA-Net[12]"></a>RA-Net[12]</h5><p>《Residual attention network for image classification》发表于CVPR 2017，利用下采样和上采样操作提出了一种基于空间注意力机制的残差注意力网络。</p><img src="/c7412cf0/11.webp"><p>以往的Attention模型大多应用于图像分割和显著性检测任务，出发点在于将注意力集中在部分感兴趣区域或显著区域上。本文尝试在常规的分类网络中引入侧边分支，该分支同样是由一系列卷积和池化操作来逐渐地提取高级语义特征并增大网络的感受野，最后再将该分支直接上采样为原始分辨率尺寸作为特征激活图叠加回原始输入。</p><img src="/c7412cf0/12.webp"><p>该方法提升效果好像并不明显，而且由于引入大量额外的参数，导致计算开销非常大。</p><h5 id="SK-Net-13"><a href="#SK-Net-13" class="headerlink" title="SK-Net[13]"></a>SK-Net[13]</h5><p>《Selective Kernel Networks》发表于CVPR 2019，原SE-Net的作者Momenta也参与到这篇文章中。SK-Net主要灵感来源于Inception-Net的多分支结构以及SE-Net的特征重标定策略，研究的是卷积核之间的相关性，并进一步地提出了一种选择性卷积核模块。SK-Net从多尺度特征表征的角度出发，引入多个带有不同感受野的并行卷积核分支来学习不同尺度下的特征图权重，使网络能够挑选出更加合适的多尺度特征表示，不仅解决了SE-Net中单一尺度的问题，而且也结合了多分枝结构的思想从丰富的语义信息中筛选出重要的特征。</p><img src="/c7412cf0/13.webp"><ul><li><strong>Split</strong> 采用不同感受野大小的卷积核捕获多尺度的语义信息；</li><li><strong>Fuse</strong> 融合多尺度语义信息，增强特征多样性；</li><li><strong>Select</strong> 在不同向量空间（代表不同尺度的特征信息）中进行Softmax操作，为合适的尺度通道赋予更高的权重。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SKConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, M=<span class="number">2</span>, G=<span class="number">32</span>, r=<span class="number">16</span>, stride=<span class="number">1</span>, L=<span class="number">32</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; Constructor</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            features: input channel dimensionality.</span></span><br><span class="line"><span class="string">            M: the number of branchs.</span></span><br><span class="line"><span class="string">            G: num of convolution groups.</span></span><br><span class="line"><span class="string">            r: the ratio for compute d, the length of z.</span></span><br><span class="line"><span class="string">            stride: stride, default 1.</span></span><br><span class="line"><span class="string">            L: the minimum dim of the vector z in paper, default 32.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SKConv, self).__init__()</span><br><span class="line">        d = <span class="built_in">max</span>(<span class="built_in">int</span>(features/r), L)</span><br><span class="line">        self.M = M</span><br><span class="line">        self.features = features</span><br><span class="line">        self.convs = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            self.convs.append(nn.Sequential(</span><br><span class="line">                nn.Conv2d(features, features, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>+i, dilation=<span class="number">1</span>+i, groups=G, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(features),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line">            ))</span><br><span class="line">        self.gap = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Sequential(nn.Conv2d(features, d, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                nn.BatchNorm2d(d),</span><br><span class="line">                                nn.ReLU(inplace=<span class="literal">False</span>))</span><br><span class="line">        self.fcs = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            self.fcs.append(</span><br><span class="line">                 nn.Conv2d(d, features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">            )</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        </span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        feats = [conv(x) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]      </span><br><span class="line">        feats = torch.cat(feats, dim=<span class="number">1</span>)</span><br><span class="line">        feats = feats.view(batch_size, self.M, self.features, feats.shape[<span class="number">2</span>], feats.shape[<span class="number">3</span>])</span><br><span class="line">        </span><br><span class="line">        feats_U = torch.<span class="built_in">sum</span>(feats, dim=<span class="number">1</span>)</span><br><span class="line">        feats_S = self.gap(feats_U)</span><br><span class="line">        feats_Z = self.fc(feats_S)</span><br><span class="line"></span><br><span class="line">        attention_vectors = [fc(feats_Z) <span class="keyword">for</span> fc <span class="keyword">in</span> self.fcs]</span><br><span class="line">        attention_vectors = torch.cat(attention_vectors, dim=<span class="number">1</span>)</span><br><span class="line">        attention_vectors = attention_vectors.view(batch_size, self.M, self.features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        attention_vectors = self.softmax(attention_vectors)</span><br><span class="line">        </span><br><span class="line">        feats_V = torch.<span class="built_in">sum</span>(feats*attention_vectors, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> feats_V</span><br></pre></td></tr></table></figure><h5 id="SPA-Net-14"><a href="#SPA-Net-14" class="headerlink" title="SPA-Net[14]"></a>SPA-Net[14]</h5><p>《Spatial Pyramid Attention Network for Enhanced Image Recognition》 发表于ICME 2020 并获得了最佳学生论文。考虑到 SE-Net 这种利用 GAP 去建模全局上下文的方式会导致空间信息的损失，SPA-Net另辟蹊径，利用多个自适应平均池化(Adaptive Averatge Pooling, APP) 组成的空间金字塔结构来建模局部和全局的上下文语义信息，使得空间语义信息被更加充分的利用到。</p><img src="/c7412cf0/14.webp"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CPSPPSELayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channel, channel, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CPSPPSELayer, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> in_channel != channel:</span><br><span class="line">            self.conv1 = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channel, channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">        self.avg_pool1 = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.avg_pool2 = nn.AdaptiveAvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.avg_pool4 = nn.AdaptiveAvgPool2d(<span class="number">4</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel*<span class="number">21</span>, channel*<span class="number">21</span> // reduction, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(channel*<span class="number">21</span> // reduction, channel, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x) <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;conv1&#x27;</span>) <span class="keyword">else</span> x</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y1 = self.avg_pool1(x).view(b, c)  <span class="comment"># like resize() in numpy</span></span><br><span class="line">        y2 = self.avg_pool2(x).view(b, <span class="number">4</span> * c)</span><br><span class="line">        y3 = self.avg_pool4(x).view(b, <span class="number">16</span> * c)</span><br><span class="line">        y = torch.cat((y1, y2, y3), <span class="number">1</span>)</span><br><span class="line">        y = self.fc(y)</span><br><span class="line">        b, out_channel = y.size()</span><br><span class="line">        y = y.view(b, out_channel, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h5 id="ECA-Net-15"><a href="#ECA-Net-15" class="headerlink" title="ECA-Net[15]"></a>ECA-Net[15]</h5><p>《ECANet：Efficient Channel Attention for Deep Convolutional Neural Networks》发表于CVPR 2020，是对SE-Net中特征变换部分进行了改进。SE-Net的通道信息交互方式是通过全连接实现的，在降维和升维的过程中会损害一部分的特征表达。ECA-Net则进一步地利用一维卷积来实现通道间的信息交互，相对于全连接实现的全局通道信息交互所带来的计算开销，ECA-Net提出了一种基于自适应选择卷积核大小的方法，以实现局部交互，从而显著地降低模型复杂度且保持性能。</p><img src="/c7412cf0/15.webp"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ECALayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a ECA module.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        channel: Number of channels of the input feature map</span></span><br><span class="line"><span class="string">        k_size: Adaptive selection of kernel size</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, k_size=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ECALayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=k_size, padding=(k_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>) </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># feature descriptor on the global spatial information</span></span><br><span class="line">        y = self.avg_pool(x)</span><br><span class="line">        <span class="comment"># Two different branches of ECA module</span></span><br><span class="line">        y = self.conv(y.squeeze(-<span class="number">1</span>).transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Multi-scale information fusion</span></span><br><span class="line">        y = self.sigmoid(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br></pre></td></tr></table></figure><hr><h4 id="混合注意力"><a href="#混合注意力" class="headerlink" title="混合注意力"></a>混合注意力</h4><blockquote><p>空间注意力由于将每个通道中的特征都做同等处理，忽略了通道间的信息交互；<br>通道注意力则是将一个通道内的信息直接进行全局处理，容易忽略空间内的信息交互；<br><strong>混合注意力</strong>主要是共同结合了通道域、空间域等注意力的形式来形成一种更加综合的特征注意力方法。</p></blockquote><h5 id="CBAM-16"><a href="#CBAM-16" class="headerlink" title="CBAM[16]"></a>CBAM[16]</h5><p>《CBAM: Convolutional Block Attention Module》发表于CVPR 2018，在原有通道注意力的基础上，衔接了一个空间注意力模块(Spatial Attention Modul, SAM)。SAM是基于通道进行全局平均池化以及全局最大池化操作，产生两个代表不同信息的特征图，合并后再通过一个感受野较大的7×7卷积进行特征融合，最后再通过Sigmoid操作来生成权重图叠加回原始的输入特征图，从而使得目标区域得以增强。总的来说，对于空间注意力来说，由于将每个通道中的特征都做同等处理，忽略了通道间的信息交互；而通道注意力则是将一个通道内的信息直接进行全局处理，容易忽略空间内的信息交互。</p><img src="/c7412cf0/16.webp"><p>作者最终通过实验验证先通道后空间的方式比先空间后通道或者通道空间并行的方式效果更佳。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CBAM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gate_channels, reduction_ratio=<span class="number">16</span>, pool_types=[<span class="string">&#x27;avg&#x27;</span>, <span class="string">&#x27;max&#x27;</span>], no_spatial=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CBAM, self).__init__()</span><br><span class="line">        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)</span><br><span class="line">        self.no_spatial=no_spatial</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> no_spatial:</span><br><span class="line">            self.SpatialGate = SpatialGate()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_out = self.ChannelGate(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.no_spatial:</span><br><span class="line">            x_out = self.SpatialGate(x_out)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br></pre></td></tr></table></figure><h5 id="BAM-17"><a href="#BAM-17" class="headerlink" title="BAM[17]"></a>BAM[17]</h5><p>《BAM: Bottleneck Attention Module》发表于BMC 2018，提出了一个简单有效的注意力模型来获取空间和通道的注意力图。</p><img src="/c7412cf0/17.webp"><p>BAM形成了一种分层的注意力机制，可以有效地抑制背景特征，使模型更加聚焦于前景特征，从而加强高级语义，实现更高的性能。</p><img src="/c7412cf0/18.webp"><p>不同于CBAM并联的方式，BAM以串联的方式来相继提取不同域的注意力图。</p><h5 id="scSE-18"><a href="#scSE-18" class="headerlink" title="scSE[18]"></a>scSE[18]</h5><p>《Concurrent Spatial and Channel Squeeze &amp; Excitation in Fully Convolutional Networks》发表于MICCAI 2018，是一种更轻量化的SE-Net变体，在SE的基础上提出cSE、sSE、scSE这三个变种。cSE和sSE分别是根据通道和空间的重要性来校准采样。scSE则是同时进行两种不同采样校准，得到一个更优异的结果。</p><img src="/c7412cf0/19.webp"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SCSEModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ch, re=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.cSE = nn.Sequential(nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">                                 nn.Conv2d(ch,ch//re,<span class="number">1</span>),</span><br><span class="line">                                 nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                 nn.Conv2d(ch//re,ch,<span class="number">1</span>),</span><br><span class="line">                                 nn.Sigmoid())</span><br><span class="line">        self.sSE = nn.Sequential(nn.Conv2d(ch,ch,<span class="number">1</span>),</span><br><span class="line">                                 nn.Sigmoid())</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * self.cSE(x) + x * self.sSE(x)</span><br></pre></td></tr></table></figure><h5 id="A2-Nets-19"><a href="#A2-Nets-19" class="headerlink" title="A2-Nets[19]"></a>A2-Nets[19]</h5><p>《A2-Nets: Double Attention Networks》发表于NIPS 2018，提出了一种双重注意力网络。该网络首先使用二阶的注意力池化(Second-order Attention Pooling, SAP) 用于将整幅图的所有关键特征归纳到一个集合当中，然后再利用另一种注意力机制将这些特征分别应用到图像中的每个区域。</p><img src="/c7412cf0/20.webp"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleAtten</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A2-Nets: Double Attention Networks. NIPS 2018</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_c</span>):</span><br><span class="line">        <span class="built_in">super</span>(DoubleAtten,self).__init__()</span><br><span class="line">        self.in_c = in_c</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Convolve the same input feature map to produce three feature maps with the same scale, i.e., A, B, V (as shown in paper).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.convA = nn.Conv2d(in_c,in_c,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.convB = nn.Conv2d(in_c,in_c,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.convV = nn.Conv2d(in_c,in_c,kernel_size=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line"></span><br><span class="line">        feature_maps = self.convA(<span class="built_in">input</span>)</span><br><span class="line">        atten_map = self.convB(<span class="built_in">input</span>)</span><br><span class="line">        b, _, h, w = feature_maps.shape</span><br><span class="line"></span><br><span class="line">        feature_maps = feature_maps.view(b, <span class="number">1</span>, self.in_c, h*w) <span class="comment"># reshape A</span></span><br><span class="line">        atten_map = atten_map.view(b, self.in_c, <span class="number">1</span>, h*w)       <span class="comment"># reshape B to generate attention map</span></span><br><span class="line">        global_descriptors = torch.mean((feature_maps * F.softmax(atten_map, dim=-<span class="number">1</span>)),dim=-<span class="number">1</span>) <span class="comment"># Multiply the feature map and the attention weight map to generate a global feature descriptor</span></span><br><span class="line"></span><br><span class="line">        v = self.convV(<span class="built_in">input</span>)</span><br><span class="line">        atten_vectors = F.softmax(v.view(b, self.in_c, h*w), dim=-<span class="number">1</span>) <span class="comment"># 生成 attention_vectors</span></span><br><span class="line">        out = torch.bmm(atten_vectors.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>), global_descriptors).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out.view(b, _, h, w)</span><br></pre></td></tr></table></figure><hr><h4 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h4><blockquote><p>自注意力是注意力机制的一种变体，其目的是为了减少对外部信息的依赖，尽可能地利用特征内部固有的信息进行注意力的交互。</p></blockquote><h5 id="Non-Local-20"><a href="#Non-Local-20" class="headerlink" title="Non-Local[20]"></a>Non-Local[20]</h5><p>《Non-local Neural Networks》发表于CVPR 2018，是第一篇将自注意力机制引入图像领域的文章。文中提出了经典的Non-Local模块，通过Self-Attention机制对全局上下午进行建模，有效地捕获长距离的特征依赖。后续许多基于自注意力的方法都是根据Non-Local来改进的。【代码链接可访问**github[21]**】</p><img src="/c7412cf0/21.webp"><p>自注意力流程一般是通过将原始特征图映射为三个向量分支，即Query、Key和Value。</p><ul><li>首先，计算Q和K的相关性权重矩阵系数；</li><li>其次，通过软操作对权重矩阵进行归一化；</li><li>最后，再将权重系数叠加到V上，以实现全局上下文信息的建模。</li></ul><h5 id="DA-Net-22"><a href="#DA-Net-22" class="headerlink" title="DA-Net[22]"></a>DA-Net[22]</h5><p>《DA-Net：Dual Attention Network for Scene Segmentation》发表于CVPR 2019，该论文将Non-local的思想同时引入到了通道域和空间域，分别将空间像素点以及通道特征作为查询语句进行上下文建模，自适应地整合局部特征和全局依赖。【代码链接可访问**github[23]**】</p><img src="/c7412cf0/22.webp"><ul><li>Position Attention Module</li></ul><img src="/c7412cf0/23.webp"><blockquote><p>PAM将更广泛的上下文信息编码为局部特征，从而提高了它们的代表性。</p></blockquote><ul><li>Channel Attention Module</li></ul><img src="/c7412cf0/24.webp"><blockquote><p>CAM通过挖掘通道图之间的相互依赖关系，可以强调相互依赖的特征图，改进特定语义的特征表示。</p></blockquote><h5 id="ANLNet-24"><a href="#ANLNet-24" class="headerlink" title="ANLNet[24]"></a>ANLNet[24]</h5><p>《ANLNet：Asymmetric Non-local Neural Networks for Semantic Segmentation》发表于ICCV 2019，是基于Non-Local的思路往轻量化方向做改进。Non-Local模块是一种效果显著的技术，但同时也受限于过大计算量而难以很好地嵌入网络中应用。为了解决以上问题，ANLNet基于Non-Local结构并融入了金字塔采样模块，在充分考虑了长距离依赖的前提下，融入了不同层次的特征，从而在保持性能的同时极大地减少计算量。【代码链接可访问**github[25]**】</p><img src="/c7412cf0/25.webp"><ul><li>结合SPP和Non-Local，前者从不同大小区域内提取出关键点，后者对这些关键点建模长距离依赖；</li><li>Non-Local 需要计算每一个像素点，可以看作是点对点建模；ANL-Net只计算通过SPP提取出的关键点，可以看作是点对区域建模；</li><li><strong>AFNB</strong>包含两个输入，一个高级特征图，一个低级特征图，分别对应图中的Stage5和Stage4。其中高级特征图作为Query，低级特征图作为Key和Value；</li><li><strong>APNB</strong>的建模方式与AFNB类似。</li></ul><h5 id="CC-Net-26"><a href="#CC-Net-26" class="headerlink" title="CC-Net[26]"></a>CC-Net[26]</h5><p>《CCNet：Criss-Cross Attention for Semantic Segmentation》发表于ICCV 2019，同样是同Non-Local轻量化的另一种尝试。【代码链接可访问**github[27]**】</p><img src="/c7412cf0/26.webp"><p>与Non-Local的每个查询点需要与其它所有点计算相关性的建模方式不同，CC-Net仅计算该查询点与其在水平和垂直方向上的点相关性，并且重复计算两次就可以获取该点的全局依赖，极大减少计算量。</p><img src="/c7412cf0/27.webp"><ul><li>CC-Net基于Non-Local的思路并利用横纵交叉方式完成对全局上下文建模。时间复杂度从 O(HW * HW) 变为 O(HW * (H+W-1))，降低了1~2个数量级;</li><li>和ANL-Net一样，CC-Net也是点对区域建模一次建模只能获取当前查询点上同一行和一列的信息，二次建模就可以捕捉全局信息;</li><li><strong>Loop1</strong>：右上方像素点（蓝色）只能捕捉水平（左上）和垂直（右下）方向的像素点信息，但无法捕捉左下角像素点（绿色）信息；</li><li><strong>Loop2</strong>：右上方像素点（蓝色）再次捕捉左上和右下的像素点信息时，由于在Loop1左上和右下像素点已经捕捉了左下的像素点信息，此时右上像素点就可以间接捕捉到左下像素点信息。</li></ul><h5 id="GC-Net-28"><a href="#GC-Net-28" class="headerlink" title="GC-Net[28]"></a>GC-Net[28]</h5><p>《GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond》发表于ICCV 2019，受SE-Net和Non-local思想的启发提出了一种更简化的空间自注意力模块。Non-local采用Self-attention机制来建模全局的像素对关系，建模长距离依赖，但这种基于全局像素点(pixel-to-pixel) 对的建模方式其计算量无疑是巨大的。SE-Net则利用GAP和MLP完成通道之间的特征重标定，虽然轻量，但未能充分利用到全局上下文信息。因此，作者提出了GC-Net可以高效的建模全局的上下文信息。【代码链接可访问**github[29]**】</p><img src="/c7412cf0/28.webp"><hr><h4 id="类别注意力"><a href="#类别注意力" class="headerlink" title="类别注意力"></a>类别注意力</h4><h5 id="OCR-Net-30"><a href="#OCR-Net-30" class="headerlink" title="OCR-Net[30]"></a>OCR-Net[30]</h5><p>《Object-Contextual Representations for SemanticSegmentation》发表于ECCV 2020，是一种基于自注意力对类别信息进行建模的方法。与先前的自注意力对全局上下文建模的角度（通道和空间）不同，OCR-Net是从类别的角度进行建模，其利用粗分割的结果作为建模的对象，最后加权到每一个查询点，这是一种轻量并有效的方法。【代码链接可访问**github[31]**】</p><img src="/c7412cf0/29.webp"><ul><li><strong>Soft Object Regions</strong> 对Backbone倒数第二层所输出的粗分割结果进行监督;</li><li><strong>Object Region Representations</strong> 融合粗分割和Backbone网络最后一层所输出的高级语义特征图生成对象区域语义，每一条向量代表不同的类别信息;</li><li><strong>Pixel-Region Relations</strong> 结合最后一层的高级语义特征图以及对象区域语义信息，建模像素与对象区域之间的相关性;</li><li><strong>Object Contextual Representations</strong> 将像素-对象区域相关性加权到对象区域信息中，完成加权目标类别信息到每一个像素上; 不难发现，这种类别信息的建模方式是完全遵循自注意力机制（Q，K，V）的。</li></ul><hr><h4 id="时间注意力"><a href="#时间注意力" class="headerlink" title="时间注意力"></a>时间注意力</h4><h5 id="IAU-Net-32"><a href="#IAU-Net-32" class="headerlink" title="IAU-Net[32]"></a>IAU-Net[32]</h5><p>《IAUnet: Global Context-Aware Feature Learning forPerson Re-Identification》发表于 IEEE Trans. on Neural Networks and Learning Systems，将自注意力机制的方法扩展到时间维度并应用于行人充识别任务，有效的解决了大多数基于卷积神经网络的方法无法充分对空间-时间上下文进行建模的弊端。【代码链接可访问**github[33]**】</p><img src="/c7412cf0/30.webp"><ul><li>交互聚合模块（Interaction-Aggregation-Update, IAU）同时包含全局空间，时间和频道上下文信息，可用于高性能的reID；</li><li>空间-时间IAU（Spatial-Temporal IAU, STIAU）可有效地融合两种类型的上下文依赖；</li><li>通道IAU（Channel IAU, CIAU）模块旨在模拟信道特征之间的语义上下文交互，以增强特征表示，尤其是对于小型视觉线索和身体部位。</li></ul><hr><h4 id="频率注意力"><a href="#频率注意力" class="headerlink" title="频率注意力"></a>频率注意力</h4><h5 id="Fca-Net-34"><a href="#Fca-Net-34" class="headerlink" title="Fca-Net[34]"></a>Fca-Net[34]</h5><p>《FcaNet: Frequency Channel Attention Networks》截止至目前还未被接收，作者从频域角度切入，证明了GAP是DCT的特例，弥补了现有通道注意力方法中特征信息不足的缺点，将GAP推广到一种更为一般的表示形式，即二维的离散余弦变换(Discrete Cosine Transform, DCT)，并提出了多光谱通道注意力Fca-Net, 通过引入更多的频率分量来充分的利用信息。【代码链接可访问**github[35]**】</p><img src="/c7412cf0/31.webp"><p>通过探讨使用不同数量的频率分量及其不同组合的影响，提出了选择频率分量的两步准则:</p><ul><li>分别计算出通道注意力中每个频率分量的结果；</li><li>根据所得结果筛选出Top-k个性能最佳的频率分量。</li></ul><hr><h4 id="全局注意力"><a href="#全局注意力" class="headerlink" title="全局注意力"></a>全局注意力</h4><h5 id="RGA-Net-36"><a href="#RGA-Net-36" class="headerlink" title="RGA-Net[36]"></a>RGA-Net[36]</h5><p>《Relation-Aware Global Attention for Person Re-identification》发表于CVPR 2020，作者针对行人重识别任务提出了的一种基于关系感知的全局注意力方法。【代码链接可访问**github[37]**】</p><img src="/c7412cf0/32.webp"><p>作者认为，要直观地判断一个特征节点是否重要，首先要知道其全局范围的特征信息，以便在决策的过程中更好地探索每个特征节点各自的全局关系，从而学习出更鲁棒的注意力特征。更详细的解读可参考<strong>本文[38]</strong></p><hr><h4 id="高阶注意力"><a href="#高阶注意力" class="headerlink" title="高阶注意力"></a>高阶注意力</h4><h5 id="GSoP-Net-39"><a href="#GSoP-Net-39" class="headerlink" title="GSoP-Net[39]"></a>GSoP-Net[39]</h5><p>《Global Second-order Pooling Convolutional Networks》发表于CVPR 2019，通过应用GSoP可以充分利用到图像中的二阶统计量，以高效的捕获全局的上下文信息。考虑到传统的一阶网络显然不能够有效的表征CNN，因为其目标是表征高维空间中数千个类别的复杂边界，因此学习高阶表示对于增强非线性建模能力至关重要。【代码链接可访问**github[40]**】</p><img src="/c7412cf0/33.webp"><p>从底层到高层逐步引入全局的二阶池化模块，通过对整体图像信息的相关性建模，来捕获长距离的统计信息，充分利用到了图像的上下文信息。与SE等操作提倡的利用二维的GAP操作不同，GSoP通过引入协方差来计算通道之间的关系。具体来说，在利用卷积和池化进行非线性变换以后，该协方差矩阵不仅可以用于沿通道维度进行张量的缩放，也可以用于沿空间维度进行张量缩放。</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在计算机视觉领域中，注意力机制大致可分为强注意力和软注意力。由于强注意力是一种随机的预测，其强调的是动态变化，虽然效果不错，但由于不可微的性质导致其应用很受限制。与之相反的是，软注意力是处处可微的，即能够通过基于梯度下降法的神经网络训练所获得，因此其应用相对来说也比较广泛，本文所列举的注意力方法均为软注意力方式。总的来说，目前所有的注意力机制方法大都是基于各个不同的维度利用有限的资源进行信息的充分利用，本质作用是增强重要特征，抑制非重要特征。注意力机制的特点是参数少-速度快-效果好。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1MzY0MDI2NA==&amp;mid=2247488701&amp;idx=1&amp;sn=4e03dd08e19a0f0fab6c43c702acf6a9">https://mp.weixin.qq.com/s?__biz=MzU1MzY0MDI2NA==&amp;mid=2247488701&amp;idx=1&amp;sn=4e03dd08e19a0f0fab6c43c702acf6a9</a><br>[1] <a target="_blank" rel="noopener" href="http://projects.i-ctm.eu/en/project/visual-attention">http://projects.i-ctm.eu/en/project/visual-attention</a><br>[2] <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34</a><br>[3] <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf">https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf</a><br>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a><br>[5] <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/xuc15">http://proceedings.mlr.press/v37/xuc15</a><br>[6] <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf</a><br>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a><br>[8] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/339215696">https://zhuanlan.zhihu.com/p/339215696</a><br>[9] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html">https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html</a><br>[10] <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8151-gather-excite-exploiting-feature-context-in-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/8151-gather-excite-exploiting-feature-context-in-convolutional-neural-networks.pdf</a><br>[11] <a target="_blank" rel="noopener" href="https://github.com/cuihu1998/GENet-Res50">https://github.com/cuihu1998/GENet-Res50</a><br>[12] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Residual_Attention_Network_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Residual_Attention_Network_CVPR_2017_paper.pdf</a><br>[13] <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf</a><br>[14] <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9102906">https://ieeexplore.ieee.org/document/9102906</a><br>[15] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.pdf</a><br>[16] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf</a><br>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.06514">https://arxiv.org/abs/1807.06514</a><br>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02579">https://arxiv.org/abs/1803.02579</a><br>[19] <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7318-a2-nets-double-attention-networks.pdf">https://papers.nips.cc/paper/7318-a2-nets-double-attention-networks.pdf</a><br>[20] <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf</a><br>[21] <a target="_blank" rel="noopener" href="https://github.com/xvjiarui/GCNet/blob/029db5407dc27147eb1d41f62b09dfed8ec88837/mmdet/models/plugins/non_local.py">https://github.com/xvjiarui/GCNet/blob/029db5407dc27147eb1d41f62b09dfed8ec88837/mmdet/models/plugins/non_local.py</a><br>[22] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.pdf</a><br>[23] <a target="_blank" rel="noopener" href="https://github.com/junfu1115/DANet">https://github.com/junfu1115/DANet</a><br>[24] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf</a><br>[25] <a target="_blank" rel="noopener" href="https://github.com/donnyyou/torchcv">https://github.com/donnyyou/torchcv</a><br>[26] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf</a><br>[27] <a target="_blank" rel="noopener" href="https://github.com/speedinghzl/CCNet">https://github.com/speedinghzl/CCNet</a><br>[28] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.pdf</a><br>[29] <a target="_blank" rel="noopener" href="https://github.com/xvjiarui/GCNet">https://github.com/xvjiarui/GCNet</a><br>[30] <a target="_blank" rel="noopener" href="https://github.com/xvjiarui/GCNet">https://github.com/xvjiarui/GCNet</a><br>[31] <a target="_blank" rel="noopener" href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR">https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR</a><br>[32] IAUnet: Global Context-Aware Feature Learning for Person Re-Identification<br>[33] <a target="_blank" rel="noopener" href="https://github.com/blue-blue272/ImgReID-IAnet">https://github.com/blue-blue272/ImgReID-IAnet</a><br>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.11879">https://arxiv.org/abs/2012.11879</a><br>[35] <a target="_blank" rel="noopener" href="https://github.com/dcdcvgroup/FcaNet">https://github.com/dcdcvgroup/FcaNet</a><br>[36] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Relation-Aware_Global_Attention_for_Person_Re-Identification_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Relation-Aware_Global_Attention_for_Person_Re-Identification_CVPR_2020_paper.pdf</a><br>[37] <a target="_blank" rel="noopener" href="https://github.com/microsoft/Relation-Aware-Global-Attention-Networks">https://github.com/microsoft/Relation-Aware-Global-Attention-Networks</a><br>[38] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/165569337">https://zhuanlan.zhihu.com/p/165569337</a><br>[39] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Global_Second-Order_Pooling_Convolutional_Networks_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Global_Second-Order_Pooling_Convolutional_Networks_CVPR_2019_paper.pdf</a><br>[40] <a target="_blank" rel="noopener" href="https://github.com/ZilinGao/Global-Second-order-Pooling-Convolutional-Networks">https://github.com/ZilinGao/Global-Second-order-Pooling-Convolutional-Networks</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/dd2a417b.html" rel="prev" title="深度学习-万字长文带你入门计算机视觉!"><i class="fa fa-chevron-left"></i> 深度学习-万字长文带你入门计算机视觉!</a></div><div class="post-nav-item"><a href="/a4951aec.html" rel="next" title="深度学习-万字长文带你入门Transformer">深度学习-万字长文带你入门Transformer <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E8%AF%BB"><span class="nav-number">1.</span> <span class="nav-text">导读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%B7%E6%BA%90"><span class="nav-number">2.</span> <span class="nav-text">起源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AA-Model-of-Saliency-Based-Visual-Attention-for-Rapid-Scene-Analysis%E3%80%8B-2"><span class="nav-number">2.1.</span> <span class="nav-text">《A Model of Saliency-Based Visual Attention for Rapid Scene Analysis》[2]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8ARecurrent-Models-of-Visual-Attention%E3%80%8B-3"><span class="nav-number">2.2.</span> <span class="nav-text">《Recurrent Models of Visual Attention》[3]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8ANeural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate%E3%80%8B-4"><span class="nav-number">2.3.</span> <span class="nav-text">《Neural Machine Translation by Jointly Learning to Align and Translate》[4]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AShow-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention%E3%80%8B-5"><span class="nav-number">2.4.</span> <span class="nav-text">《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》[5]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8ALook-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition%E3%80%8B-6"><span class="nav-number">2.5.</span> <span class="nav-text">《Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition》[6]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AAttention-is-All-Your-Need%E3%80%8B-7"><span class="nav-number">2.6.</span> <span class="nav-text">《Attention is All Your Need》[7]</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E5%B1%95"><span class="nav-number">3.</span> <span class="nav-text">发展</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E9%81%93-amp-%E7%A9%BA%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.1.</span> <span class="nav-text">通道&amp;空间注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SE-Net-9"><span class="nav-number">3.1.1.</span> <span class="nav-text">SE-Net[9]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GE-Net-10"><span class="nav-number">3.1.2.</span> <span class="nav-text">GE-Net[10]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RA-Net-12"><span class="nav-number">3.1.3.</span> <span class="nav-text">RA-Net[12]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SK-Net-13"><span class="nav-number">3.1.4.</span> <span class="nav-text">SK-Net[13]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SPA-Net-14"><span class="nav-number">3.1.5.</span> <span class="nav-text">SPA-Net[14]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ECA-Net-15"><span class="nav-number">3.1.6.</span> <span class="nav-text">ECA-Net[15]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.2.</span> <span class="nav-text">混合注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CBAM-16"><span class="nav-number">3.2.1.</span> <span class="nav-text">CBAM[16]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BAM-17"><span class="nav-number">3.2.2.</span> <span class="nav-text">BAM[17]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#scSE-18"><span class="nav-number">3.2.3.</span> <span class="nav-text">scSE[18]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A2-Nets-19"><span class="nav-number">3.2.4.</span> <span class="nav-text">A2-Nets[19]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.3.</span> <span class="nav-text">自注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Non-Local-20"><span class="nav-number">3.3.1.</span> <span class="nav-text">Non-Local[20]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DA-Net-22"><span class="nav-number">3.3.2.</span> <span class="nav-text">DA-Net[22]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ANLNet-24"><span class="nav-number">3.3.3.</span> <span class="nav-text">ANLNet[24]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CC-Net-26"><span class="nav-number">3.3.4.</span> <span class="nav-text">CC-Net[26]</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GC-Net-28"><span class="nav-number">3.3.5.</span> <span class="nav-text">GC-Net[28]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.4.</span> <span class="nav-text">类别注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#OCR-Net-30"><span class="nav-number">3.4.1.</span> <span class="nav-text">OCR-Net[30]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.5.</span> <span class="nav-text">时间注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#IAU-Net-32"><span class="nav-number">3.5.1.</span> <span class="nav-text">IAU-Net[32]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%91%E7%8E%87%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.6.</span> <span class="nav-text">频率注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Fca-Net-34"><span class="nav-number">3.6.1.</span> <span class="nav-text">Fca-Net[34]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.7.</span> <span class="nav-text">全局注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RGA-Net-36"><span class="nav-number">3.7.1.</span> <span class="nav-text">RGA-Net[36]</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.8.</span> <span class="nav-text">高阶注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GSoP-Net-39"><span class="nav-number">3.8.1.</span> <span class="nav-text">GSoP-Net[39]</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">840</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">50</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">2.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">109:35</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>