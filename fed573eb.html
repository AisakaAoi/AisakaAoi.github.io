<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="Abstract 摘要情绪是多通道的过程扮演着重要的角色在我们的日常生活。识别情绪变得越来越重要在广泛的应用领域，如医疗、教育、人机交互、虚拟现实、智能代理、娱乐等等。 面部宏表情macro-expressions或强烈的面部表情识别情绪状态是最常见的形式。然而，由于面部表情可以主动控制，它们可能不会准确地代表情绪状态。早些时候的研究表明，面部微表情比面部宏表情揭示情绪更可靠。它们是微妙的，无意识"><meta property="og:type" content="article"><meta property="og:title" content="将面部微表情与脑电图和生理信号结合起来进行情绪识别-《Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition》"><meta property="og:url" content="https://aisakaaoi.github.io/fed573eb.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="Abstract 摘要情绪是多通道的过程扮演着重要的角色在我们的日常生活。识别情绪变得越来越重要在广泛的应用领域，如医疗、教育、人机交互、虚拟现实、智能代理、娱乐等等。 面部宏表情macro-expressions或强烈的面部表情识别情绪状态是最常见的形式。然而，由于面部表情可以主动控制，它们可能不会准确地代表情绪状态。早些时候的研究表明，面部微表情比面部宏表情揭示情绪更可靠。它们是微妙的，无意识"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/fed573eb/10.webp"><meta property="article:published_time" content="2022-11-25T13:01:13.000Z"><meta property="article:modified_time" content="2026-01-26T17:33:35.247Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/fed573eb/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/fed573eb.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>将面部微表情与脑电图和生理信号结合起来进行情绪识别-《Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition》 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1023</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/fed573eb.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">将面部微表情与脑电图和生理信号结合起来进行情绪识别-《Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition》</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-11-25 21:01:13" itemprop="dateCreated datePublished" datetime="2022-11-25T21:01:13+08:00">2022-11-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/fed573eb.html" class="post-meta-item leancloud_visitors" data-flag-title="将面部微表情与脑电图和生理信号结合起来进行情绪识别-《Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition》" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/fed573eb.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/fed573eb.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>23k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>59 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract 摘要"></a>Abstract 摘要</h3><p>情绪是多通道的过程扮演着重要的角色在我们的日常生活。识别情绪变得越来越重要在广泛的应用领域，如医疗、教育、人机交互、虚拟现实、智能代理、娱乐等等。</p><p>面部宏表情macro-expressions或强烈的面部表情识别情绪状态是最常见的形式。然而，由于面部表情可以主动控制，它们可能不会准确地代表情绪状态。早些时候的研究表明，面部微表情比面部宏表情揭示情绪更可靠。它们是微妙的，无意识的运动响应外界刺激，无法控制。</p><p>本文提出了使用面部微表情结合大脑和更可靠的检测潜在的情感生理信号。模型测量唤醒和效价水平从面部表情、脑电图(EEG)信号、皮肤电反应(GSR)和光谱分析(PPG)信号。然后评估模型使用DEAP数据集和基于subject-independent方法自采数据集。最后讨论结果、工作的局限性，如何克服这些限制，还讨论未来的发展方向使用面部表情和情感生理信号识别。</p><span id="more"></span><hr><h3 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction 介绍"></a>Introduction 介绍</h3><p>人类的情绪涉及众多的外部和内部活动，在我们的日常生活中起着至关重要的作用。面部表情、语言和身体姿态是受情绪影响的一些外部活动。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Verma and Tiwary (2014)</td><td align="left">大脑活动、心率、血压、呼吸频率、体温和皮肤传导的变化是内部情绪影响的例子*。</td></tr><tr><td align="left">Zheng et al. (2018)</td><td align="left">如今，在现代社会中，我们被数字字符、智能设备和计算机所包围。有必要与这些系统进行更好的互动，在许多人与人、人与计算机的互动中，识别情感变得越来越重要*。</td></tr><tr><td align="left">Khalfallah和Slama (2015)</td><td align="left">如果我们的远程互动、治疗、咨询或培训课程配备了情绪识别系统，其效果就会得到改善。例如，在远程电子学习中识别情绪*可以提高学习的绩效。</td></tr><tr><td align="left">Piumsomboon et al. (2017)</td><td align="left">在移情计算的应用中，目标是测量一起举行远程会议的人的情绪，并利用结果来改善远程通信*。</td></tr></tbody></table><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Huang et al. (2016)</td><td align="left">创建具有情感识别能力的智能代理可能对医疗保健、教育、娱乐、犯罪调查和其他领域有帮助*。</td></tr><tr><td align="left">Marcos-Pablos et al. (2016)</td><td align="left">它可以有利于智能助手*测量用户的情感。</td></tr><tr><td align="left">Bartlett et al. (2003)</td><td align="left">它可以有利于人形机器人*能够测量用户的情感。</td></tr><tr><td align="left">Zepf et al. (2020)</td><td align="left">*讨论emotion-aware系统在汽车上的重要性。</td></tr><tr><td align="left">Hu et al. (2021)</td><td align="left">*提出了一个对话代理，它可以根据语音的声学特征来识别情绪。</td></tr><tr><td align="left">Chin et al. (2020)</td><td align="left">*对话代理和人之间的移情可以改善攻击性行为。</td></tr><tr><td align="left">Schachner et al. (2020)</td><td align="left">讨论了为健康护理开发智能对话代理，特别是针对慢性病。</td></tr><tr><td align="left">Aranha et al. (2019)</td><td align="left">*回顾了在健康、教育、安全和艺术等不同领域中能够识别情绪的智能用户界面的软件。根据他们的评论，情绪识别经常被用于根据用户的情绪来调整声音、用户界面、图形和内容。</td></tr></tbody></table><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Sun et al. (2020)</td><td align="left">面部表情是最常用的输入模式之一，被分析用来识别情绪状态*。</td></tr><tr><td align="left">Samadiani et al. (2019)</td><td align="left">面部表情被用于许多人机交互应用中*。</td></tr><tr><td align="left">Li and Deng (2020)</td><td align="left">虽然研究表明，从面部表情中识别情绪的效果很好*。</td></tr><tr><td align="left">Hossain and Gedeon (2019)</td><td align="left">但在日常生活中使用这些方法面临一些挑战，因为它们可以被人类控制或伪造*。</td></tr><tr><td align="left">Weber et al. (2018); Li and Deng (2020)</td><td align="left">许多从面部表情中识别情绪的方法都是基于非自发的面部表情或夸张的面部表情的数据集，这并不能正确反映真实的情绪*。</td></tr><tr><td align="left">Zeng et al. (2008)</td><td align="left">在现实世界中，人们通常会表现出微妙的不自主的表情*或根据刺激的类型表现出强度较低的表情。</td></tr></tbody></table><p>这些研究表明了开发和改进识别自发情绪的有效方法的重要性。</p><hr><h4 id="Recognizing-Spontaneous-Emotions-识别自发的情绪"><a href="#Recognizing-Spontaneous-Emotions-识别自发的情绪" class="headerlink" title="Recognizing Spontaneous Emotions 识别自发的情绪"></a>Recognizing Spontaneous Emotions 识别自发的情绪</h4><p>文献中提出了三种主要的方法来识别现实世界中微妙的、自发的情绪：</p><ul><li>从脸部提取不自主的表情</li><li>使用无法伪造的生理信号</li><li>使用各种输入模式的组合</li></ul><hr><h5 id="Extracting-Facial-Micro-Expressions-From-Faces-从脸部提取面部微表情"><a href="#Extracting-Facial-Micro-Expressions-From-Faces-从脸部提取面部微表情" class="headerlink" title="Extracting Facial Micro-Expressions From Faces 从脸部提取面部微表情"></a>Extracting Facial Micro-Expressions From Faces 从脸部提取面部微表情</h5><p>在这种方法中，重点是提取面部微表情而不是面部宏观表情。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Ekman and Rosenberg (1997)</td><td align="left">面部宏观表情或强烈的面部表情是指面部的自愿性肌肉运动，这些运动是可以区分的，覆盖了面部的大部分区域，其持续时间在0.5到4秒之间*。</td></tr><tr><td align="left">Yan et al. (2013)</td><td align="left">相比之下，面部微表情指的是短暂的、非自愿的面部变化，如眉毛内侧的上扬或鼻子的皱褶，这些变化是对外部刺激的自发反应，通常在65至500毫秒的短时间内发生*。</td></tr><tr><td align="left">Takalkar et al. (2018)</td><td align="left">面部微表情很难伪造，可以用来检测真实的情绪*。</td></tr><tr><td align="left">Qu et al. (2016)</td><td align="left">这些表情持续时间短，动作细微，人类很难识别它们*。</td></tr></tbody></table><p>图1显示了一些与面部宏观表情相比的面部微表情的例子</p><img src="/fed573eb/1.webp"><hr><h5 id="Using-Physiological-Signals-That-Cannot-Be-Faked-使用无法伪造的生理信号"><a href="#Using-Physiological-Signals-That-Cannot-Be-Faked-使用无法伪造的生理信号" class="headerlink" title="Using Physiological Signals That Cannot Be Faked 使用无法伪造的生理信号"></a>Using Physiological Signals That Cannot Be Faked 使用无法伪造的生理信号</h5><p>这种方法依赖于难以伪造的生理反应，并提供对潜在情绪的更好理解。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Kreibig, 2010</td><td align="left">这些反应来自中枢（大脑和脊髓）和自主神经系统（调节身体功能，如心率）*。</td></tr><tr><td align="left">Alarcao and Fonseca, 2017</td><td align="left">脑电图（EEG）是测量大脑活动的方法之一，通常用于情绪研究*。</td></tr><tr><td align="left">Perez-Rosero et al., 2017; Setyohadi et al., 2018; Shu et al., 2018</td><td align="left">皮肤电泳反应（GSR）和心率变异（HRV）也可以用来可靠地测量情绪状态，并被广泛用于情绪识别研究*。</td></tr><tr><td align="left">Wioleta, 2013</td><td align="left">虽然EEG和生理信号更可靠，不能被人类控制或伪造*。</td></tr><tr><td align="left">Jiang et al., 2020</td><td align="left">这些信号可能非常弱，容易被噪音污染*。</td></tr></tbody></table><p>所以，只用生理信号来识别情绪是相当有挑战性的。</p><hr><h5 id="Using-a-Combination-of-Various-Input-Modalities-使用各种输入模式的组合"><a href="#Using-a-Combination-of-Various-Input-Modalities-使用各种输入模式的组合" class="headerlink" title="Using a Combination of Various Input Modalities 使用各种输入模式的组合"></a>Using a Combination of Various Input Modalities 使用各种输入模式的组合</h5><p>在这种方法中，各种模式被结合起来以克服每个单独模式的弱点。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Yazdani et al., 2012; Shu et al., 2018</td><td align="left">结合不同的生理信号进行情感识别*。</td></tr><tr><td align="left">Busso et al., 2008; McKeown et al., 2011</td><td align="left">只融合行为模式的方法已经被广泛探索*。</td></tr><tr><td align="left">Zheng et al., 2018; Huang et al., 2019; Zhu et al., 2020</td><td align="left">最近一些研究试图通过利用生理和行为技术来改进情绪识别方法*。</td></tr><tr><td align="left">Koelstra and Patras, 2013; Huang et al., 2017; Zhu et al., 2020</td><td align="left">许多研究使用面部表情和EEG信号的组合来实现这种改进*。</td></tr><tr><td align="left">Koelstra et al., 2011; Soleymani et al., 2011</td><td align="left">通常，这些研究人员的工作是在受试者观看视频或看静态图像时收集的数据*。然而，人们在这些任务中往往不会表现出很多面部表情。因此，常规的面部表情策略可能无法准确识别情绪。</td></tr><tr><td align="left">Huang et al., 2016</td><td align="left">有限的研究使用了面部微表情来代替面部宏观表情*，但这个领域仍然需要更多的研究和探索。</td></tr></tbody></table><p>此外，根据Doma和Pirouz（2020）的研究，真正的情绪何时开始并不清楚。他们假设，在观看视频刺激的前几秒，参与者可能还处于之前的情绪状态。而在最后几秒钟，他们可能更沉浸在视频中，感受到真正的情感。这是因为他们在最后几秒钟更好地理解视频。他们发现，最后几秒钟的脑电图数据信息量更大，显示出更好的情绪预测结果。感受情绪最强烈的峰值时间受到许多因素的影响，如刺激流、参与者的个性或以前的经验。</p><hr><h4 id="Goals-Overview-and-Contributions-目的、总览与贡献"><a href="#Goals-Overview-and-Contributions-目的、总览与贡献" class="headerlink" title="Goals, Overview, and Contributions 目的、总览与贡献"></a>Goals, Overview, and Contributions 目的、总览与贡献</h4><p><strong>本文假设：</strong>通过识别和分析每个刺激物中最情绪化的部分或出现情绪的时间，可以更好地理解身体对情绪的反应，并创建更强大的模型来识别情绪。</p><p><strong>研究目的：</strong>通过将面部微表情策略与脑电图和生理信号相结合来改善情绪识别。</p><p><strong>本文方法：</strong>首先对每个面部视频进行扫描，寻找大致表明出现情绪刺激的微表情。微表情窗口被用来大致确定情绪产生的时间。然后，分析每次试验中微表情出现前后的脑电和生理数据，与整个试验的分析进行比较。最后比较了这两种策略，并基于独立于主体的方法评估的方法。文章还使用DEAP数据集作为基准来评估的方法。作者进行了一项用户研究，在观看与DEAP数据集类似的视频任务时收集面部视频、EEG、PPG和GSR数据，但使用不同的传感器。</p><p><strong>主要贡献：</strong></p><ul><li>将面部微表情与脑电图和生理信号相融合以识别情绪。</li><li>利用面部微表情来识别情绪刺激或更多信息期的数据，以提高识别精度。</li><li>使用低成本和开源的EEG采集设备创建一个新的多模态数据集用于情感识别。</li></ul><hr><h3 id="Conclusions-结论"><a href="#Conclusions-结论" class="headerlink" title="Conclusions 结论"></a>Conclusions 结论</h3><p>本文展示了如何将面部微表情与EEG和生理信号一起有效地用于识别情绪状态。</p><p><strong>本文方法：</strong></p><ul><li>使用了面部微表情的情感识别，而不是结合生理模式的面部宏观表情的情感识别，这在识别真实情感方面更加可靠。</li><li>使用了面部微表情识别策略来大致确定数据中最具情感和信息的部分。</li><li>使用基于地标的发现策略来检测微表情，确定了每个试验的兴趣区域（ROI）。提取了微表情周围的几个帧，并将其输入到一个三维卷积网络。</li><li>从脑电图和生理数据中提取了一连串的特征向量，这些数据被划分为1秒的窗口。</li><li>为了从生理信号和EEG信号中提取时间特征，采用了LSTM。评估了用LSTM、SVM、KNN和RF分类器对ROI的分类与对所有数据的分类的比较。</li><li>本文的方法是基于独立于主体的方法进行评估的（subject-independent）。</li></ul><p><strong>本文效果：</strong></p><ul><li>根据结果与所有数据相比，可以通过使用一小部分数据获得类似甚至更好的准确性。</li><li>根据研究结果，面部微表情可以识别出具有足够信息和低噪音的数据中更多的情感部分。</li><li>本文使用一个低成本、开源的EEG采集设备来收集多模态的情绪数据。</li><li>根据DEAP数据集和自采数据评估了本文的方法。最后结合了多种模式，并发现融合它们的输出可以改善情绪识别。- 本文发现面部微表情比面部宏观表情方法更有效地检测出真实情绪。</li></ul><p><strong>未来工作：</strong></p><ul><li>由于OpenBCI硬件的高数据质量和易用性，文章希望在后续的研究中用OpenBCI的各种设置收集更多的数据。收集到的数据将被用于预训练即将到来的模型，以创建一个强大的模型来识别EEG数据中的情绪。</li><li>在获得发布数据集的伦理批准后，作者希望公开EEG和生理学数据。这将有助于研究人员训练更强大的情感识别模型。</li><li>想在未来研究更多的特征，看看改变特征集或使用更复杂的特征是否会改善LSTM方法的性能。</li><li>还想使用更复杂的融合策略来有效利用多模态传感器。</li><li>探索如何从更自然的头部运动中提取面部微表情（例如，不要求人们保持严肃）。</li><li>此外，在常规面部表情存在的情况下识别面部微表情，并探索如何将两者结合起来用于识别情绪，这将是非常有趣的。</li><li>希望可以将这种情绪识别方法纳入到医疗保健的应用中，如远程治疗课程，识别病人的情绪障碍，或创建智能助手来帮助病人或老人。</li><li>这种情感模型还可以用于我们与人类的日常互动中，比如加强远程会议，使远程互动更具有沉浸感</li><li>改善我们与虚拟代理和其他我们经常使用的互动设备的互动，让它们有能力识别和回应我们的情绪。</li></ul><hr><h3 id="Preliminaries-前言"><a href="#Preliminaries-前言" class="headerlink" title="Preliminaries 前言"></a>Preliminaries 前言</h3><h4 id="Emotion-Models-情绪模型"><a href="#Emotion-Models-情绪模型" class="headerlink" title="Emotion Models 情绪模型"></a>Emotion Models 情绪模型</h4><p>一些研究人员认为，存在一些适用于所有年龄和文化的普遍情绪（Maria等人，2019）。为了避免在情感识别中犯错并设计一个可靠的系统，有必要对情感建模进行更深入的了解。研究人员以两种方式表示情绪：</p><ul><li>第一种观点是Ekman和Friesen（1971）提出的著名的离散情绪模型，它将情绪分为六种基本类型；快乐、悲伤、惊讶、愤怒、厌恶和恐惧。</li><li>第二种观点认为情绪是三个心理维度的组合：唤醒和效价以及支配或强度之一。早期的研究已经证明，唤醒和效价这两个维度足以解释基本的情绪，这些情绪主要是由神经生理因素驱动的（Eerola和Vuoskoski，2011）。</li></ul><p>文献中最常用的维度模型是Russel的Circumplex模型（Posner等人，2005），它只用效价和唤醒来代表情绪，其中价值代表从消极到积极的情绪范围。相比之下，唤醒代表一种从被动到主动的情绪。</p><p>根据罗素的圆环模型，将情绪状态归类为不连续的情绪是不正确的，因为人类的情绪状态总是几种情绪的混合物。因此，当人们报告恐惧是他们的情绪时，可能是兴奋、快乐和恐惧的混合，或者是负面情绪和恐惧的混合。所以，在积极和消极的恐惧情况下，大脑和生理信号的模式是不一样的，把它们归为一类会导致错误的识别。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Maria et al. (2019)</td><td align="left">基于经验、文化、年龄和许多其他因素，对情绪的感知也有很大的不同，这使得评估变得困难*。</td></tr><tr><td align="left">Lichtenstein et al. (2008)</td><td align="left">*的研究表明，维度方法对自我评估更准确。</td></tr><tr><td align="left">Eerola and Vuoskoski (2011)</td><td align="left">在对复杂的情绪刺激进行评分时，离散情绪模型不如维度模型可靠*。他们还观察到离散模型和维度模型之间有很高的对应关系。</td></tr></tbody></table><p><strong>本文情绪模型：</strong><br>面部宏表情和面部微表情通常用离散情绪来表达，以前的研究使用离散情绪模型来评估他们的策略。然而，大多数关于神经生理学情绪识别的研究和本文使用的基准数据集，都使用了环状模式（唤醒和效价）来评估他们的方法。由于本文研究的重点是揭示潜在的情绪，并且除了面部微表情外还使用了三种神经生理线索，因此使用二维环状模型来评估本文在基准数据集和自采数据集上的方法。</p><hr><h4 id="Emotion-Stimulation-Methods-情绪刺激方法"><a href="#Emotion-Stimulation-Methods-情绪刺激方法" class="headerlink" title="Emotion Stimulation Methods 情绪刺激方法"></a>Emotion Stimulation Methods 情绪刺激方法</h4><p>情绪有不同的诱导情绪的方法。然而，所有情绪诱导方法的效果是不一样的。Siedlecka和Denson（2019）将情绪刺激分为五种策略：</p><ol><li>观看图像和视频等视觉刺激；</li><li>听音乐；</li><li>回忆个人情绪记忆；</li><li>完成心理程序；</li><li>想象情绪场景。</li></ol><p>他们展示了不同类型的刺激如何对各种生理变量产生不同的影响。根据他们的研究，视觉刺激是文献中使用较多的最有效的诱导方法。另外：</p><ul><li>Roberts等人（2007）发现，双人互动可以被认为是一种情感诱导方法。</li><li>Quigley等人（2014）增加了言语、身体运动、生理操纵器如咖啡因和虚拟现实（VR）。</li></ul><hr><h4 id="Facial-Micro-Expressions-面部微表情"><a href="#Facial-Micro-Expressions-面部微表情" class="headerlink" title="Facial Micro Expressions 面部微表情"></a>Facial Micro Expressions 面部微表情</h4><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Ekman, 2003</td><td align="left">面部微表情是对情绪刺激的简短面部动作，它揭示了隐藏的情绪*。</td></tr><tr><td align="left">Yan et al., 2013</td><td align="left">微表情已被用于测谎、安全系统以及临床和心理领域，以揭示潜在的情绪*。</td></tr><tr><td align="left">Liong et al., 2015</td><td align="left">与宏观表情相比，较少的动作和较短的持续时间是面部微表情的主要特征*。</td></tr><tr><td align="left">Yan et al., 2013</td><td align="left">*研究了微表情的持续时间，结果显示其持续时间在65至500毫秒之间。</td></tr><tr><td align="left">Li et al., 2013; Yan et al., 2014</td><td align="left">由于视频情节是动态的、持续时间长的情感刺激，它们被用于微表情研究，并创造了大多数微表情数据集*。</td></tr><tr><td align="left">Li et al., 2013; Yan et al., 2013, 2014</td><td align="left">为了防止微表情记录中的面部宏观表情污染，在许多研究中，参与者被要求在观看视频时抑制任何面部动作并保持扑克脸*。</td></tr><tr><td align="left">Yan et al., 2013</td><td align="left">然而，在应对情绪化的视频刺激时，抑制是很难实现的。</td></tr></tbody></table><p>一个微表情有三个阶段；开始阶段、顶点阶段和抵消阶段。在对情绪刺激的反应中，快速的肌肉运动发生在开始阶段，这是非自愿的，显示出真正的情绪泄漏。有时这些反应会持续片刻，作为顶点阶段。最后，情绪反应在偏移期消失，面部恢复到放松状态。</p><table><thead><tr><th align="left">Author</th><th align="left">Effects</th></tr></thead><tbody><tr><td align="left">Yan et al., 2013</td><td align="left">由于皮肤的自然紧张，恢复到放松状态对某些人来说可能需要更长的时间，或者因为与随后的情绪刺激合并而没有发生*。</td></tr><tr><td align="left">Goh et al., 2020</td><td align="left">在录制的视频中，开始阶段的第一帧表示开始帧，而表情最丰富的那一帧是顶点帧。偏移帧是表情消失的时候*。</td></tr></tbody></table><p>利用面部微表情识别情绪有两个主要步骤：</p><ul><li>第一步是在视频序列中发现或定位有微表情的帧或帧。</li><li>第二步是识别微表情的情绪状态（Oh等人，2018；Tran等人，2020）。</li></ul><p>一些工作使用<strong>手工特征</strong>的策略，如：</p><ul><li>三正交平面的局部二进制模式（LBP-TOP）（Pfister等人，2011）</li><li>定向梯度直方图（HOG）（Davison等人，2015）从帧中提取特征，以发现和识别情绪。</li><li>（Guermazi等人，2021）提出了一种基于LBP的微表情识别方法，以创建面部视频的低维高相关表示，并使用随机森林分类器对微表情进行分类。</li></ul><p><strong>深度学习</strong>提取深度特征：</p><ul><li>深度学习技术被用来提取深度特征，并利用面部微表情进行情绪分类（Van Quang等人，2019；Tran等人，2020）。</li><li>Hashmi等人（2021）提出了一个无损注意力残差网络（LARNet），用于编码特定关键位置的面部空间和时间特征，并对面部微表情进行分类。虽然他们在实时识别情绪方面取得了可喜的成绩，但他们的模型只有在帧率超过200帧时才有效。</li><li>Xia等人（2019）提出了一个递归卷积神经网络（RCN）来提取面部微表情的时空变形。他们使用基于外观和基于几何的方法将面部序列转化为矩阵并提取面部运动的几何特征。他们对他们的策略进行了评估，基于留下一个视频（LOVO）和留下一个主体（LOSO）的方法，并取得了令人满意的结果。</li><li>Xia等人（2020）提出了一个RCN网络来识别多个数据集的微表情。他们还讨论了输入和模型复杂性对深度学习模型性能的影响。他们表明，在组合数据集上运行模型时，低分辨率的输入数据和较浅的模型是有益的。</li></ul><p><strong>数据集</strong>方面：</p><ul><li>Ben等人（2021）回顾了现有的面部微表情数据集，并讨论了用于识别面部微表情的不同特征提取方法。在这项研究中，他们引入了一个新的微表情数据集，并讨论了微表情研究的未来方向。</li><li>Pan等人（2021）总结和比较了现有的发现和微表情策略，并讨论了该领域的局限性和挑战。</li><li>检测面部微表情已受到越来越多的关注。许多数据集已经被创建，发现和识别方法也有了很大发展。然而，识别面部微表情仍然面临许多挑战（Weber等人，2018；Zhao和Li，2019；Tran等人，2020）。</li><li>Oh等人（2018）讨论了数据集、发现和识别领域的各种挑战。他们表明，处理面部宏观运动，开发更强大的发现策略，以及忽略不相关的面部信息，如头部运动和跨数据集的评估，仍然需要更多的关注和研究。</li></ul><hr><h4 id="Electroencephalography-EEG-Signals-脑电图信号"><a href="#Electroencephalography-EEG-Signals-脑电图信号" class="headerlink" title="Electroencephalography (EEG) Signals 脑电图信号"></a>Electroencephalography (EEG) Signals 脑电图信号</h4><p>最近，许多神经心理学研究调查了情绪和大脑信号之间的相关性。脑电图（EEG）是神经成像技术之一，通过安装在头皮上的电极读取大脑电活动。</p><ul><li>脑电图设备根据电极的类型和数量、电极的位置（灵活或固定位置）、连接类型（无线或有线）、放大器和过滤步骤的类型、设置和可佩戴性而有所不同（Teplan等，2002）。</li><li>像g.tec1或Biosemi2或EGI3这样具有较高数据质量的脑电图设备通常是昂贵和笨重的，需要耗时的设置。另外，还有一些数据质量较低的EEG设备，如Emotiv Epoc4或MindWave5。这些脑电图设备价格低廉，是无线设备，需要的设置时间较短（Alarcao和Fonseca，2017）。</li><li>OpenBCI6提供了一个轻量级和开源（硬件和软件）的脑电图耳机，它的定位在这两个产品类别之间。它可以捕获高质量的数据，同时成本低，易于设置。如今，由于EEG设备的可穿戴性提高和价格降低，利用EEG信号识别情绪已经吸引了许多研究人员（Alarcao和Fonseca，2017）。</li></ul><p><strong>基于EEG的情绪识别</strong>是一个令人兴奋和快速增长的研究领域。</p><ul><li>由于EEG信号的振幅较弱，使用EEG识别情绪具有挑战性（Islam等人，2021）。</li><li>一些研究专注于提取手工制作的特征，并使用浅层机器学习方法对健康护理等不同应用领域的情绪进行分类（Aydın等人，2018；Bazgir等人，2018；Pandey和Seeja，2019a；Huang等人，2021）。</li></ul><p>一些综述研究讨论了各种<strong>手工特征</strong>的效果，如脑波段功率，以及使用各种分类器，如支持向量机（SVM）或随机森林（RF）来识别情绪。</p><ul><li>Alarcao和Fonseca（2017）回顾了EEG情绪识别研究。他们讨论了文献中用于情绪识别的最常见的数据清理和特征提取。根据他们的回顾，大脑波段功率，包括α、β、θ、γ和δ波段，是情绪识别的有效特征。</li><li>Wagh和Vasanth（2019）对基于脑机接口和机器学习算法的人类情绪分析中涉及的各种技术进行了详细调查。</li></ul><p><strong>深度学习</strong>方法：</p><ul><li>许多研究人员使用原始EEG信号并应用深度学习方法来提取深度特征并识别情绪（Keelawat等人，2019；Aydın，2020）。</li><li>Sharma等人（2020）使用基于LSTM的深度学习方法，根据EEG信号对情绪状态进行分类。</li><li>Topic和Russo（2021）使用深度学习来提取EEG信号的地形和全息表征，并对情绪状态进行分类。</li><li>Islam等人（2021）对基于EEG的情绪识别方法进行了全面的回顾。他们讨论了各种特征提取方法以及用于识别情绪的浅层和深层学习方法。</li></ul><p>近年来，研究人员专注于<strong>更先进的网络架构</strong>以提高性能。</p><ul><li>Li等人（2021）提出了一个基于强化学习（RL）的神经架构搜索（NAS）框架。他们用RL训练了一个循环神经网络（RNN）控制器，以使验证集上生成的模型性能最大化。在DEAP数据集上，他们用一种依赖主体的方法，对唤醒和情感取得了约98%的高平均准确性。</li><li>在另一项研究中（Li等人，2022），他们提出了一个多任务学习机制，同时进行唤醒、价值和支配力的学习步骤。他们还使用了一个胶囊网络来寻找通道之间的关系。最后，他们使用注意力机制来寻找通道的最佳权重，以便从数据中提取最重要的信息。他们在依赖主体的方法中，对唤醒和价值的平均准确率达到了97.25％，97.41％。</li><li>同样，Deng等人（2021）使用注意力机制为通道分配权重，然后用胶囊网络和LSTM来提取空间和时间特征。他们对唤醒和情绪水平的平均准确率达到了97.17%，97.34%。</li></ul><hr><h4 id="Galvanic-Skin-Responses-GSR-Signals-皮肤电反应信号"><a href="#Galvanic-Skin-Responses-GSR-Signals-皮肤电反应信号" class="headerlink" title="Galvanic Skin Responses (GSR) Signals 皮肤电反应信号"></a>Galvanic Skin Responses (GSR) Signals 皮肤电反应信号</h4><p>研究表明，神经系统和人类皮肤上的汗腺之间存在联系。因为情绪亢奋而导致的汗液分泌水平的变化导致了皮肤电阻的变化（Tarnowski等人，2018；Kołodziej等人，2019），这被称为皮肤电活动（EDA）或皮肤电反应（GSR）。</p><p>当皮肤接收到大脑由情绪唤醒引起的兴奋信号时，人体的出汗就会发生变化，GSR信号也会上升。</p><ul><li>Kreibig（2010）表明，虽然EDA信号显示了情绪唤醒的变化，但还需要更多的研究来利用EDA信号识别情绪的类型。</li><li>Tarnowski等人（2018）使用GSR局部最小值作为EEG的情绪纪元的指标。他们表明，GSR是情绪唤醒的一个很好的指标。</li><li>在许多研究中，GSR信号的统计特征被用作情绪分类的特征（Udovicic等人，2017；Yang等人，2018）。</li><li>Kołodziej等人（2019）计算了一些峰值（局部最大值）和原始GSR信号的统计数据，作为信号的特征。他们使用了不同的分类器，并表明SVM在使用这些统计特征识别情绪唤醒方面比其他分类器效果更好。</li></ul><p>一些研究使用了<strong>时间序列或平均信号</strong>作为特征向量。</p><ul><li>Setyohadi等人（2018）收集了每一秒的平均信号并应用了特征缩放。他们用这个数据对积极、中性和消极的情绪状态进行分类。他们使用了不同的分类器，带有Radial Based Kernel的SVM显示出最好的准确性。</li><li>Kanjo等人（2019）使用GSR时间序列和深度学习分析来了解在城市中间行走时的情绪水平。</li><li>Ganapathy等人（2021）表明，多尺度卷积神经网络（MSCNN）在提取GSR信号的深层特征和分类情绪方面是有效的。</li></ul><p>在许多研究中，GSR信号已被独立用于识别情绪。但是，它们主要被用作补充信号或与其他生理信号相结合来识别情绪（Das等人，2016；Udovicic等人，2017；Wei等人，2018；Yang等人，2018；Maia和Furtado，2019）。</p><hr><h4 id="Photoplethysmography-PPG-Signals-光谱分析信号"><a href="#Photoplethysmography-PPG-Signals-光谱分析信号" class="headerlink" title="Photoplethysmography (PPG) Signals 光谱分析信号"></a>Photoplethysmography (PPG) Signals 光谱分析信号</h4><ul><li>光密度计（PPG）是一种利用红外线测量血容量脉冲（BVP）的新方法（Elgendi，2012）。</li><li>事实证明，PPG可以测量心率变异性（HRV）。HRV是对心率的时间变化的测量，以揭示医疗或精神状态（Maria等人，2019）。</li></ul><p>由于像智能手表这样可以传输PPG信号的可穿戴设备的出现，利用PPG信号的研究受到更多关注。</p><ul><li>Kreibig（2010）已经显示了不同情绪状态下心率变异和心率的变化。</li><li>最近，有限的研究使用深度学习策略来提取PPG信号的深度特征。Lee等人（2019）使用一维卷积神经网络（1D CNN）来提取PPG信号的深层特征并对情绪状态进行分类。与GSR信号类似，PPG数据通常与其他生理信号一起使用来识别情绪状态。</li></ul><hr><h3 id="Related-Works-相关工作"><a href="#Related-Works-相关工作" class="headerlink" title="Related Works 相关工作"></a>Related Works 相关工作</h3><h4 id="Multimodal-Datasets-for-Emotion-Recognition-情绪识别多模态数据集"><a href="#Multimodal-Datasets-for-Emotion-Recognition-情绪识别多模态数据集" class="headerlink" title="Multimodal Datasets for Emotion Recognition 情绪识别多模态数据集"></a>Multimodal Datasets for Emotion Recognition 情绪识别多模态数据集</h4><p>多模态情感识别已经吸引了许多研究者的注意。数量有限的包含面部视频、EEG和生理信号的多模态数据集可供下载，用于情绪识别。</p><ul><li>DEAP数据集（Koelstra等人，2011）</li><li>MAHNOB-HCI数据集（Soleymani等人，2011）</li></ul><p>由于EEG信号对肌肉伪影很敏感（Jiang等人，2019年），这类数据集使用了看视频或听音乐等被动任务，以尽量减少主体运动。</p><hr><h5 id="DEAP-Dataset-DEAP数据集"><a href="#DEAP-Dataset-DEAP数据集" class="headerlink" title="DEAP Dataset DEAP数据集"></a>DEAP Dataset DEAP数据集</h5><p>DEAP数据集包含32名参与者的EEG数据、面部视频、GSR、血容量压力（BVP）、温度和呼吸数据。它使用了40个音乐视频来刺激情绪，而EEG数据是使用Biosemi ActiveTwo EEG headset7收集的，它有32个通道。参与者使用自我评估人体模型（SAM）调查表（Bradley和Lang，1994）报告他们的唤醒、价值、支配和喜欢程度。然而，在这个数据集中，只有22名参与者有视频数据，其中4人的一些试验被遗漏。面部视频的照度很低，面部的一些传感器覆盖了部分面部表情。</p><hr><h5 id="MAHNOB-HCI-Dataset-MAHNOB-HCI数据集"><a href="#MAHNOB-HCI-Dataset-MAHNOB-HCI数据集" class="headerlink" title="MAHNOB-HCI Dataset MAHNOB-HCI数据集"></a>MAHNOB-HCI Dataset MAHNOB-HCI数据集</h5><p>在MAHNOB-HCI数据集中，眼球运动、声音、EEG数据和呼吸模式已被收集，用于图像和视频内容的标记。在观看视频片段后，参与者使用价值-唤醒模型报告他们的情绪状态。招募了30名参与者来创建这个数据集。使用了具有32个通道的Biosemi active II headset8来收集脑电图数据。</p><hr><h4 id="Exploring-the-Relationship-Between-Modalities-探讨各种模式之间的关系"><a href="#Exploring-the-Relationship-Between-Modalities-探讨各种模式之间的关系" class="headerlink" title="Exploring the Relationship Between Modalities 探讨各种模式之间的关系"></a>Exploring the Relationship Between Modalities 探讨各种模式之间的关系</h4><p>一些研究专注于多模态情感识别中行为反应和生理变化之间的关系：</p><ul><li>Benlamine等人（2016）和Raheel等人（2019）使用EEG信号来识别面部微表情。</li><li>Hassouneh等人（2020年）使用单模态策略，使用脑电图和面部数据识别身体残疾者或自闭症患者的情绪。虽然他们没有使用多模态策略，但他们表明，使用每个面部表情或EEG信号可以成功识别情绪。在他们的实验数据集中，EEG的准确率达到87.3%，面部微表情的准确率达到99.8%。</li><li>Sun等人（2020年）研究了自发面部表情与脑电图和近红外光谱（fNIRS）测量的大脑活动之间在情绪价值上的强烈关联。</li><li>Soleymani等人（2015）认为，虽然EEG信号对基于面部表情的情绪识别有一些补充信息，但它们不能提高面部表情系统的准确性。然而，后来的研究表明，通过结合脑电图和面部表情，可以改善。</li></ul><hr><h4 id="Fusing-Behavioral-and-Physiological-Modalities-融合行为和生理模式"><a href="#Fusing-Behavioral-and-Physiological-Modalities-融合行为和生理模式" class="headerlink" title="Fusing Behavioral and Physiological Modalities 融合行为和生理模式"></a>Fusing Behavioral and Physiological Modalities 融合行为和生理模式</h4><p>在许多研究中，研究人员表明情绪刺激对生理变化的影响，如心率、体温、皮肤传导、呼吸模式等。然而，他们无法确定哪些情绪被激发了。一些研究表明，将生理情绪识别和行为模式结合起来可以提高识别结果。将面部表情与生理模式相结合吸引了该领域一些研究人员的关注。这些研究大多集中在传统的面部表情方法上，并使用所有录制的视频帧来识别情绪。</p><ul><li>Koelstra和Patras（2013）使用EEG和面部表情的组合来生成视频的情感标签。他们提取了14个左右对的功率谱密度和横向化，并提取了230个EEG数据的特征。他们试图逐帧识别动作单元的激活，最后为每段视频提取了三个特征。他们使用了特征级和决策级的融合策略。根据他们的结果，与单一模式相比，融合策略提高了标签的性能。通过融合EEG和人脸数据，唤醒的准确性从EEG的64.7%和人脸的63.8%提高到70.9%。通过融合情绪值，这一改进从EEG的70.9%和脸部的62.8%提高到73%。</li><li>Huang等人（2017）研究了融合面部宏观表情和EEG信号在决策层面上的情绪识别。他们使用一个前馈网络对每一帧视频中提取的面部的基本情绪进行分类。他们在这项研究中使用了他们的实验数据，在融合EEG和面部表情时，在依赖主体的策略中取得了82.8%的准确性。</li><li>后来，他们通过使用CNN模型改进面部表情识别来扩展他们的工作（Huang等人，2019）。他们使用FER2013数据集（Goodfellow等人，2013）预训练了一个模型，并使用小波提取功率带和SVM对EEG数据进行分类。他们在DEAP数据集上使用多模态方法中的主体依赖策略，对情绪和唤醒的准确率分别达到80%和74%。</li><li>在一项类似的研究中，Zhu等人（2020年）使用加权决策水平融合策略，结合脑电图、外周生理信号和面部表情来识别唤醒-情绪状态。他们使用三维卷积神经网络（CNN）来提取面部特征并进行分类，他们还使用一维CNN来提取EEG特征并进行分类。当将面部表情与EEG和生理信号相结合时，他们取得了更高的准确性。</li><li>Chaparro等人（2018年）还提出了一种特征级融合策略，用于结合EEG和面部特征（使用70个地标坐标）来提高识别结果。</li></ul><p>在大多数多模态情感数据集的记录视频中，在许多帧中无法观察到任何表情。这些数据集使用被动的任务，如观看视频来刺激情绪，所以情绪化的面孔只能在一小部分的帧中看到。因此，在数据分析中考虑所有的帧，或者在不考虑这个问题的情况下使用帧之间的多数票，都不能产生一个好的情感识别结果。然而，在应对这些被动任务时，可以观察到许多微表情。</p><ul><li>只有Huang等人（2016）考虑了中性脸和微妙表情的存在。他们基于局部二进制模式（LBP）策略提取了所有帧的空间-时间特征。然后，他们使用这些特征训练了一个线性核SVM来计算表情百分比特征，并使用这个特征向量进行情绪分类。他们提取了所有的频率和频段，然后使用ANOVA测试来选择这些特征的子集用于EEG。对于面部分类，他们使用K-Nearest-Neighbor（KNN）分类器进行EEG和支持向量机（SVM）。他们表明，决策层的融合策略比单一模式或特征的融合效果更好。他们对情绪和唤醒的准确率分别达到了62.1%和61.8%。</li></ul><img src="/fed573eb/2.webp"><p>表1总结了最新的相关工作。可以看出，数量有限的研究将面部表情与EEG数据结合起来。当训练和测试集中有一些所有参与者的试验时，大多数以前的工作都会评估他们的方法是依赖主体或跨主体的。尽管设计识别未见过的参与者的情绪的一般模型在日常生活中是非常有用的，但只有少数研究使用了独立于主体的方法来设计和评估他们的方法。与依赖主体和跨主体的评价相比，独立于主体的方法的准确性较低，需要更多的研究和探索。另外，尽管一些研究集中于将面部表情与生理信号相结合，但大多数都是基于强烈的面部表情来设计和训练的。而在大多数使用的多模态数据集中，人们不允许表现出激烈的表情。</p><p>这项研究通过研究使用面部微表情策略与EEG和生理信号相结合进行多模态情感识别的最佳方式来解决这一问题，还探讨了如何利用面部微表情来识别面部视频、EEG和生理数据中最具情感的部分。此外，创建了一个新的面部视频、生理信号和EEG信号的数据集，这有助于开发情感识别的稳健模型。还探索了从OpenBCI EEG headset收集的数据的性能和质量，这是一个用于识别情绪的低成本EEG headset。此外，提出了使用多模态数据进行情感识别的策略，并最终对其进行了评估。</p><p>总的来说，这项研究的<strong>主要创新之处</strong>在于<strong>将面部微表情识别与EEG和生理信号相融合</strong>。这项工作的另一个重要贡献是<strong>利用面部微表情来识别中性状态和情绪状态，以提高使用EEG和生理信号的情绪识别</strong>。</p><hr><h3 id="Experimental-Setup-实验设置"><a href="#Experimental-Setup-实验设置" class="headerlink" title="Experimental Setup 实验设置"></a>Experimental Setup 实验设置</h3><p>创建了一个新的多模态数据集，用于使用轻型可穿戴设备和网络摄像头进行情感识别。从大学生和工作人员中招募了23名志愿者（12名女性和11名男性），年龄在21至44岁之间（μ&#x3D;30，σ&#x3D;6）。只针对六种基本情绪中的四种，包括快乐、悲伤、愤怒和恐惧，外加一个中性状态。在观看视频的任务中收集了面部视频、EEG、PPG和GSR信号。使用唤醒-价值模型来测量情绪，自我报告数据也被用作基础真实值。</p><p>（其它详见原文）</p><hr><h3 id="Methodology-方法"><a href="#Methodology-方法" class="headerlink" title="Methodology 方法"></a>Methodology 方法</h3><h4 id="Ground-Truth-Labeling-正确标签"><a href="#Ground-Truth-Labeling-正确标签" class="headerlink" title="Ground Truth Labeling 正确标签"></a>Ground Truth Labeling 正确标签</h4><p>使用来自SAM调查问卷的自我报告数据进行基础真实标记。只使用DEAP和自采数据集的报告的唤醒和价值。为了对唤醒和情感水平进行分类，尽管在SAM问卷中有九个级别的唤醒和情感，但与以前的研究类似，使用二进制分类。认为五个是创建二进制标签的阈值，对应于高和低的唤醒和价值。</p><img src="/fed573eb/3.webp"><p>表3显示了当评分值在1到9之间时，自我报告的唤醒评分和数值的平均值。该表还显示了每个视频片段中报告每种情绪的参与者的百分比。例如，78.9%的参与者对《追求幸福》视频片段报告了幸福，而对这个视频片段只有4.3%报告了恐惧，7.8%报告了中性，8.7%报告了悲伤。可以看出，大多数参与者对所有刺激物都报告了目标情绪。尽管在自我报告问卷中包括了所有的基本情绪，但除了目标情绪列表中的情绪，没有一个参与者报告其他的情绪。因此，没有在本表和评价结果中包括其他情绪。</p><hr><h4 id="Imbalanced-Data-不平衡的数据"><a href="#Imbalanced-Data-不平衡的数据" class="headerlink" title="Imbalanced Data 不平衡的数据"></a>Imbalanced Data 不平衡的数据</h4><p>在DEAP数据集中，所有参与者的低级和高级试验的总数量为339和381，而唤醒的数量为279和444。在数据集中，这些值对于价值类来说是100和130，对于唤醒类来说是94和136，分别是低级和高级。可以看出，这两个数据集在类别之间是不平衡的。另外，使用了一个离开一些对象的策略来分割训练和测试数据。因此，每一组的训练和测试集之间的不平衡状态取决于参与者的评分。使用成本敏感学习（Ling and Sheng, 2008）来处理不平衡的数据。成本敏感型学习在模型训练过程中使用了预测错误的成本。它采用了一种惩罚性的学习算法，提高了少数类的分类错误的成本。使用Scikit-learn库来测量类权重，并在训练模型时使用估计的权重。还使用成本敏感的SVM和RF来处理不平衡的数据。</p><hr><h4 id="Video-Emotion-Recognition-视频情绪识别"><a href="#Video-Emotion-Recognition-视频情绪识别" class="headerlink" title="Video Emotion Recognition 视频情绪识别"></a>Video Emotion Recognition 视频情绪识别</h4><p>在DEAP数据集和自采数据集中，由于EEG信号对肌肉伪影的敏感性，要求参与者在观看视频时保持扑克脸。这个条件与微表情数据集完全相同。在微表情数据集中，参与者被要求在观看视频时抑制他们的表情并保持一张扑克脸，以防止宏观表情污染（Goh等人，2020）。这个条件导致几乎所有的帧都是中性脸，只有真正的情绪会作为微表情泄露出来。</p><p>图4显示了来自DEAP数据集、自采数据集、SMIC数据集（Li等人，2013）和FER2013数据集（Goodfellow等人，2013）的一些试验帧。SMIC数据集是专门为面部微表情的情感识别研究而收集的。正如在所有这些数据集中所看到的，情绪几乎不能被注意到，大多看到的是一张中立的脸。相比之下，在FER2013（Goodfellow等人，2013）和CK+（Lucey等人，2010）等面部宏观表情数据集中，有几组表情强烈的脸（图4）。</p><img src="/fed573eb/4.webp"><p>使用FER2013数据集训练了一个深度卷积神经网络，在所有试验的框架上进行了测试，主要从面部表情识别中得到了中性情绪。该模型有五个卷积层和池化层块，其结构类似于VGG-16（Simonyan和Zisserman，2014），每个块中有一些额外的层。图5显示了模型的结构。FER2013是一个由谷歌图像搜索API自动收集的大规模数据集，已被广泛用于面部情绪识别研究。它包含28,709张训练图像、3,589张验证图像和3,589张测试图像，有七个表情标签：愤怒、厌恶、恐惧、快乐、悲伤、惊讶和中性。对数据进行了预处理，将图像转换为灰度图像，使用Dlib库中的人脸检测模块提取人脸区域，对其进行标准化和大小调整，最后，将其送入深度卷积网络。将未检测到的人脸从训练和测试集中移除，在FER2013测试集数据上达到了85%的准确率。使用训练好的模型从DEAP数据集和自采数据集的每一帧记录的视频中检测情绪。使用训练好的模型，应用同样的预处理步骤，预测每一帧的情绪。</p><img src="/fed573eb/5.webp"><p>表4显示了DEAP和自采数据集的预测结果。可以看出，基于对所有框架情绪的多数投票策略，在DEAP的试验中100%检测到的情绪是中性的，在实验的试验中89.1%是中性的。对于有限的参与者，在所有的试验中，中性脸被错误地预测为悲伤的情绪。</p><img src="/fed573eb/6.webp"><p>这一结果表明，中性脸或有细微或微表情的脸不容易用面部宏观表情方法识别。由于DEAP和自采数据集中录制视频的条件与微表情数据集相同，使用微表情方法来检测这两个数据集中的面部视频表情，并研究其性能。因此，将DEAP和自采数据集中的面部数据视为具有微表情的面部数据，并使用面部微表情策略进行视频-情绪识别。</p><p>使用了一个两步的面部微表情识别策略。首先，使用了一个自动发现策略，根据与试验的第一和最后一帧相比，最大的面部组件的运动，自动找到顶点帧。然后，提取了顶点帧周围的一组帧，并考虑这些帧而不是整个视频进行分类。最后，将提取的序列输入一个三维卷积神经网络。</p><p>为了准备发现微表情的框架，首先，在WIDER FACE数据集（Yang等人，2016）上采用了预训练的YOLO v3网络（Redmon和Farhadi，2018）进行人脸检测。选择WIDER FACE数据集是因为它包含了不同程度的比例、遮挡和姿势的图像，增强了模型学习的特征空间，在任何条件下都有更好的实时性能。然后，按照Van Quang等人（2019）介绍的定点方法来识别每个视频中的顶点帧（有微表情的帧）。在这个定点方法中，首先提取了面部组件周围的十个区域，这些区域的肌肉运动发生得非常频繁。下一步，将视频序列的第一帧视为起始帧，最后一帧视为偏移帧，并计算出这十个区域中每一帧与起始帧和偏移帧之间的绝对像素差异。最后，计算了每一帧的每个像素的平均值。认为具有较高强度差异的帧为顶点帧。认为顶点帧周围的窗口是感兴趣的区域（ROI），在分类步骤中只使用这些帧（图6A）。</p><img src="/fed573eb/7.webp"><p>虽然自采数据集和DEAP数据集的录制视频较长，而且可能包含比面部微表情数据集更多的中性帧，但面部微表情的发现方法仍然可以找到顶点帧。因此，仍然有起始、顶点和偏移帧。在实际起始帧之前和偏移帧之后可能有几个中性帧，但所有这些帧都是一样的，不会影响发现算法的结果。这是因为实际起始帧和第一帧或实际偏移帧和最后一帧几乎相同。因此，顶点帧和实际起始或偏移帧之间测量的绝对像素差异将与顶点帧和第一或最后一帧之间的绝对像素差异实际上是一样的。虽然视频中可能有更多的面部表情和顶点帧，但实际的偏移帧，和最后一帧几乎是相同的，因为这两个帧都描述了面部的中性状态。</p><p>考虑了ROI的不同窗口大小，并在结果部分进行了讨论。图4说明了DEAP和自采数据集的顶点框架周围提取的六帧序列，此外还有SMIC数据集的一个序列。使用三维卷积神经网络（3D CNN）来对微表情序列进行分类。它是微表情情感识别领域最先进的模型之一（Reddy等人，2019），在两个流行的微表情数据集CASME II（Yan等人，2014）和SMIC（Li等人，2013）上取得了良好的表现。该方法在CASME II数据集上取得了87.8%的准确性，在SMIC数据集上取得了68.75%的准确性。用这个模型来提取深度特征并对DEAP和自采数据集中的微表情进行分类。由于这两个数据集的基础事实标签都是基于唤醒和情感水平的，所以没有根据基本的情绪对微表情进行分类，而是根据唤醒和情感水平对微表情进行分类。为了对基于唤醒或价值的情绪状态进行分类，对数据应用了两次模型，一次是对唤醒水平进行分类，一次是对价值水平进行分类。在这个模型中，没有用6作为最后一个密集层的输出形状，而是用2来对基于低和高的唤醒或价值的微表情进行分类。</p><p>首先，使用YOLO人脸检测算法来检测ROI中每一帧的人脸，然后将其转换为灰度图像，归一化，并调整其大小。最后，将预处理过的序列送入Reddy等人（2019）介绍的两个三维CNN模型，用于分别对唤醒和价值进行分类。图6B说明了三维CNN模型的结构。</p><hr><h4 id="EEG-and-Physiological-Emotion-Recognition-脑电图生理情感的识别"><a href="#EEG-and-Physiological-Emotion-Recognition-脑电图生理情感的识别" class="headerlink" title="EEG and Physiological Emotion Recognition 脑电图生理情感的识别"></a>EEG and Physiological Emotion Recognition 脑电图生理情感的识别</h4><p>微表情是识别每个试验中最有情绪的时间的一个指标。然后使用一个基于ROI的策略，使用EEG和生理数据来识别唤醒和价值。顶点框架的时间是每次试验中最有情绪的时间。然后，在脑电图和生理学数据中找到这个时间段的相应样本。由于脑电图、生理数据和视频帧之间的采样率不同，将此时每个信号的采样率相乘来确定ROI。最后，在其周围提取了几秒钟的数据，将提取的部分视为ROI，并只对提取的数据进行分析。考虑了不同的窗口大小来提取ROI，并在结果部分进行了讨论。</p><p>为了分析EEG和生理数据，遵循情绪识别的主要步骤：预处理、特征提取和分类。首先，清理了数据，然后提取了ROI部分，并且只使用ROI数据作为特征提取步骤的输入。为了对数据进行分类，使用了两种方法对EEG和生理学数据进行分类。在第一种方法中，从整个数据或ROI部分提取了一些特征————在下面的章节中描述。用这些特征作为支持向量机（SVM）、K-近邻（KNN）（Bressan和Vitria，2003）和随机森林（RF）（Criminisi等人，2011）分类器的输入。在第二种方法中，首先，将每个试验划分为不重叠的窗口。然后从每个窗口中提取与前一种方法相同的特征，并制成一个后果特征向量的序列。用这些序列作为堆叠的长短期记忆（LSTM）网络（Staudemeyer和Morris，2019）的输入，用两层LSTM提取时间特征。最后，用一个带有Adam优化器的密集层（Kingma和Ba，2014）来分别对数据进行唤醒和价值标签分类。图6C显示了EEG和生理数据分析的整体结构。</p><hr><h4 id="Data-Cleaning-数据清洗"><a href="#Data-Cleaning-数据清洗" class="headerlink" title="Data Cleaning 数据清洗"></a>Data Cleaning 数据清洗</h4><h5 id="EEG"><a href="#EEG" class="headerlink" title="EEG"></a>EEG</h5><p>使用DEAP数据集中的预处理过的EEG数据，去除前8s的数据，包括3s的基线，并将5s作为参与时间，最后将数据归一化。参与时间是通过观察选择的，是参与者沉浸在视频中的平均时间。对于自采数据集，使用带通滤波器，提取1到45赫兹之间的频率，这是脑波的频率范围（Huang等人，2016）。然后应用一个共同的平均参考，最后，将数据归一化。图7A,B显示了数据清理前后的脑电图通道的频率。</p><hr><h5 id="PPG-and-GSR"><a href="#PPG-and-GSR" class="headerlink" title="PPG and GSR"></a>PPG and GSR</h5><p>一个低切频率为0.7赫兹、高切频率为2.5赫兹的带通滤波器被用来去除PPG信号的噪声。同样地，用0.1的低切频率和15赫兹的高切频率来清除GSR信号。还使用了一个中值滤波器来去除GSR信号中的快速瞬态伪影。最后，将这些GSR和PPG信号归一化。图7C-F显示了数据清理前后的GSR和PPG信号的一个样本的振幅。</p><hr><h4 id="Feature-Extraction-特征提取"><a href="#Feature-Extraction-特征提取" class="headerlink" title="Feature Extraction 特征提取"></a>Feature Extraction 特征提取</h4><h5 id="EEG-1"><a href="#EEG-1" class="headerlink" title="EEG"></a>EEG</h5><p>为了提取脑电特征，对每个数据窗口应用快速傅里叶变换（FFT）来提取脑电波段功率。通过从每个窗口中提取脑电功率波段，并将每个波段的平均值视为一个特征向量，从而形成五个特征。提取了Delta（1-4 HZ）、Theta（4-8 HZ）、Alpha（8-12 HZ）、Beta（12-30 HZ）和Gamma（30-45）带。这些特征通常在以前的研究中使用（Wagh和Vasanth，2019）。</p><hr><h5 id="PPG-and-GSR-1"><a href="#PPG-and-GSR-1" class="headerlink" title="PPG and GSR"></a>PPG and GSR</h5><p>计算了GSR和PPG信号的一些统计特征。GSR信号的平均值和标准差以及GSR信号的一阶和二阶离散差构成了GSR特征向量。为了建立PPG特征向量，考虑了PPG信号的平均和标准偏差。PPG和GSR特征向量具有相似的特征，因此将这两个特征向量串联起来，并将其称为生理数据。</p><hr><h4 id="Fusion-Strategy-融合策略"><a href="#Fusion-Strategy-融合策略" class="headerlink" title="Fusion Strategy 融合策略"></a>Fusion Strategy 融合策略</h4><p>有几种方法来融合来自不同来源的数据。融合数据主要有两种方式：（1）特征级或早期融合；（2）决策级融合或后期融合（Shu等人，2018）。</p><p>在特征层面上融合了PPG和GSR信号，将创建的特征作为生理特征处理，并对其进行分类。在决策层使用了两种不同的策略来融合面部微表情、EEG和生理学分类结果。第一个策略是基于多数投票，在脑电图、面部和生理预测中选择得票最多的预测作为最终预测。在第二种策略中，使用所有概率的加权和作为决策层的融合策略（Koelstra和Patras，2013；Huang等人，2017）。给这三个分类器在[0，1]的范围内以0.01的步长赋予各种权重，在训练数据上测量最佳权重，并在融合步骤中使用这些权重。方程（1）中，px模态表示每个类别使用特定模态的概率，a、b、c是权重。</p><img src="/fed573eb/8.webp"><hr><h3 id="Result-and-Discussion-结果和讨论"><a href="#Result-and-Discussion-结果和讨论" class="headerlink" title="Result and Discussion 结果和讨论"></a>Result and Discussion 结果和讨论</h3><h4 id="Evaluation-Strategy-评估策略"><a href="#Evaluation-Strategy-评估策略" class="headerlink" title="Evaluation Strategy 评估策略"></a>Evaluation Strategy 评估策略</h4><p>使用了独立于主体的策略来评估本文方法，并找到一个通用的模型。使用了离开部分主体的策略交叉验证。由于本文模型并不复杂，而且数据集的大小也不明显，所以没有使用GPU来训练模型。所有的模型都是在一台装有Gnu-Linux Ubuntu 18.04、英特尔（R）酷睿（TM）i7-8700K CPU（3.70 GHz）的电脑上训练的，有六个内核。将参与者随机洗牌成六个组，并对所有组的模型进行并行训练。对于DEAP数据集，每个折叠中的测试集都有3个参与者。在自采数据集中，有4个参与者被考虑到测试集中。报告的结果是所有组结果的平均值。</p><p>评估模型的四个主要指标是准确性、精确性、召回率和F-Score或F1。它们使用方程（2）来衡量二进制分类。在本节中，所有的结果都是基于F-Score的。在这些公式中，TP是真阳性，指的是正确的阳性类预测的数量。真阴性（TN）衡量有多少正确的阴性预测。假阳性是指错误预测的阳性类的数量。FN代表假阴性，即错误的阴性类预测的数量。使用二元分类法对唤醒和情绪分别进行分类，并选择F-Score来评估本文方法，这适合于不平衡数据。(Sun et al., 2009)。</p><img src="/fed573eb/9.webp"><hr><h4 id="Hyper-Parameter-Tuning-超参数调优"><a href="#Hyper-Parameter-Tuning-超参数调优" class="headerlink" title="Hyper-Parameter Tuning 超参数调优"></a>Hyper-Parameter Tuning 超参数调优</h4><p>为了衡量SVM、RF和KNN的最佳超参数，使用了网格搜索交叉验证的参数调整（Claesen和De Moor，2015）。当基于离开部分受试者策略分割数据时，使用六倍交叉验证法调整超参数。当考虑Radial Basis Function（RBF）核时，得到了最好的结果，200作为SVM的调节参数，KNN的五个邻居，RF的500个估计值。对于微观面部表情的三维卷积模型，使用的参数与源研究（Reddy等人，2019）相同。只将epochs的数量设置为50。还根据经验发现，当第一个LSTM有80个神经元，第二个有30个神经元时，使用两个堆叠的LSTM会产生更好的结果。考虑将128、32和64作为脑电图、GSR和PPG分类器的LSTM模型训练中的批次大小，并将它们的历时数设置为100。没有调整学习率。相反，在0.0010.0001的范围内使用了一个降低的学习率，当验证损失不发生变化时，学习率以0.5的速度递减。</p><hr><h4 id="Identifying-ROI-Size-确定ROI大小"><a href="#Identifying-ROI-Size-确定ROI大小" class="headerlink" title="Identifying ROI Size 确定ROI大小"></a>Identifying ROI Size 确定ROI大小</h4><p>微表情的持续时间在65到500毫秒之间变化。当情绪持续一段时间后，这个时间可能会增加，也可能与下一个微表情合并，而这个微表情是后续情绪刺激的反应（Yan等人，2013）。DEAP数据集以每秒50帧的速度记录面部数据。这意味着，如果认为一个微表情的长度为半秒，当帧率为50赫兹时，一个微表情会出现在25帧中。在自采数据集中，帧率是每秒30帧，所以一个微表情的长度是15帧。考虑了两种不同的窗口大小，包括20和60帧，围绕着顶点帧，以涵盖短的微表情或长的微表情。考虑了更大的窗口尺寸，以覆盖保持较长时间或与下一个微表情重叠的微表情。表5比较了这两种窗口大小对预测结果的影响，当想根据唤醒和价值水平对情绪进行分类。可以看出，对于两个数据集，60帧的结果都比较好。由于增加窗口大小会增加包括其他头部运动的概率，在序列中增加非信息数据，并增加计算成本，没有考虑更大的窗口大小。表5显示了来自DEAP和自采数据集的3D CNN模型在这两种不同窗口大小下的f-score。使用面部微表情分类的预测结果与决策层面的其他模式相结合，对唤醒和价值水平进行分类。</p><p>考虑了从EEG和生理数据中提取ROI的各种尺寸，并比较了ROI尺寸对分类结果的影响。图8显示了使用LSTM方法时各种ROI大小对分类结果的影响。报告的数值是所有折叠的F-Score值的平均值。如图8所示，对于两个数据集，使用多数融合时，15的窗口大小几乎创造了最高的F-Score。对于DEAP数据集，当考虑所有的数据时，加权融合创造了预测唤醒的最佳结果。尽管在这里显示的两个不同的数据集中没有看到任何一致的模式，但假设在最情绪化的部分的一小部分数据可以产生一个类似或比使用所有数据更好的结果。这表明，如果准确地确定数据中最情绪化的部分，可以准确地研究大脑和身体对情绪刺激的反应。</p><p>还使用SVM、KNN和RF分类器对窗口大小为15和考虑整个数据时的ROI部分进行分类。在表6中比较了这些分类器与LSTM方法在所有数据或只考虑ROI部分进行分类时的F-Score。表5中报告的窗口大小为60的面部微表情的F-Score已被考虑在融合策略中。将面部微表情方法的预测结果与使用的所有分类器进行了融合。从这些表格中可以看出，LSTM方法在两个数据集的唤醒和价值方面都取得了最佳结果。这表明，利用时间和空间特征可以帮助检测情绪。同时，融合策略的结果也大大优于单一模式。在自采数据集中，大多数人对唤醒和情感的融合进行投票，而在DEAP中只有情感的融合优于加权的融合。当应用于ROI数据时，结合PPG和GSR只提高了LSTM方法在价值水平分类中的性能。另外，基于ROI的LSTM的FScore相对接近于或有时优于在整个数据上使用LSTM。这表明，使用一小部分数据可以像使用全部数据一样具有信息量。</p><p>虽然其他分类器在其中一个数据集中的唤醒或情感的某些模式产生了良好的结果，但与LSTM方法相比，他们的预测在与其他模式融合后并没有改善。当预测精度低于或接近随机预测或二元分类的50%时，它无法在数据中找到任何特定的模式。因此，单模态预测之间的不匹配性增加，导致融合策略的f-score下降。根据表6，SVM、KNN或RFC对某些模式的f-scores低于或接近50%。因此，这导致了无效的融合。</p><hr><h4 id="Computation-Cost-计算成本"><a href="#Computation-Cost-计算成本" class="headerlink" title="Computation Cost 计算成本"></a>Computation Cost 计算成本</h4><p>没有使用所有的帧作为3D卷积模型的输入，而是只使用了每个视频的60个帧作为模型的输入。DEAP数据集在每个视频中有3,000帧，而自采数据集在每个视频中有2,400帧。通过提取微表情的ROI，减少了DEAP的输入大小，比率为（60&#x2F;3000），自采数据集为（60&#x2F;2400）。输入规模的下降导致了计算成本的大幅下降。自采数据集有230（23 * 10）次试验，而DEAP的数据集有720（18 * 40）次试验，适用于所有参与者。两个数据集的人脸模型的输入都是（60 * 64 * 64），其中60是每个试验的帧数，64 * 64是人脸区域灰度的帧的尺寸。对DEAP数据集的六折人脸模型进行并行训练花了1小时37分钟（每个历时235秒）。由于每个参与者的试验次数较少，自采数据集的训练时间为33分钟（每个震荡期为79秒）。</p><p>此外，尽管以前的研究使用LSTM网络对EEG信号进行分类，并将原始信号作为网络的输入（Ma等人，2019年），但从每秒钟的数据中提取了有限的特征，以减少输入规模。创建了一个新的数据序列，该序列比原始数据小得多，同时仍然具有信息量。例如，对于EEG数据，每个试验的大小是（持续时间（秒） * 采样率 * 通道）。将这一大小减少到（持续时间（秒） * 五个功率段）。这种减少对生理学数据也是一样的。脑电图、PPG和GSR的LSTM模型的训练是以六倍的方式平行进行的。为DEAP数据集训练所有这些模型需要12分钟和14秒，而每个历时大约需要1-3秒。自采数据集的训练时间为5分钟，每个历时的运行时间在25到100ms之间。</p><hr><h4 id="Final-Result-最终结果"><a href="#Final-Result-最终结果" class="headerlink" title="Final Result 最终结果"></a>Final Result 最终结果</h4><p>表7显示了当ROI窗口大小为15秒时，单一模式或融合策略对ROI部分进行分类的最终结果。可以看出，在两个数据集中，将微表情与EEG和生理信号融合，比使用单一模式有更高的准确性和F-Score。与使用独立于主体的策略的相关作品相比，在识别唤醒和情绪水平方面取得了类似或更好的准确性。</p><img src="/fed573eb/10.webp"><p>没有任何标准的基准来评估各种情感识别研究。有多个数据集，在数据收集方面有不同的场景，使用不同的模式和传感器记录情感数据。数据集、情感模型、分割数据的方式、评估策略和评估指标的多样性会影响最终的情感识别结果。出于这个原因，应该考虑所有这些因素来比较各种研究。与表1中报告的前人工作相比，在考虑独立于主体的方法时，所提出的方法的准确性相当高，这是最具挑战性的评价条件。</p><p>虽然检测面部微表情在文献中仍是一个很大的挑战，需要更多的探索，但已经证明，它可以大大降低视频情感识别的计算成本。检测微表情存在一些影响情绪识别性能的挑战，包括与其他面部动作的污染、姿势变化、光照不足，以及伪造或摆放微表情的可能性（Zhao and Li, 2019）。在DEAP数据集和自采数据集中，由于扑克脸的条件，伪造微表情的机会很低。然而，有一些不必要的动作会影响检测微表情和识别感兴趣区域的结果。</p><p>低成本的OpenBCI脑电图帽可以达到与DEAP数据集中使用的Biosemi Active II帽类似的性能。结果表明，虽然这个工具成本很低，但它可以作为一个可靠的工具，用于收集大脑信号进行情感识别。</p><p>与之前的研究类似，结果显示结合各种模式会带来更好的识别结果，融合后的识别率提高了38%。在DEAP数据集中，对唤醒的准确率达到65.1%，对情感的准确率达到69.2%，这比单一模式要好。在自采数据集中，这些相应的数值为70.8％和69.2％的唤醒和情感。表7显示了这些改进。虽然采用多模态数据有一些缺点，如增加计算成本和数据分析的复杂性，但提高预测性能的好处超过了它们。现在，大多数处理系统都有多个核心，使并行处理变得容易。可以在几乎与单模态分析相同的时间内利用并行处理进行多模态数据分析。</p><hr><h3 id="Limitations-局限性"><a href="#Limitations-局限性" class="headerlink" title="Limitations 局限性"></a>Limitations 局限性</h3><p>尽管显示面部微表情可以有效地识别情绪，但仍面临着一些挑战，应该在未来解决。由于不自主的面部运动，如眨眼、头部运动或常规的面部表情，微表情可能被错误地检测到（Tran等人，2020）。这些运动会导致对顶点框架的错误检测。在未来，可以通过引入新的面部微表情数据集和使用深度学习方法来大大改善发现策略的结果。在本文中，使用了一个简单的传统微表情识别策略来检测顶点框架。在情感识别中，面部微表情可以与其他模式相结合。在未来，希望使用更强大的定点策略来提高识别质量。</p><p>此外，面部微表情方法面临着与面部宏观表情类似的挑战，包括光照条件、文化多样性、性别和年龄。这些限制可以通过使用新的数据集和更强大的深度学习方法来克服。此外，将面部微表情与生理信号相结合，将改善识别结果。对于大多数人来说，脑电图headset和生理传感器并不像相机那样容易获得。现在比以往任何时候都更接近开发情感识别的稳健模型，因为越来越多价格低廉的可穿戴设备，如智能手表、活动追踪器和VR头盔都配备了生理传感器。可以通过引入更准确、可穿戴和可负担的EEG传感器，以及开发更稳健的生理情绪识别算法来实现这一目标。</p><p>在研究中，使用了一个定点策略来检测顶点框架。由于定点方法仍然需要更多的探索，是一个开放的挑战（Oh等人，2018），可以在未来通过手动注释DEAP和自采数据集来改善结果。手动注释这些数据集是一项劳动密集型和耗时的活动。尽管如此，由于它们是在类似于微表达数据集的条件下收集的，可以将它们作为微表达数据集，用于制作更强大的微表达模型。</p><hr><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><div class="pdfobject-container" data-target="./file/paper/2022-Using-Facial-Micro-Expressions-in-Combination-With-EEG-and-Physiological-Signals-for-Emotion-Recognition.pdf" data-height="500px"></div><hr></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/3b4d60e3.html" rel="prev" title="深度学习-图像处理特征融合相关延伸"><i class="fa fa-chevron-left"></i> 深度学习-图像处理特征融合相关延伸</a></div><div class="post-nav-item"><a href="/f8f3424b.html" rel="next" title="UCB CS61A: Computer Programs [Fall 2020] Lecture 5 Environments">UCB CS61A: Computer Programs [Fall 2020] Lecture 5 Environments <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">Abstract 摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction-%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">Introduction 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recognizing-Spontaneous-Emotions-%E8%AF%86%E5%88%AB%E8%87%AA%E5%8F%91%E7%9A%84%E6%83%85%E7%BB%AA"><span class="nav-number">2.1.</span> <span class="nav-text">Recognizing Spontaneous Emotions 识别自发的情绪</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Extracting-Facial-Micro-Expressions-From-Faces-%E4%BB%8E%E8%84%B8%E9%83%A8%E6%8F%90%E5%8F%96%E9%9D%A2%E9%83%A8%E5%BE%AE%E8%A1%A8%E6%83%85"><span class="nav-number">2.1.1.</span> <span class="nav-text">Extracting Facial Micro-Expressions From Faces 从脸部提取面部微表情</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Using-Physiological-Signals-That-Cannot-Be-Faked-%E4%BD%BF%E7%94%A8%E6%97%A0%E6%B3%95%E4%BC%AA%E9%80%A0%E7%9A%84%E7%94%9F%E7%90%86%E4%BF%A1%E5%8F%B7"><span class="nav-number">2.1.2.</span> <span class="nav-text">Using Physiological Signals That Cannot Be Faked 使用无法伪造的生理信号</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Using-a-Combination-of-Various-Input-Modalities-%E4%BD%BF%E7%94%A8%E5%90%84%E7%A7%8D%E8%BE%93%E5%85%A5%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BB%84%E5%90%88"><span class="nav-number">2.1.3.</span> <span class="nav-text">Using a Combination of Various Input Modalities 使用各种输入模式的组合</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Goals-Overview-and-Contributions-%E7%9B%AE%E7%9A%84%E3%80%81%E6%80%BB%E8%A7%88%E4%B8%8E%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.2.</span> <span class="nav-text">Goals, Overview, and Contributions 目的、总览与贡献</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusions-%E7%BB%93%E8%AE%BA"><span class="nav-number">3.</span> <span class="nav-text">Conclusions 结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Preliminaries-%E5%89%8D%E8%A8%80"><span class="nav-number">4.</span> <span class="nav-text">Preliminaries 前言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Emotion-Models-%E6%83%85%E7%BB%AA%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">Emotion Models 情绪模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Emotion-Stimulation-Methods-%E6%83%85%E7%BB%AA%E5%88%BA%E6%BF%80%E6%96%B9%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">Emotion Stimulation Methods 情绪刺激方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Facial-Micro-Expressions-%E9%9D%A2%E9%83%A8%E5%BE%AE%E8%A1%A8%E6%83%85"><span class="nav-number">4.3.</span> <span class="nav-text">Facial Micro Expressions 面部微表情</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Electroencephalography-EEG-Signals-%E8%84%91%E7%94%B5%E5%9B%BE%E4%BF%A1%E5%8F%B7"><span class="nav-number">4.4.</span> <span class="nav-text">Electroencephalography (EEG) Signals 脑电图信号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Galvanic-Skin-Responses-GSR-Signals-%E7%9A%AE%E8%82%A4%E7%94%B5%E5%8F%8D%E5%BA%94%E4%BF%A1%E5%8F%B7"><span class="nav-number">4.5.</span> <span class="nav-text">Galvanic Skin Responses (GSR) Signals 皮肤电反应信号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Photoplethysmography-PPG-Signals-%E5%85%89%E8%B0%B1%E5%88%86%E6%9E%90%E4%BF%A1%E5%8F%B7"><span class="nav-number">4.6.</span> <span class="nav-text">Photoplethysmography (PPG) Signals 光谱分析信号</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Related-Works-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">5.</span> <span class="nav-text">Related Works 相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multimodal-Datasets-for-Emotion-Recognition-%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.1.</span> <span class="nav-text">Multimodal Datasets for Emotion Recognition 情绪识别多模态数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DEAP-Dataset-DEAP%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.1.1.</span> <span class="nav-text">DEAP Dataset DEAP数据集</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MAHNOB-HCI-Dataset-MAHNOB-HCI%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.1.2.</span> <span class="nav-text">MAHNOB-HCI Dataset MAHNOB-HCI数据集</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-the-Relationship-Between-Modalities-%E6%8E%A2%E8%AE%A8%E5%90%84%E7%A7%8D%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">5.2.</span> <span class="nav-text">Exploring the Relationship Between Modalities 探讨各种模式之间的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fusing-Behavioral-and-Physiological-Modalities-%E8%9E%8D%E5%90%88%E8%A1%8C%E4%B8%BA%E5%92%8C%E7%94%9F%E7%90%86%E6%A8%A1%E5%BC%8F"><span class="nav-number">5.3.</span> <span class="nav-text">Fusing Behavioral and Physiological Modalities 融合行为和生理模式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experimental-Setup-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="nav-number">6.</span> <span class="nav-text">Experimental Setup 实验设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Methodology-%E6%96%B9%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">Methodology 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ground-Truth-Labeling-%E6%AD%A3%E7%A1%AE%E6%A0%87%E7%AD%BE"><span class="nav-number">7.1.</span> <span class="nav-text">Ground Truth Labeling 正确标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Imbalanced-Data-%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">7.2.</span> <span class="nav-text">Imbalanced Data 不平衡的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Video-Emotion-Recognition-%E8%A7%86%E9%A2%91%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB"><span class="nav-number">7.3.</span> <span class="nav-text">Video Emotion Recognition 视频情绪识别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EEG-and-Physiological-Emotion-Recognition-%E8%84%91%E7%94%B5%E5%9B%BE%E7%94%9F%E7%90%86%E6%83%85%E6%84%9F%E7%9A%84%E8%AF%86%E5%88%AB"><span class="nav-number">7.4.</span> <span class="nav-text">EEG and Physiological Emotion Recognition 脑电图生理情感的识别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Cleaning-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="nav-number">7.5.</span> <span class="nav-text">Data Cleaning 数据清洗</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#EEG"><span class="nav-number">7.5.1.</span> <span class="nav-text">EEG</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PPG-and-GSR"><span class="nav-number">7.5.2.</span> <span class="nav-text">PPG and GSR</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Extraction-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="nav-number">7.6.</span> <span class="nav-text">Feature Extraction 特征提取</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#EEG-1"><span class="nav-number">7.6.1.</span> <span class="nav-text">EEG</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PPG-and-GSR-1"><span class="nav-number">7.6.2.</span> <span class="nav-text">PPG and GSR</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fusion-Strategy-%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-number">7.7.</span> <span class="nav-text">Fusion Strategy 融合策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Result-and-Discussion-%E7%BB%93%E6%9E%9C%E5%92%8C%E8%AE%A8%E8%AE%BA"><span class="nav-number">8.</span> <span class="nav-text">Result and Discussion 结果和讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluation-Strategy-%E8%AF%84%E4%BC%B0%E7%AD%96%E7%95%A5"><span class="nav-number">8.1.</span> <span class="nav-text">Evaluation Strategy 评估策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyper-Parameter-Tuning-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="nav-number">8.2.</span> <span class="nav-text">Hyper-Parameter Tuning 超参数调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Identifying-ROI-Size-%E7%A1%AE%E5%AE%9AROI%E5%A4%A7%E5%B0%8F"><span class="nav-number">8.3.</span> <span class="nav-text">Identifying ROI Size 确定ROI大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computation-Cost-%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC"><span class="nav-number">8.4.</span> <span class="nav-text">Computation Cost 计算成本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Final-Result-%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C"><span class="nav-number">8.5.</span> <span class="nav-text">Final Result 最终结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitations-%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">9.</span> <span class="nav-text">Limitations 局限性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">10.</span> <span class="nav-text">原文</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1023</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">4m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">164:56</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>