<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文将分 3 期进行连载，共介绍 15 个在图像分类任务上曾取得 SOTA 的经典模型。  第 1 期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt 第 2 期：DenseNet、MobileNet、SENet、DPN、IGC V1 第 3 期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFN"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-EfficientNet、ShuffleNet、NFNet…你都掌握了吗？一文总结图像分类必备经典模型（三）"><meta property="og:url" content="https://aisakaaoi.github.io/1d840294.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本文将分 3 期进行连载，共介绍 15 个在图像分类任务上曾取得 SOTA 的经典模型。  第 1 期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt 第 2 期：DenseNet、MobileNet、SENet、DPN、IGC V1 第 3 期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFN"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/20.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/23.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/24.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/25.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/26.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/27.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/28.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/29.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/30.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/31.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/32.webp"><meta property="og:image" content="https://aisakaaoi.github.io/1d840294/33.webp"><meta property="article:published_time" content="2022-12-02T20:53:34.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:47.659Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/1d840294/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/1d840294.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度学习-EfficientNet、ShuffleNet、NFNet…你都掌握了吗？一文总结图像分类必备经典模型（三） | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1017</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/1d840294.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习-EfficientNet、ShuffleNet、NFNet…你都掌握了吗？一文总结图像分类必备经典模型（三）</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-12-03 04:53:34" itemprop="dateCreated datePublished" datetime="2022-12-03T04:53:34+08:00">2022-12-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span></span><span id="/1d840294.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-EfficientNet、ShuffleNet、NFNet…你都掌握了吗？一文总结图像分类必备经典模型（三）" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/1d840294.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/1d840294.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>14k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>36 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文将分 3 期进行连载，共介绍 15 个在图像分类任务上曾取得 SOTA 的经典模型。</p><ul><li>第 1 期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt</li><li>第 2 期：DenseNet、MobileNet、SENet、DPN、IGC V1</li><li>第 3 期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFNet</li></ul><p>您正在阅读的是其中的第 3 期。前往 SOTA！模型资源站（sota.jiqizhixin.com）即可获取本文中包含的模型实现代码、预训练模型及 API 等资源。</p><span id="more"></span><hr><p>本期收录模型速览</p><table><thead><tr><th align="left">模型</th><th align="left">SOTA！模型资源站收录情况</th><th align="left">模型来源论文</th></tr></thead><tbody><tr><td align="left">Residual Attention Network</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/60c3b7ac-f424-476c-8373-2727af52d472">https://sota.jiqizhixin.com/models/models/60c3b7ac-f424-476c-8373-2727af52d472</a></td><td align="left">Residual Attention Network for Image Classification</td></tr><tr><td align="left">ShuffleNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/445701e7-940d-4905-bbce-cf47377adfc7">https://sota.jiqizhixin.com/models/models/445701e7-940d-4905-bbce-cf47377adfc7</a></td><td align="left">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</td></tr><tr><td align="left">MnasNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/ca934188-fc29-435d-b98a-814b1d0592d7">https://sota.jiqizhixin.com/models/models/ca934188-fc29-435d-b98a-814b1d0592d7</a></td><td align="left">MnasNet: Platform-Aware Neural Architecture Search for Mobile</td></tr><tr><td align="left">EfficientNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/4fcc29d1-e919-4b62-accc-6d9ba00c1591">https://sota.jiqizhixin.com/models/models/4fcc29d1-e919-4b62-accc-6d9ba00c1591</a></td><td align="left">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</td></tr><tr><td align="left">NFNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/2f7b43e7-1db3-48b9-8f39-0bd5244c367b">https://sota.jiqizhixin.com/models/models/2f7b43e7-1db3-48b9-8f39-0bd5244c367b</a></td><td align="left">High-Performance Large-Scale Image Recognition Without Normalization</td></tr></tbody></table><p>图像分类是计算机视觉领域最经典的任务之一，目的是将输入的图像对应到预定义的语义类别中，即打上类别标签。传统的图像分类方法由底层特征学习、特征编码、空间约束、分类器设计、模型融合等步骤组成。</p><p>首先，从图像中提取特征，经典的特征提取方法包括 HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)、SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换)等，也可以将多种特征融合以保留更多有用信息。然后，对特征进行编码后去除冗余和噪声，生成特征编码，经典方法包括稀疏编码、局部线性约束编码、Fisher 向量编码等。再然后，经过空间特征约束后实现特征汇聚，例如经典的金字塔特征匹配方法。最后，利用分类器进行分类，经典分类器包括 SVM、随机森林等等。</p><img src="/1d840294/1.webp"><p>Alex Krizhevsky 在 2012 年 ILSVRC 提出的 CNN 模型首次将深度学习应用于大规模图像分类任务，其效果远超传统的图像分类方法，一举获得 ILSVRC 2012 的冠军，开启了深度学习模型在图像分类中的应用历程。这个模型就是著名的 AlexNet。自此，图像分类改变了传统的致力于提取有效的特征、改进分类器的有效性的研究和应用路径，转为研究不同的深度学习模型架构。</p><p>深度卷积神经网络为图像分类带来了一系列突破。深度神经网络集成了低&#x2F;中&#x2F;高层次的特征，可以被端到端训练，特征的层次可以通过网络的深度来丰富。甚至有研究人员称，正是在图像分类任务中应用的巨大成功，带动了深度学习的方法。</p><hr><h3 id="Residual-Attention-Network"><a href="#Residual-Attention-Network" class="headerlink" title="Residual Attention Network"></a>Residual Attention Network</h3><p>本文是CVPR 2017的文章，提出了<strong>残差注意力网络（Residual attention network），这是一种使用注意力机制的卷积神经网络，可以以端到端的训练方式与最新的前馈网络体系结构结合。</strong>残差注意力网络是通过堆叠注意力模块构建的，这些模块会生成注意力感知功能。每个注意力模块被分为两个分支：掩码分支和主干分支。主干分支进行特征处理，可以适应任何先进的网络结构。随着模块的深入，来自不同模块的注意力感知功能会自适应地变化。在每个注意模块内，自下而上、自上而下的前馈结构用于将前馈展开并将反馈注意过程反馈到单个前馈过程中。作者提出了注意力残差学习以训练非常深的残差注意力网络，该网络可以轻松扩展到数百层。</p><p>在这项工作中，作者使用 pre-activation Residual Unit、ResNeXt和Inception作为残差注意力网络的基本单元来构建注意力模块。给定主干分支的输出 T(x) 与输入 x，掩码分支使用自下而上、自上而下的结构来学习相同大小的掩码 M(x)。自下而上、自上而下的结构模仿了快速前馈和反馈的注意力过程。输出掩码被用作主干分支的神经元的控制门。Attention module H 的输出是：</p><img src="/1d840294/2.webp"><p>其中，i的范围是所有的空间位置，c∈{1，…，C}是通道的索引。整个结构可以进行端到端的训练。</p><p>在注意力模块中，注意力掩码不仅可以作为前向推理过程中的特征选择器，还可以作为反向传播过程中的梯度更新过滤器。在 soft 掩码分支中，输入特征的掩码梯度为：</p><img src="/1d840294/3.webp"><p>其中，θ 是掩码分支参数，φ 是主干分支参数。这一特性使注意力模块对噪声标签具有鲁棒性。掩码分支可以防止错误的梯度（来自噪声标签）来更新主干参数。</p><p>与我们设计的堆叠注意力模块不同的是，有一种简单的方法可以使用单个网络分支来生成 soft 权重掩码，类似于 spatial transformer layer。但是，这些方法在具有挑战性的数据集（例如 ImageNet）上有几个缺点。首先，背景图像混乱、场景复杂且外观变化较大的图像需要通过不同类型的注意力进行建模。在这种情况下，需要使用不同的注意力掩码对来自不同图层的特征进行建模。使用单个掩码分支将需要指数数量的通道来捕获不同因素的所有组合。其次，单个“注意力模块”只能修改一次功能。如果修改在图像的某些部分上失败，则以下网络模块不会再有机会。作者表示，残差注意力网络可以解决上述问题。在注意力模块中，每个主干分支都有其自己的掩码分支，以学习专门针对其特征的注意力。</p><p>但是，很显然如果只是简单的堆叠，效果肯定不会好，作者也解释了一下为什么这样简单的做法效果不好：</p><ul><li>掩码的范围是 0-1，重复的相乘会使得特征值逐渐变小</li><li>掩码可能会破坏 trunk branch 的好的特性，比如 ResNet 中的恒等特性。<br>由此，作者提出了 Attention Residual Learning。如果 soft 掩码单元可以被构造成相同的映射，那么其性能应该不会比没有注意力的对应单元差。因此，将注意力模块的输出 H 修改为：</li></ul><img src="/1d840294/4.webp"><p>M(x) 的范围是 [0, 1]，M(x) 近似于 0，H(x) 将近似于原始特征 F(x)。作者称这种方法为注意残差学习。</p><p>掩码分支包含快速前馈扫描和自上而下的反馈步骤。前一种操作快速收集整个图像的全局信息，后一种操作将全局信息与原始特征图结合在一起。在卷积神经网络中，这两个步骤展开为自下而上、自上而下、完全卷积的结构。从输入开始，执行几次 max-pooling，以在少量 Residual Units 后迅速增加感受野。达到最低分辨率后，全局信息将通过对称的自上而下的体系结构进行扩展，以指导每个位置的输入特征。在 Residual Units 之后对输出进行线性插值上采样。双线性插值的数量与 max-pooling 的数量相同，以使输出大小与输入特征图相同。然后，在两个连续的 1x1 卷积层之后，sigmoid型层将输出范围归一化为 [0, 1] 。作者还在自下而上和自上而下的部件之间添加了跳接，以捕获不同比例的信息。完整的模块如下图所示。使用三个超参数来设计注意力模块：p,t,r 。超参数 p 表示在分解为主干分支和掩码分支之前的预处理 Residual Units数。t 表示主干分支中的 Residual Units 数。r 表示掩码分支中相邻池化层之间的Residual Units 数。一个 Attention 模块是一个 stage。</p><img src="/1d840294/5.webp"><div align="center">图1 用于ImageNet的网络结构实例</div><p>最后，掩码分支提供的注意力会随着主干分支功能而适应性地变化。但是，仍然可以通过在 soft 掩码输出之前更改激活函数中的标准化步骤来将对注意力的约束添加到掩码分支中。作者引入了三种类型的激活函数，分别对应混合注意力、通道注意力和空间注意力。没有额外限制的混合注意力 f1 对每个通道和空间位置使用简单的 Sigmoid 型。通道注意力 f2 对每个空间位置在所有通道内执行 L2 归一化，以删除空间信息。空间注意力 f3 在每个通道的特征图中执行归一化，然后再进行 Sigmoid 形变换以获得仅与空间信息有关的 soft 掩码。</p><img src="/1d840294/6.webp"><p>其中，i 的范围是所有的空间位置，c 的范围是所有的通道。mean_c 和 std_c 表示第 c个通道的特征图的平均值和标准差。x_i 表示第 i 个空间位置的特征向量。</p><p>当前 SOTA！平台收录 Residual Attention Network 共 15 个模型实现资源，支持的主流框架包含 TensorFlow、PyTorch、CANN 等。</p><img src="/1d840294/7.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">Residual Attention Network</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/60c3b7ac-f424-476c-8373-2727af52d472">https://sota.jiqizhixin.com/models/models/60c3b7ac-f424-476c-8373-2727af52d472</a></td></tr></tbody></table><hr><h3 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h3><p>ShuffleNet 是旷视科技提出的一种计算高效的 CNN 模型，发表在 CVPR 2018，其和 MobileNet 一样主要是想应用在移动端。ShuffleNet 的目标也是通过模型结构设计、利用有限的计算资源来达到最好的模型精度，这需要很好地在速度和精度之间做平衡。<strong>ShuffleNet 的核心是采用了两种操作：pointwise group convolution 和 channel shuffle，在保持精度的同时大大降低了模型的计算量</strong>，即：使用点群（pointwise group）卷积来减少 1×1 卷积的计算复杂度，以及，为了克服点群卷积带来的副作用，引入通道混洗（channel shuffle）操作，以帮助信息流过特征通道。</p><hr><h4 id="Channel-Shuffle-for-Group-Convolutions"><a href="#Channel-Shuffle-for-Group-Convolutions" class="headerlink" title="Channel Shuffle for Group Convolutions"></a>Channel Shuffle for Group Convolutions</h4><p>现代卷积神经网络通常由具有相同结构的重复构建块组成。例如，ResNeXt 将分组卷积引入到构建块中，从而在表示能力和计算成本之间取得了很好的平衡。但是，这种设计没有考虑 1×1 卷积（在 MobileNet 中也称为逐点卷积）的复杂性。例如，在 ResNeXt 中，只有 3×3 层配备了分组卷积。结果，对于 ResNeXt 中的每个残差单元，逐点卷积占据 93.4％ 的乘法加法（如ResNeXt中所建议的基数&#x3D; 32）。在小型网络中，昂贵的逐点卷积导致通道数量有限，无法满足复杂性约束，这可能会严重影响精度。</p><p>为了解决这个问题，可以是在 1×1 层上应用通道稀疏连接，例如分组卷积。通过确保每个卷积仅在相应的输入通道组上运行显着降低了计算成本。但是，如果多个分组卷积堆叠在一起，则会产生一个副作用：某个通道的输出仅从一小部分输入通道派生。图 2（a）说明了两个堆叠的分组卷积层的情况。显然，某个分组的输出仅与该分组内的输入有关。此属性阻止通道组之间的信息流并削弱了表示能力。如果我们允许分组卷积从不同分组中获取输入数据（如图 2b 所示），则输入和输出通道将完全相关。具体来说，对于从上一个分组图层生成的特征图，我们可以先将每个分组中的通道划分为几个子组，然后在下一层中的每个分组中使用不同的子组。图2（c）给出了通道混洗的操作：假设一个具有 g 个组的卷积层，其输出具有 g×n 个信道。首先将输出通道的尺寸调整为 (g, n)，转置后再变平，作为下一层的输入。即使两个卷积具有不同数量的分组，该操作仍然有效。而且，信道混洗也是可微分的，这意味着可以将其嵌入到网络结构中以进行端到端训练。</p><img src="/1d840294/8.webp"><div align="center">图2 用两个堆叠的分组卷积进行通道混洗。GConv代表分组卷积。a）两个堆叠的卷积层，分组数相同。每个输出通道只与组内的输入通道有关。没有交叉对话；b）当GConv2在GConv1之后从不同的分组中获取数据时，输入和输出通道完全相关；c）为与b）使用通道混洗的等价实现</div><hr><h4 id="ShuffleNet-Unit"><a href="#ShuffleNet-Unit" class="headerlink" title="ShuffleNet Unit"></a>ShuffleNet Unit</h4><p>基于上面的设计理念构造 ShuffleNet 的基本单元，如图 3 所示。ShuffleNet 的基本单元是在一个残差单元的基础上改进而成的。如图 3（a）所示，这是一个 3 层的残差单元：首先是 1x1 卷积，然后是 3x3 的 depthwise convolution（DWConv，主要是为了降低计算量），这里的 3x3 卷积是瓶颈层（bottleneck），紧接着是 1x1 卷积，最后是一个 shortcut，将输入直接加到输出上。</p><p>然后，将第一个 1×1 卷积替换为逐点卷积，然后进行通道随机混洗操作，以形成ShuffleNet 单元，如图 3（b）。值得注意的是 3x3 卷积后面没有增加通道混洗，作者认为，对于这样一个残差单元，一个通道混洗操作是足够了。还有就是 3x3 的 depthwise convolution 之后没有使用 ReLU 激活函数。</p><p>对于 ShuffleNet Stride 应用的情况，如图 3（c）所示，在 shortcut 路径上添加 3×3平均池化，这样可以得到与输出一样大小的特征图；用通道级联（concat）替换逐元素加法，这使得扩展通道尺寸变得容易，而额外的计算成本却很少。</p><p>此外，在 ShuffleNet 中，深度卷积仅对瓶颈特征图执行。尽管深度卷积通常理论上的复杂度很低，但作者发现很难在低功率移动设备上高效实现，这可能是由于与其他密集操作相比，计算&#x2F;内存访问比更差。在 ShuffleNet 单元中，仅在瓶颈上使用深度卷积，以尽可能减小开销。</p><img src="/1d840294/9.webp"><div align="center">图3 a）具有深度卷积（DWConv）的瓶颈单元；b）具有逐点卷积（GConv）和通道混洗的ShuffleNet单元；c）stride=2的ShuffleNet单元</div><img src="/1d840294/10.webp"><div align="center">表1 ShuffleNet结构。使用FLOPs评估复杂性。对于阶段2，不在第一层逐点卷积上应用分组卷积，因为输入通道的数量相对较少</div><p>基于上面改进的 ShuffleNet 基本单元，设计的 ShuffleNet 架构如表 1 所示。除了开始使用的普通的 3x3 的卷积和 max pool 层，网络主要由 ShuffleNet 单元堆叠组成，包括三个阶段。每个阶段的第一个构建块都是 stride&#x3D;2，这样特征图 width 和 height各降低一半，而通道数增加一倍，其它超参不变。对于下一个阶段，输出通道加倍，与ResNet 相似，将每个 ShuffleNet 单元的瓶颈通道数设置为输出通道的 1&#x2F;4。</p><p>组号 g 控制了分组卷积中的分组数，分组越多，在相同计算资源下，可以使用更多的通道数。表 8 中给出了不同组数的情况，我们调整了输出通道，以确保整体计算成本大致不变（∼140 MFLOPs）。显然，在给定的复杂度约束下，较大的组数会导致更多的输出通道（因此有更多的卷积过滤器），这有助于编码更多的信息，不过由于相应的输入通道有限，这也可能导致单个卷积过滤器的性能下降。为了将网络定制为所需的复杂度，我们可以简单地在通道数量上应用一个比例因子 s。例如，我们将表 8 中的网络表示为 “ShuffleNet 1×”，那么 “ShuffleNet s×”意味着将 ShuffleNet 1× 中的过滤器数量放大 s 倍，因此整体复杂度大约是 ShuffleNet 1× 的 s^2 倍。</p><p>当前 SOTA！平台收录 ShuffleNet 共 26 个模型实现资源，支持的主流框架包含 TensorFlow、PyTorch、CANN、MXNet 等。</p><img src="/1d840294/11.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">ShuffleNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/445701e7-940d-4905-bbce-cf47377adfc7">https://sota.jiqizhixin.com/models/models/445701e7-940d-4905-bbce-cf47377adfc7</a></td></tr></tbody></table><hr><h3 id="MnasNet"><a href="#MnasNet" class="headerlink" title="MnasNet"></a>MnasNet</h3><p>MnasNet 的文章发表在 CVPR 2019。MnasNet 以及我们接下来介绍的 EfficientNet，还有用于目标检测的 EfficientDet 都是谷歌 AutoML 大佬 Tan Mingxing 的系列化工作，对卷积神经网络的结构进行优化。其中，<strong>MnasNet 利用NAS 方法对卷积网络的基础模块进行搜索，EfficientNet 和 EfficientDet 分析了输入图像分辨率、网络的宽度和深度这三个相互关联的影响网络精度和实时性的因素，对分类网络和检测网络结构进行合理化设计。</strong></p><p>MnasNet 使用<strong>强化学习</strong>的思路，是一种资源约束的终端 CNN 模型的自动神经结构搜索方法。终端轻量化神经网络模型需要同时考虑三个维度：参数少、速度快和精度高。MnasNet 将实时性显式地结合到主要目标中，这样搜索空间就可以识别出一个在准确性和实时性实现良好平衡的模型，即将实时性和准确率作为强化学习中的 reward，并且直接使用手机平台运行模型，直接测量实时性和准确率。MnasNet 总体流程主要包括三个部分，如图 4 所示：一个基于 RNN 的学习和采样模型架构控制器，一个建立和训练模型以获得准确率的训练器，以及一个使用 TensorFlow Lite 测量真实手机上模型速度的推理引擎。</p><img src="/1d840294/12.webp"><div align="center">图4 MnasNet流程</div><p>作者将问题描述为一个考虑 CNN 模型精度和推理实时性的多目标优化问题，旨在实现高准确率和高速。使用架构搜索和强化学习以找到模型，在准确率和实时性间取得平衡，并利用带有定制奖励函数的强化学习算法来寻找帕累托最优解 (例如，具有最高准确率但速度不变的模型)。</p><p>给定一个模型 m，ACC(m)表示它在目标任务上的准确性，LAT(m) 表示在目标移动平台上的推理延迟，T 是目标延迟。一个常见的方法是将 T 作为一个硬约束，并在这个约束下使精度最大化：</p><img src="/1d840294/13.webp"><p>然而，这种方法只最大化了一个指标，并没有提供多个帕累托最优解决方案。如果一个模型在不增加延迟的情况下具有最高的准确性，或者在不减少准确性的情况下具有最低的延迟，那么该模型就被称为帕累托最优。作者使用定制的加权乘积法来接近帕累托最优解，其优化目标定义为：</p><img src="/1d840294/14.webp"><p>为了进一步在灵活性和搜索空间大小之间取得适当的平衡，论文还提出一种新的分解层次搜索空间，将卷积神经网络分解为一个由模块组成的序列，然后使用层级搜索空间决定每一个模块的层级结构。该空间允许整个网络的分层多样性，允许不同的层级使用不同的运算与连接。同时，强制每一个模块共享相同的结构，因此与逐层搜索相比搜索空间要小几个数量级。</p><p>如图 27 所示，将 CNN 模型划分为一系列预定义的 Block 序列，逐渐降低输入分辨率和增加滤波器尺寸。每一个 Block 内含有一系列 identical layers，其卷积操作和连接由每一个 Block 搜索空间确定。搜索空间包括：</p><ol><li>卷积类型 ConvOp：常规卷积 (conv)、深度卷积 (dconv)t 和移动倒置瓶颈卷积</li><li>卷积核 kernal Size：3 * 3 和 5 * 5</li><li>SE 比例 SE ratio：0 和 0.25 (这个是 SENet 中，计算权重因子时的参数数量的缩放比例)</li><li>直连操作 SkipOp：池化、恒等和无直连</li><li>输出卷积核个数 Fi</li><li>每层叠加的数量 Ni</li></ol><p>以上决定了一个 block 的结构，其中，ConvOp、Kernel Size、SE ratio、SkipOp 和 Fi 决定了一个 layer 的结构，Ni 决定了 layer 重复的数量。图 5 中，block 4 的每个layer 有一个倒置的瓶颈 5x5 卷积层、恒等残差，这样的 layer 重复了 N4 次。</p><img src="/1d840294/15.webp"><div align="center">图5 因子化层次搜索空间</div><p>最后，MnasNet 基于强化学习的 CNN 结构搜索算法。将搜索空间中的每个 CNN 模型映射为一系列 tokens。Tokens 由强化学习 agent (参数为 θ )的一组动作 a_(1:T)决定。目标是最大化期望奖励：</p><img src="/1d840294/16.webp"><p>搜索框架由三部分组成：循环神经网络控制器，一个训练器获得模型精度和移动推理引擎获得模型推理延迟。在每一步中，控制器使用它现在的参数采样一组模型，基于 RNN的 softmax logits 预测一组 tokens。对于每个采样模型 m，在目标任务上训练，计算精度 ACC(m)，在平台上运行模型获得推理延迟 LAT(m)，根据目标函数计算奖励值 R(m)。在每一步的最后，最大化期望奖励来更新控制器参数。一直循环到达到最大的步数或参数收敛。</p><p>当前 SOTA！平台收录 MnasNet 共 6 个模型实现资源，支持的主流框架包含 PyTorch、CANN、MindSpore、TensorFlow 等。</p><img src="/1d840294/17.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">MnasNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/ca934188-fc29-435d-b98a-814b1d0592d7">https://sota.jiqizhixin.com/models/models/ca934188-fc29-435d-b98a-814b1d0592d7</a></td></tr></tbody></table><hr><h3 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h3><p><strong>EfficientNet 是谷歌公司于 2019 年提出</strong>的高效神经网络，故得名为 EfficientNet，<strong>大幅度的缩小了参数的同时提高了预测准确度</strong>。EfficientNet 首先量化了三个指标对提升网络性能的影响：增加网络宽度（增加卷积核个数）、增加网络深度（使用更多层结构）、增加输入网络分辨率。</p><img src="/1d840294/18.webp"><div align="center">图6 模型缩放。(a)基线网络示例；(b)-(d)是传统的缩放方法，只增加网络宽度、深度或分辨率的一个维度。(e)是本文提出的复合缩放方法，以一个固定的比例统一缩放所有三个维度</div><p>增加网络的深度能够得到更加丰富、复杂的特征并且能够在其他新任务中很好的泛化性能。但是，由于逐渐消失的梯度问题，更深层的网络也更难以训练。宽度更广的网络往往能够捕获更多细粒度的功能，并且更易于训练。但是，极其宽泛但较浅的网络在捕获更高级别的特征时往往会遇到困难。使用更高分辨率的输入图像，ConvNets 可以捕获更细粒度的图案，但对于非常高分辨率的图片，准确度会降低。EfficientNet 的思路是Block 中先进行 1x1 卷积提升通道数，然后进行 DepthwiseConv 深度卷积减少参数量，并且在 Block 中引入残差结构和 Squeeze-and-Excitation 模块。建立多个网络深度、网络宽度、图像分辨率不同的模型，从三个方面拓展网络性能。</p><p>作者在论文中对整个网络的运算过程和复合扩展方法进行了抽象。首先定义了每一层卷积网络为 F_i( X_i)，X_i 是输入张量，Y_i 是输出张量，而 tensor 的形状是&lt; H i , W i , C i &gt;。整个卷积网络由 k 个卷积层组成，整个卷积网络为：</p><img src="/1d840294/19.webp"><p>为了探究 d , r , w 这三个因子对最终准确率的影响，将它们加入到公式中，可以得到抽象化后的优化问题（在指定资源限制下）：</p><img src="/1d840294/20.webp"><p>作者提出了一种新的复合缩放方法，该方法使用一个统一的复合系数 ϕ 对网络的宽度，深度和分辨率进行均匀缩放。</p><img src="/1d840294/21.webp"><p>其中 α , β , γ 是通过一个小网格搜索的方法决定的常量。通常来说，ϕ 是一个用户指定的系数来控制有多少的额外资源能够用于模型的缩放，α , β , γ 指明了怎么支配这些额外的资源分别到网络的宽度、深度和分辨率上。尤其是，一个标准卷积操作的运算量的比例是 d , w^2 , r^2 双倍的网络深度将带来双倍的运算量，但是双倍的网络宽度或分辨率将会增加运算为 4 倍。</p><p>作者通过利用多目标神经架构搜索来开发基线网络，以优化准确性和 FLOPS。具体来说，使用 ACC(m)x[FLOPS(m)&#x3D;T]^w 作为优化目标，其中 ACC(m) 和 FLOPS(m) 表示模型 m 的精度和 FLOPS，T 是目标 FLOPS，w&#x3D;-0.07 是控制精度和 FLOPS 之间折衷的超参。优化 FLOPS 而不是延迟，因为我们不针对任何特定的硬件设备。搜索生成一个高效的网络，称之为 EfficientNet-B0。结构如下：</p><img src="/1d840294/22.webp"><div align="center">表2 EfficientNet-B0结构</div><p>它的主要构件是移动式倒置瓶颈 MBConv，作者还在其中加入了 squeeze-and-excitation 优化。从基线 EfficientNet-B0 开始，作者应用复合扩展方法，分两步对其进行扩展。<br>步骤1：首先固定&#x3D;1，假设有两倍的可用资源，对 α , β , γ 进行小网格搜索。<br>步骤2：然后固定 α , β , γ 为常数，将基线网络的规模扩大，以获得 EfficientNet-B1 至 B7。<br>作者表示，有可能通过在一个大的模型周围直接搜索 α , β , γ 来实现更好的性能，但是在更大的模型上搜索成本会变得非常昂贵。作者通过上述方法解决了这个问题，只在小型基线网络上做一次搜索（步骤1），然后对所有其他模型使用相同的缩放系数（步骤2）。</p><p>当前 SOTA！平台收录 EfficientNet 共 14 个模型实现资源，支持的主流框架包含 CANN、PyTorch、TensorFlow、MindSpore 等。</p><img src="/1d840294/23.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">EfficientNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/4fcc29d1-e919-4b62-accc-6d9ba00c1591">https://sota.jiqizhixin.com/models/models/4fcc29d1-e919-4b62-accc-6d9ba00c1591</a></td></tr></tbody></table><hr><h3 id="NFNet"><a href="#NFNet" class="headerlink" title="NFNet"></a>NFNet</h3><p>NFNet 发表在 2021 年的 CVPR 中。这篇论文的核心在于<strong>提出了一种不需要 Batch Normalization 的基于 ResNet 的网络结构。移除了 BN 层以后，准确率不降反升，而且训练时间也缩短了。</strong></p><p>为了在没有引入 normalization 的情况下训练网络，使其达到有竞争力的精度，找到更好的替代方案，作者首先探讨了在训练过程中 BN 带来的影响。在深度学习模型中，跳接（skip connection）和批量归一化（batch normalization，BN）的结合使我们能够训练具有数千层的更深的网络。BN 处理依赖于批处理大小，且与各个处理样本相关，具体包括：BN 消耗计算资源高、内存开销大，造成网络中计算梯度时间较长。此外，BN 在训练阶段造成模型行为之间的差异，需要调整超参。最重要的是，BN 打破了mini-batch 之间样本的独立性。而 BN 带来的好处包括：减小了残差分支、消除了均值偏移、具有归一化的效果，以及可以让大批量大规模的数据实现高效训练。</p><p>目前为了移除 BN 层所作的主要工作是：引入小常量或可学习的标量，抑制初始化时残差分支上激活的规模。本文的主要工作是采用自适应梯度裁剪，基于梯度范数与参数范数的单位比来裁剪梯度，可以适应更大的 batch size 和数据增强的数据。本文构建了NF-ResNets，这是一类预激活的 ResNets，可以在没有归一化层的情况下训练出有竞争力的训练和测试精度。NF-ResNets 采用如下形式的残差块：</p><img src="/1d840294/24.webp"><p>其中，h_i 表征第 i 个残差块的输入，f_i 表示由第 i 个残差分支计算得到的函数。函数f_i 在初始化时被参数化为 variance preserving 的，Var(f_i(z)) &#x3D; Var(z)。超参 α 指定了每个残差块（初始化时）后激活的方差增加的速度，通常被设置为一个较小的值，如α&#x3D;0.2。β_i 是通过预测第 i 个残差块的输入的标准差来确定的，β_i&#x3D;sqrt(Var(h_i))，其中 Var(h_i+1)&#x3D;Var(h_i)+α^2，过渡块除外（发生空间下采样）。对于过渡块，skip path 在降尺度的输入上操作（h_i&#x2F;β_i），预期方差在过渡块后被重置为h_i+1&#x3D;1+α^2。Squeeze-excite 层的输出乘以 2 的系数。为了防止隐藏激活函数中出现的“均值转移”现象，引入比例权重标准化（SWS）：</p><img src="/1d840294/25.webp"><p>为了将 NF-resnet 扩展到更大的批量，使大学习率训练时梯度下降成为可能，加速收敛，作者引入 Adaptive Gradient Clipping（AGC）自适应梯度裁剪。AGC 核心是约束梯度的范数。梯度剪裁通常是通过限制梯度的归一化处理来进行的。具体来说，对于梯度向量 G&#x3D;∂L&#x3D;∂θ，其中 L 表示损失，θ 表示包含所有模型参数的向量，标准剪裁算法在更新 θ 之前对梯度进行剪裁，即：</p><img src="/1d840294/26.webp"><p>作者分析，虽然这种剪裁算法使我们能够以比以前更高的批量大小进行训练，但训练的稳定性对剪裁阈值的选择极为敏感，在改变模型深度、批量大小或学习速率时需要进行精细的调整。为了解决这一问题，作者引入 AGC 策略：</p><img src="/1d840294/27.webp"><p>其中，W^l 是 的第 L 层的权重矩阵， G^l 代表 W^l 的梯度。AGC 关键在于：通过观察梯度 G 的范数与层权重 W 的范数之比，以判断单个梯度下降在多大程度上改变原始权重 W 。根据梯度归一与参数归一的单位长度比值来剪辑梯度，作者发现这比采取层间归一比值的经验表现更好。具体来说，在 AGC 算法中，第 l 层 (G_i)^l（定义为矩阵G^l 的第 i 行）的梯度的每个单元 i 被剪切为：</p><img src="/1d840294/28.webp"><p>如果梯度和原始权重之比大于限符阈值，我们就将梯度替换为：限符阈值 * 范数的反比 * 原始梯度，否则不改变原始梯度。对于大批量训练，λ应该变小。</p><img src="/1d840294/29.webp"><p>最后，作者设计了具有最先进精度和训练速度的无归一化架构（Normalizer-Free architectures）。作者的设计思路是专注于手动设计模型，这些模型针对现有加速器的训练延迟进行了优化。通过手动搜索设计趋势来探索模型设计的空间，这些设计实现了ImageNet 上的 holdout top-1 与设备上的实际训练延迟的 pareto front 的改进。图7展示了这些修改。</p><img src="/1d840294/30.webp"><div align="center">图7 NFNet bottleneck block设计和架构差异的总结</div><p>NFNet 模型是一个改良的 SE-ResNeXt-D。该模型的输入是一个 H×W 的 RGB 图像，它已经被整个 ImageNet 训练集的每个通道的平均值&#x2F;标准差归一化，这是大多数图像分类器的标准输入。该模型有一个初始的 “stem”，包括：一个 16 通道的 3×3 stride 2 convolution，两个分别为 32 通道和 64 通道的 3×3 stride 1 convolution，以及一个128 通道的 3×3 stride 2 convolution。在 stem 的每个卷积之间放置一个 non-linearity。</p><p>stem 之后是四个残差 “stages”，对于本文的基线 F0 变体，每个 stage 的块数是[1,2,6,3]，随后的每个变体的块数都是这个数字乘以 N（其中 F0 的 N&#x3D;1）。残差 stages从 “过渡（transition） “块开始，然后是标准残差块（如图 22）。</p><p>所有块都采用预激活的 ResNe(X)t bottleneck 模式，在 bottleneck 内增加了一个 3×3 的分组卷积。这意味着 main path 包括：一个 1×1 卷积，其输出通道等于 0.5× 该块的输出通道数；两个 3×3 分组卷积，组宽为 128（第一组在过渡块中分流）；一个 1×1 卷积，其输出通道数等于块的输出通道数。</p><p>在最后一个 1×1 卷积之后是 Squeeze&amp;Excite 层，它全局平均汇集激活，对汇集的激活应用两个具有交错缩放非线性的线性层，应用一个 sigmoid，然后以这个 sigmoid 值的两倍重新缩放张量通道。具体的，这一层的输出为：</p><img src="/1d840294/31.webp"><p>在所有的残差阶段之后，应用一个 1×1 的扩展卷积，将通道数增加一倍，类似于EfficientNets 中的最终扩展卷积，然后进行全局平均汇集。这一层主要是在使用非常薄的网络时有帮助，因为通常希望最终激活向量（分类器层收到的）的维度大于或等于类的数量，但作者在更广泛的网络中也保留了它，目的是未来可能寻求在我们的骨干网基础上训练非常薄的网络。</p><p>最后一层是一个全连接的分类器层，具有 learnable biases，输出 1000 个类别向量（可以进行 soft 放大处理，以获得归一化的类别概率）。作者用 0.01 的标准差来初始化这一层的权重。作者发现，如果用零来初始化权重，在用非常多的输出类进行训练时，有时会导致不稳定。</p><p>在本文的残差块中没有使用激活归一化层。相反，作者采用了 Normalizer-Free variance downscaling 策略。这意味着残差块的主路径的输入乘以 1&#x2F;β，其中 β 是初始化时该块方差的分析预测值，块输出乘以一个超参 α。此外，作者还引入 SkipInit，这是一个可学习的零初始化标量增益，除了 α 之外，它将残差块初始化为 identity（除了在过渡层），作者发现这可以提高非常深的网络的稳定性。所有的卷积运算都采用 Scaled Weight Standardization，对标准化的权重采用可学习的 affine gain，对卷积运算的输出采用可学习的 affine bias。</p><img src="/1d840294/32.webp"><div align="center">图8 NFNet网络结构图，应用 AGC 到除了最后的线性层上的每一层，得到最终的NFNet 配置</div><p>当前 SOTA！平台收录 NFNet 共 15 个模型实现资源，支持的主流框架包含 TensorFlow、PyTorch、MindSpore、JAX 等。</p><img src="/1d840294/33.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">NFNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/2f7b43e7-1db3-48b9-8f39-0bd5244c367b">https://sota.jiqizhixin.com/models/models/2f7b43e7-1db3-48b9-8f39-0bd5244c367b</a></td></tr></tbody></table><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/8Svli4dmU3Hyq-932_SXgQ">https://mp.weixin.qq.com/s/8Svli4dmU3Hyq-932_SXgQ</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/cf26f90d.html" rel="prev" title="深度学习-DenseNet、MobileNet、DPN…你都掌握了吗？一文总结图像分类必备经典模型（二）"><i class="fa fa-chevron-left"></i> 深度学习-DenseNet、MobileNet、DPN…你都掌握了吗？一文总结图像分类必备经典模型（二）</a></div><div class="post-nav-item"><a href="/2e59fe20.html" rel="next" title="使用Transformer的端到端弱监督语义分割">使用Transformer的端到端弱监督语义分割 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-Attention-Network"><span class="nav-number">1.</span> <span class="nav-text">Residual Attention Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleNet"><span class="nav-number">2.</span> <span class="nav-text">ShuffleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Channel-Shuffle-for-Group-Convolutions"><span class="nav-number">2.1.</span> <span class="nav-text">Channel Shuffle for Group Convolutions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ShuffleNet-Unit"><span class="nav-number">2.2.</span> <span class="nav-text">ShuffleNet Unit</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MnasNet"><span class="nav-number">3.</span> <span class="nav-text">MnasNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EfficientNet"><span class="nav-number">4.</span> <span class="nav-text">EfficientNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NFNet"><span class="nav-number">5.</span> <span class="nav-text">NFNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1017</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.8m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">160:09</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>