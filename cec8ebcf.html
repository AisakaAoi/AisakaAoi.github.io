<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="自监督学习有生成式学习和对比学习，对比学习需要从无标注的数据中学习特征表示，并用于下游任务中。指导原则是: 通过构造相似实例和不相似实例，学习一个表示学习模型，使得相似的实例在投影空间中较接近，不相似的实例在投影空间中距离较远。对比学习有三个关键问题: 1. 正负样本的构造 2. 编码器的设计 3. Loss函数的选取。 过去几年，尤其是18年开始到现在，对比学习在计算机视觉领域的发展历程，总结一"><meta property="og:type" content="article"><meta property="og:title" content="对比学习Contrastive Learning综述"><meta property="og:url" content="https://aisakaaoi.github.io/cec8ebcf.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="自监督学习有生成式学习和对比学习，对比学习需要从无标注的数据中学习特征表示，并用于下游任务中。指导原则是: 通过构造相似实例和不相似实例，学习一个表示学习模型，使得相似的实例在投影空间中较接近，不相似的实例在投影空间中距离较远。对比学习有三个关键问题: 1. 正负样本的构造 2. 编码器的设计 3. Loss函数的选取。 过去几年，尤其是18年开始到现在，对比学习在计算机视觉领域的发展历程，总结一"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/20.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/23.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/24.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/25.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/26.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/27.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/28.webp"><meta property="og:image" content="https://aisakaaoi.github.io/cec8ebcf/29.webp"><meta property="article:published_time" content="2021-12-31T15:28:04.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:39.718Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/cec8ebcf/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/cec8ebcf.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>对比学习Contrastive Learning综述 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1014</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/cec8ebcf.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">对比学习Contrastive Learning综述</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-12-31 23:28:04" itemprop="dateCreated datePublished" datetime="2021-12-31T23:28:04+08:00">2021-12-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/cec8ebcf.html" class="post-meta-item leancloud_visitors" data-flag-title="对比学习Contrastive Learning综述" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/cec8ebcf.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/cec8ebcf.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>8.2k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>21 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>自监督学习有生成式学习和对比学习，对比学习需要从无标注的数据中学习特征表示，并用于下游任务中。指导原则是: 通过构造相似实例和不相似实例，学习一个表示学习模型，使得相似的实例在投影空间中较接近，不相似的实例在投影空间中距离较远。对比学习有三个关键问题: 1. 正负样本的构造 2. 编码器的设计 3. Loss函数的选取。</p><p>过去几年，尤其是18年开始到现在，对比学习在计算机视觉领域的发展历程，总结一下可以规划成四个阶段:</p><ol><li>2018~2019年中，Inst Disc、CPC、CMC等方法和模型都还没有统一，目标函数和代理任务也还没有统一;</li><li>2019~2020年中，SimCLR 、 Moco、 CPC&#x2F;CMC的延伸工作、SwAV，这个阶段发展非常迅速;</li><li>不用负样本也可以做对比学习，如BYOL和后续改进，SimSiam把所有方法总结归纳融入SimSiam框架，是用CNN做对比学习的一个总结;</li><li>Transformer时代，MoCo-v3和DINO。随着Vision Transformer的爆火，对于自监督学习，不论是对比学习还是最新的掩码学习，都是用VIT做的。</li></ol><span id="more"></span><hr><h3 id="第一阶段：2018-2019年中"><a href="#第一阶段：2018-2019年中" class="headerlink" title="第一阶段：2018~2019年中"></a>第一阶段：2018~2019年中</h3><p>Inst Disc、CPC、CMC等，方法和模型都还没有统一，目标函数和代理任务也还没有统一。</p><h4 id="Inst-Disc-Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination"><a href="#Inst-Disc-Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination" class="headerlink" title="Inst Disc - Unsupervised Feature Learning via Non-Parametric Instance Discrimination"></a>Inst Disc - Unsupervised Feature Learning via Non-Parametric Instance Discrimination</h4><p>basic idea：让图片聚集在一起的原因并不是因为他们有相同的语义标签，而是图片(object)很相似。采用个体判别任务——无监督学习方式，把每个instance看成一个类别，目标就是能学一种特征，从而把每个图片都区分开。</p><img src="/cec8ebcf/1.webp"><p>正样本：图片本身+数据增强 负样本：其他图片<br>memory bank: 存所有图片的特征，也就是一个字典，每个特征的维度不能太高——128维。<br>NCE loss 算对比学习的目标函数。更新完网络后，把mini batch的特征到memory bank更换掉。反复更新encoder和memory bank。<br>Proximal Regularization: 给模型训练加了约束，让memory bank里的特征进行动量式的更新。——跟Moco想法很一致。<br>实验超参数设置：温度0.07 4096个负样本(从memory bank抽取) 起始learning rate0.03 …<br>取得了不错的无监督表征学习的结果。</p><hr><h4 id="Unsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature-2019-CVPR"><a href="#Unsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature-2019-CVPR" class="headerlink" title="Unsupervised Embedding Learning via Invariant and Spreading Instance Feature - 2019 - CVPR"></a>Unsupervised Embedding Learning via Invariant and Spreading Instance Feature - 2019 - CVPR</h4><p>基本的对比学习，SimCLR的前身。没有使用额外的数据结构存负样本，正负样本来自同一个minibatch, 就可以只使用一个编码器进行端到端学习。</p><img src="/cec8ebcf/2.webp"><p>basic idea: 相似的图片和物体特征应该保持不变性。对于不相似的任务，特征应该尽可能分散开。<br>代理任务——个体判别。目标函数——NCE loss的一个变体</p><img src="/cec8ebcf/3.webp"><p>原始数据 256 数据增强 256 * 2。正样本 256 负样本(256-1)*2。<br>end-to-end<br>字典必须足够大，对比学习时候负样本最好是足够多，本文没有TPU，也没有SimCLR那么强大的数据增广和MLP projector, 所以结果没有那么出彩。</p><hr><h4 id="CPC-Representation-Learning-with-Contrastive-Predictive-Coding"><a href="#CPC-Representation-Learning-with-Contrastive-Predictive-Coding" class="headerlink" title="CPC- Representation Learning with Contrastive Predictive Coding"></a>CPC- Representation Learning with Contrastive Predictive Coding</h4><p>CPC——用预测的代理任务做对比学习</p><img src="/cec8ebcf/4.webp"><p>gar——自回归模型。常见的自回归模型auto-regressive：RNN、LSTM<br>t当前时刻，Ct 代表上下文的一个特征表示，可以用来预测下文。<br>正样本——未来的输入通过编码器以后得到的未来时刻的特征输出，做出的预测是query，真正的输出是由输入决定的。负样本——很广泛，可以任意选取输入，通过编码器得到输出。<br>图中的序列可以换成文字序列、图片patch等。</p><hr><h4 id="CMC-Contrastive-Multiview-Coding"><a href="#CMC-Contrastive-Multiview-Coding" class="headerlink" title="CMC - Contrastive Multiview Coding"></a>CMC - Contrastive Multiview Coding</h4><p>CMC——第一个或者比较早做多视角对比学习的，证明了对比学习的灵活性和多视角多模态的可行性。</p><img src="/cec8ebcf/5.webp"><p>正样本——四个视角：原始图像，深度信息(每个物体离观察者多远), surface normal、物体分割图像，虽然输入来自不同的传感器，或者不同的模态，但是对应一张图片，互为正样本。特征空间中，四个绿色点应该接近，负样本特征红点应该远离绿色。</p><p>基于CMC，OpenAI在2021年提出了<strong>CLIP模型</strong>，利用text信息监督视觉任务自训练，本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。多模态的对比学习。CLIP文本端用BERT,图像端用VIT，不同的模态需要不同的编码器。Transformer的好处：可以同时处理不同模态的数据。</p><p>CMC利用对比学习的思想，做了一篇蒸馏的工作：不论用什么网络，只要输入是同一张图片，得到的特征应该尽可能类似(eg. teacher和student作为正样本对，从而做对比学习)。</p><hr><h3 id="第二阶段：2019年中-2020年中"><a href="#第二阶段：2019年中-2020年中" class="headerlink" title="第二阶段：2019年中~2020年中"></a>第二阶段：2019年中~2020年中</h3><p>SimCLR 、 Moco、 CPC&#x2F;CMC的延伸工作、SwAV，这个阶段发展非常迅速。</p><h4 id="MoCo-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning"><a href="#MoCo-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning" class="headerlink" title="MoCo - Momentum Contrast for Unsupervised Visual Representation Learning"></a>MoCo - Momentum Contrast for Unsupervised Visual Representation Learning</h4><p>MoCo的主要贡献是把之前对比学习的一些方法都归纳总结为一个字典查询的问题。<br>提出了两个东西：队列queue和动量编码器momentum encoder，从而形成一个又大又一致的字典帮助更好地对比学习。</p><img src="/cec8ebcf/6.webp"><p>MoCo和 Inst Disc很相似，用queue取代memory bank去存负样本，解决大字典的问题。用动量编码器取代loss里的约束项，从而达到动量更新编码器的目的，而不是动量的更新特征，解决特征不一致问题，从而能得到更好的结果。</p><p>实验参数：ResNet as encoder、基线模型res50、特征维度128、L2-norm、目标函数info NCE、 loss温度0.07…</p><hr><h4 id="SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations"><a href="#SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations" class="headerlink" title="SimCLR - A Simple Framework for Contrastive Learning of Visual Representations"></a>SimCLR - A Simple Framework for Contrastive Learning of Visual Representations</h4><img src="/cec8ebcf/7.webp"><p>对mini-batch的所有图片做更多的数据增强，得到xi和xj。同一图片延伸得到的两个图片就是正样本，batch size是N的话，正样本个数是N，负样本个数就是剩下所有的样本及增强过后的样本2(N-1)。</p><p>经过编码器f 共享权重，得到特征表示h。创新点在h之后又加了一个g函数—projector（只有训练的时候才用g,做下游任务时候是扔掉的），就是一个MLP， 包含全连接层后加一个激活函数relu(Non-linear)。最后衡量一下正样本之间是不是能不能达到最大的一致性。</p><p>采用normalized(h后进行L2归一化) temperature scaled(在loss上×个τ)的交叉熵函数。<br>用了更大的batch size且训练更久</p><img src="/cec8ebcf/8.webp"><p>最有效的数据增强——crop和color，其他的增强可有可无 锦上添花</p><img src="/cec8ebcf/9.webp"><p>使用Non-linear会比什么都不用提升十几个点<br>z最后的维度不管是多少，都没太大区别，所以对比学习都选一个比较低的维度，128就够了。</p><hr><h4 id="MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning"><a href="#MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning" class="headerlink" title="MoCo v2 - Improved Baselines with Momentum Contrastive Learning"></a>MoCo v2 - Improved Baselines with Momentum Contrastive Learning</h4><img src="/cec8ebcf/10.webp"><p>在SimCLR的基础上，moco v2：MLP + aug+ + cosine learning rate schedule + 800 epochs</p><p>无监督学习训练越久 模型越大 结果会更好些</p><img src="/cec8ebcf/11.webp"><p>MoCo v2相对于SimCLR的优越性——硬件。SimCLR——end to end</p><img src="/cec8ebcf/12.webp"><hr><h4 id="SimCLR-v2-Big-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners"><a href="#SimCLR-v2-Big-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners" class="headerlink" title="SimCLR v2 - Big Self-Supervised Models are Strong Semi-Supervised Learners"></a>SimCLR v2 - Big Self-Supervised Models are Strong Semi-Supervised Learners</h4><img src="/cec8ebcf/13.webp"><p>自监督的对比学习去训练一个大的模型出来， 有一小部分有监督的数据去做一个有监督的微调， 用模型生成很多伪标签，可以在更多无标签的数据上去做自学习。</p><p>SimCLR v1 - v2的改进：v2是个两页的技术报告</p><ul><li>更大的模型，152层的ResNet Selective kernels SK net，骨干网络很强</li><li>SimCLR的projector MLP层有用， 多加几层？两层的MLP就够了，由原来的fc+relu变成fc+relu fc+relu，加深了projection head。</li><li>也使用了动量编码器，提升不大，因为负样本已经相当多了，字典大小和特征一致性SimCLR已经足够好了。</li></ul><hr><h4 id="SwAV-Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignments-CNN中用res50分最高的工作75-3"><a href="#SwAV-Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignments-CNN中用res50分最高的工作75-3" class="headerlink" title="SwAV - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments - CNN中用res50分最高的工作75.3"></a>SwAV - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments - CNN中用res50分最高的工作75.3</h4><p>basic idea: 给定同样的图片，生成不同的视角， 希望可以用一个view得到的特征去预测另外一个视角得到的特征， 所以view的特征应该是非常接近的。</p><p>做法：把对比学习和聚类(无监督的特征表示学习方式，相似的物体聚集在某个聚类中心附近，不相似的尽量推开)结合。</p><img src="/cec8ebcf/14.webp"><p>左：之前对比学习的方法，同一个图片做两次数据增强，所有的样本通过一个编码器(res50或加一个projection head)，得到一个特征，去做对比学习的loss就可以。<br>右：SwAV, 左图简单，但是直接拿所有图片的特征去跟特征作对比，有点原始且有点费资源， 因为所有的图片都是自己的类。能不能不做近似， 能不能借助先验信息不去跟大量的负样本比，而去跟一些更简洁的东西比？—— 可以去和聚类中心比，右图中的c(矩阵),维度d*k，d为特征的维度，k是有多少个聚类中心，本文是3000。</p><p>区别：得到特征z后并不是直接在特征上做对比学习的loss，而是通过聚类的方法，让特征z和c生成一个目标q1,q2 相当于一个ground truth。z1,z2很相似，按道理可以互相去做预测，代理任务是 z1和c做点乘，可以预测q2，z2和c点乘，可以预测q1，通过换位预测对模型进行训练。</p><p>聚类的好处：如果很很多负样本进行类比，需要成千上万的负样本，也是一个近似。而跟聚类中心做类比，可以用几百最多3000个聚类中心就足以表示。另外，聚类中心是有明确地语义含义的，如果只是之前的随机抽样负样本去做对比，有可能抽到正样本，负样本类别也不均衡，不如使用聚类中有效。(参考deep cluster, deep cluster two)</p><p>另一个性能提升点：multi-crop，使用了更多的正样本。之前是取224 * 224的两个crop，现在取两个160的crop学习全局的特征，为了增加正样本数量学习局部特征，随机选4个小点的crop 96 * 96。（multi-crop对其他对比学习方法也有用）</p><img src="/cec8ebcf/15.webp"> <img src="/cec8ebcf/16.webp"><p>聚类和对比学习的方法结合也没什么优势，multi-crop这个全局和局部特征都要关注的思想才是重点。SwAV算是一个承上启下的工作，也没有用什么负样本，用的聚类中心。</p><p>附：CPC v2——用了更大的模型，更大的图像块，做了更多方向上的预测任务，把batch norm换成了layer norm，使用了更多的数据增强</p><p>Info Min—— CMC的作者做的分析延伸工作，到底选什么样的视角才能对对比学习最好，提出了InfoMin的原则-最小化互信息，想要不多不少刚好的互信息。按InfoMin原则做合适的数据增强，拿到合适的对比学习的视角。</p><p>综上，对比学习到了第二阶段 很多细节都趋于统一，目标函数都是用Info NCE或类似函数去算，模型都是用一个编码器加一个projection head，都采用了更强的数据增强，使用动量编码器，尝试训练更久，在ImageNet上的准确度也逐渐逼近于有监督的基线模型。第三阶段就是不用负样本的对比学习。第二阶段的SwAV也算是一个承上启下的工作，也没有用什么负样本，用的聚类中心，但是明确对比的对象，第三阶段的BYOL和SimSiam就是正样本自己玩，没有负样本和聚类中心 这样明确的东西去做对比。</p><hr><h3 id="第三阶段：2020-不用负样本也可以做对比学习"><a href="#第三阶段：2020-不用负样本也可以做对比学习" class="headerlink" title="第三阶段：2020 不用负样本也可以做对比学习"></a>第三阶段：2020 不用负样本也可以做对比学习</h3><h4 id="BYOL-Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning"><a href="#BYOL-Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning" class="headerlink" title="BYOL - Bootstrap Your Own Latent A New Approach to Self-Supervised Learning"></a>BYOL - Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</h4><p>Bootstrap: 已经有的东西之上改造， Latent: 特征。看论文看到latent、hidden、feature、embedding都是特征的意思。BYOL自己跟自己学，完全没用任何负样本。在ImageNet上得到74.3的top1准确率。</p><p>对比学习中负样本是约束，如果没有负样本，只能让所有相似的物体 特征也尽可能相似，不管给什么输入，都是相同的输出，算对比学习的loss都是0，负样本限制这种什么都学不到的情况发生(model collapse)。</p><img src="/cec8ebcf/17.webp"><p>mini-batch的输入，数据增强后，上下编码器使用同样的网络架构，但是参数是不同的，上面的通过梯度更新而更新，下面的和MoCo一样是用动量编码器Moving average更新。得到特征，跟SimCLR一样用了一个projector，ztheta是256维，比128维效果好一点， 上下是一样的网络结构但参数不一样。</p><p>BYOL加了新的一层q theta，跟g theta一样也是一个MLP，得到一个新的特征q(z theta)，让这个预测跟z xi尽可能一致，把匹配的问题换成一个预测的问题。自己预测自己，sg &#x3D; stop gradient没有梯度。</p><p>上面一行相当于query编码器，下面相当于key的编码器，key的编码器都是query编码器的动量更新，不一样是代理任务不一样，用自己一个视角的特征去预测另外一个视角的特征，去完模型性的训练。训练完成后只有编码器f theta留下了，最后用y theta 2048维的特征去做下游任务。</p><p>目标函数MSE loss。</p><hr><h4 id="Blog-How-to-understand-BYOL"><a href="#Blog-How-to-understand-BYOL" class="headerlink" title="Blog - How to understand BYOL"></a>Blog - How to understand BYOL</h4><img src="/cec8ebcf/18.webp"><p>projection head是MLP，fc+BN+Relu+fc。</p><p>BYOL训练的时候不坍塌，肯定是和Batch Norm有关系的。BN是把batch里所有样本的特征拿过来算均值和方差，去做归一化。当算某个正样本的loss的时候，其实也看到了其他样本的特征，这里面是有信息泄露的（Shuffling BN防止信息泄露）。</p><p>当有了batch norm之后，BYOL其实并不光是正样本自己跟自己学，其实也在做对比，一种隐式的对比学习：正样本的图片和平均图片(batch norm产生的，有聚类中心的意思，中值的意思)有什么差别。</p><hr><h4 id="BYOL-works-even-without-batch-statistics"><a href="#BYOL-works-even-without-batch-statistics" class="headerlink" title="BYOL works even without batch statistics"></a>BYOL works even without batch statistics</h4><p>BYOL的作者不想让大家觉得BYOL的成功是依赖于batch norm，做了一系列实验，回应上个blog：</p><img src="/cec8ebcf/19.webp"><p>当projector有BN的时候，有一个BYOL还是训练失败了，不能解释BN很关键。当encoder和Projector都没有BN的是，SimCLR也失败了，证明BN不是提供了一个隐式的负样本，即使给了显式的负样本还是训练不出来。</p><p>最后达成一致：BN主要作用是提高模型训练的稳健性，导致不会模型坍塌。</p><p><strong>BYOL作者实验：Using GN with WS leads to competitive performance</strong><br>采用Group Norm 、 Weight Standardization模型初始化方式 都没有计算批统计量，没有隐式的对比，BYOL还是一个全新的自学方式。</p><hr><h4 id="SimSiam-Exploring-Simple-Siamese-Representation-Learning"><a href="#SimSiam-Exploring-Simple-Siamese-Representation-Learning" class="headerlink" title="SimSiam - Exploring Simple Siamese Representation Learning"></a>SimSiam - Exploring Simple Siamese Representation Learning</h4><p>不需要负样本、不需要大的batch size、不需要动量编码器，不仅不模型坍塌，结果也好。</p><img src="/cec8ebcf/20.webp"><p>孪生网络——两个编码器，一般结构是一样的，共享参数。整体架构和BYOL非常一样。SimSiam Pseudocode, PyTorch-like伪代码：D函数是算loss的，MSE loss 对称性的loss，然后梯度回传，更新网络。</p><img src="/cec8ebcf/21.webp"><p>SimSiam能够不模型坍塌成功训练，主要是有stop gradient操作的存在，SimSiam可以看成一种EM算法，一个训练过程或者一套模型参数被人为劈成两份，相当于解决两个子问题一样，模型更新也在交替进行。通过逐步更新的方式避免模型坍塌。</p><img src="/cec8ebcf/22.webp"><p>所有孪生网络的做法归纳：SimCLR end-to-end学习，两边都是有梯度回传的，还是做的一个对比任务。SwAV也是对比任务，没有跟负样本比，跟聚类中心(Sinkhorn-Knopp算法产生的)去比。BYOL新的贡献—predictor，从对比任务变为预测任务，用左边预测右边，还使用了动量编码器，SimSiam和BYOL很像，左边一样，右边没有用动量编码器。</p><img src="/cec8ebcf/23.webp"><p>分类来说最强的方法 ——BYOL<br>下游任务：MoCo v2和SimSiam表现最好，对比学习的工作，MoCo v2可当基线模型，训练快、稳，下游任务迁移的好。</p><p>Barles Twins:主要就是更换了一个目标函数。把之前的对比和预测，变成两个矩阵之间比相似性。因为是2021年3月提出的，很快淹没在ViT的洪流中。</p><hr><h3 id="第四阶段：2021-Transformer时代"><a href="#第四阶段：2021-Transformer时代" class="headerlink" title="第四阶段：2021 Transformer时代"></a>第四阶段：2021 Transformer时代</h3><h4 id="MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers"><a href="#MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers" class="headerlink" title="MoCo v3 - An Empirical Study of Training Self-Supervised Vision Transformers"></a>MoCo v3 - An Empirical Study of Training Self-Supervised Vision Transformers</h4><p>MoCo v3只是一种架构，CNN也可以用，ViT也可以用。本文改进自监督训练VIT而更稳定。MoCo v3是MoCo v2和SimSiam的延伸工作</p><img src="/cec8ebcf/24.webp"> <img src="/cec8ebcf/25.webp"><p>batch size变大后，准确度会掉下，恢复后也不如原来高。观察训练时候每一层回传梯度的情况。梯度波峰发生在第一层做patch projection的时候。如何把图片打成一个patch给它一个特征是一个可训练的全连接层，梯度不正常不如不训练。随机初始化了一个patch projection层然后冻住，整个训练过程都不变。对MoCo v3和BYOL都有用，用BYOL框架把残差网络换成VIT，patch projection冻住，一样能获得更平滑的曲线，更好地结果。</p><p>不改Transformer本身，只能改开头Tokenazition和结尾loss 。</p><hr><h4 id="DINO-Emerging-Properties-in-Self-Supervised-Vision-Transformers"><a href="#DINO-Emerging-Properties-in-Self-Supervised-Vision-Transformers" class="headerlink" title="DINO - Emerging Properties in Self-Supervised Vision Transformers"></a>DINO - Emerging Properties in Self-Supervised Vision Transformers</h4><p>自监督训练ViT的方式，ViT在自监督训练下的特性: 完全不用任何标签信息，训练出来的ViT，把自注意力图拿出来，可以准确的抓住每个物体的轮廓，媲美无监督分割。</p><img src="/cec8ebcf/26.webp"> <img src="/cec8ebcf/27.webp"><p>DINO——Self distillation with no labels 蒸馏的框架，延续了BYOL，换了个名字，student预测teacher，teacher可以想成group-truth。为了避免模型坍塌，DINO做了一个centering，把batch里的样本算一个均值，减掉均值就算centering。stop gradient操作。最后用p1去预测p2。</p><img src="/cec8ebcf/28.webp"><p>从方法和模型方面，跟第三阶段基本是一样的，主要是融合了ViT。</p><hr><h3 id="一张图总结对比学习历程"><a href="#一张图总结对比学习历程" class="headerlink" title="一张图总结对比学习历程"></a>一张图总结对比学习历程</h3><img src="/cec8ebcf/29.webp"><p>对比学习还是一个很火的方向，虽然没有ViT火，MAE火爆了后都尝试掩码学习而不是对比学习，对比学习从火爆发展期变成发展潜伏期。而多模态的对比学习还是一个主流，CLIP的效果就很好。在多模态中图像和文本之间的对比学习loss还是一个标准的目标函数。对比学习只是一个想法而不是一个具体工作，几十年前就已经提出来了，接下来应该还会看到很多对比学习和其他方法的结合工作。</p><hr><h3 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/555359995">https://zhuanlan.zhihu.com/p/555359995</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/af072c4e.html" rel="prev" title="对比学习论文综述-总结再整理"><i class="fa fa-chevron-left"></i> 对比学习论文综述-总结再整理</a></div><div class="post-nav-item"><a href="/ef2d8f98.html" rel="next" title="SFFAI 134 | 关系抽取专题《崔立：一种基于关系原型表示的持续关系抽取方法》">SFFAI 134 | 关系抽取专题《崔立：一种基于关系原型表示的持续关系抽取方法》 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A2018-2019%E5%B9%B4%E4%B8%AD"><span class="nav-number">1.</span> <span class="nav-text">第一阶段：2018~2019年中</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inst-Disc-Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination"><span class="nav-number">1.1.</span> <span class="nav-text">Inst Disc - Unsupervised Feature Learning via Non-Parametric Instance Discrimination</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Unsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature-2019-CVPR"><span class="nav-number">1.2.</span> <span class="nav-text">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature - 2019 - CVPR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CPC-Representation-Learning-with-Contrastive-Predictive-Coding"><span class="nav-number">1.3.</span> <span class="nav-text">CPC- Representation Learning with Contrastive Predictive Coding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CMC-Contrastive-Multiview-Coding"><span class="nav-number">1.4.</span> <span class="nav-text">CMC - Contrastive Multiview Coding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A2019%E5%B9%B4%E4%B8%AD-2020%E5%B9%B4%E4%B8%AD"><span class="nav-number">2.</span> <span class="nav-text">第二阶段：2019年中~2020年中</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MoCo-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning"><span class="nav-number">2.1.</span> <span class="nav-text">MoCo - Momentum Contrast for Unsupervised Visual Representation Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SimCLR-A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations"><span class="nav-number">2.2.</span> <span class="nav-text">SimCLR - A Simple Framework for Contrastive Learning of Visual Representations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MoCo-v2-Improved-Baselines-with-Momentum-Contrastive-Learning"><span class="nav-number">2.3.</span> <span class="nav-text">MoCo v2 - Improved Baselines with Momentum Contrastive Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SimCLR-v2-Big-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners"><span class="nav-number">2.4.</span> <span class="nav-text">SimCLR v2 - Big Self-Supervised Models are Strong Semi-Supervised Learners</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SwAV-Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignments-CNN%E4%B8%AD%E7%94%A8res50%E5%88%86%E6%9C%80%E9%AB%98%E7%9A%84%E5%B7%A5%E4%BD%9C75-3"><span class="nav-number">2.5.</span> <span class="nav-text">SwAV - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments - CNN中用res50分最高的工作75.3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9A2020-%E4%B8%8D%E7%94%A8%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">第三阶段：2020 不用负样本也可以做对比学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BYOL-Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning"><span class="nav-number">3.1.</span> <span class="nav-text">BYOL - Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Blog-How-to-understand-BYOL"><span class="nav-number">3.2.</span> <span class="nav-text">Blog - How to understand BYOL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BYOL-works-even-without-batch-statistics"><span class="nav-number">3.3.</span> <span class="nav-text">BYOL works even without batch statistics</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SimSiam-Exploring-Simple-Siamese-Representation-Learning"><span class="nav-number">3.4.</span> <span class="nav-text">SimSiam - Exploring Simple Siamese Representation Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E9%98%B6%E6%AE%B5%EF%BC%9A2021-Transformer%E6%97%B6%E4%BB%A3"><span class="nav-number">4.</span> <span class="nav-text">第四阶段：2021 Transformer时代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MoCo-v3-An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers"><span class="nav-number">4.1.</span> <span class="nav-text">MoCo v3 - An Empirical Study of Training Self-Supervised Vision Transformers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DINO-Emerging-Properties-in-Self-Supervised-Vision-Transformers"><span class="nav-number">4.2.</span> <span class="nav-text">DINO - Emerging Properties in Self-Supervised Vision Transformers</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%BC%A0%E5%9B%BE%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%8E%86%E7%A8%8B"><span class="nav-number">5.</span> <span class="nav-text">一张图总结对比学习历程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">原文链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1014</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.7m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">153:20</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haru02.model.json"},"display":{"position":"right","width":208,"height":520},"mobile":{"show":false},"log":false});</script></body></html>