<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="什么是范数我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。 在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量"><meta property="og:type" content="article"><meta property="og:title" content="机器学习-什么是范数"><meta property="og:url" content="https://aisakaaoi.github.io/da6b54c.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="什么是范数我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。 在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/1.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/12.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/13.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/2.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/3.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/4.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/5.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/6.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/7.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/8.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/9.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/10.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/20.svg"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/23.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/24.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/25.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/26.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/27.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/28.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/29.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/30.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/31.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/32.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/33.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/34.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/35.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da6b54c/36.webp"><meta property="article:published_time" content="2021-12-09T18:36:23.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:39.608Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/da6b54c/1.svg"><link rel="canonical" href="https://aisakaaoi.github.io/da6b54c.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习-什么是范数 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1006</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/da6b54c.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习-什么是范数</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-12-10 02:36:23" itemprop="dateCreated datePublished" datetime="2021-12-10T02:36:23+08:00">2021-12-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span></span><span id="/da6b54c.html" class="post-meta-item leancloud_visitors" data-flag-title="机器学习-什么是范数" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/da6b54c.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/da6b54c.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.6k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>14 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="什么是范数"><a href="#什么是范数" class="headerlink" title="什么是范数"></a>什么是范数</h3><p>我们知道距离的定义是一个宽泛的概念，只要满足<strong>非负、自反、三角不等式</strong>就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。</p><p>在数学上，范数包括<strong>向量范数</strong>和<strong>矩阵范数</strong>，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算AX&#x3D;B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。</p><p>向量的范数表示这个原有集合的大小。</p><p>矩阵的范数表示这个变化过程的大小的一个度量。</p><p>简单说：L0范数表示向量中非零元素的个数（即为其稀疏度），L1范数表示为绝对值之和，而L2范数则指模。</p><span id="more"></span><hr><h3 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h3><h4 id="L-P范数"><a href="#L-P范数" class="headerlink" title="L-P范数"></a>L-P范数</h4><p>x 的 n 范数：x 到零点的 n 阶闵氏距离。即向量元素绝对值的p次方和的1&#x2F;p次幂</p><img src="/da6b54c/1.svg"><p>根据P的变化，范数也有着不同的变化，一个经典的有关P范数的变化图如下：</p><img src="/da6b54c/11.webp"><p>实际上，在0时，Lp并不满足三角不等式的性质，也就不是严格意义下的范数。</p><hr><h4 id="0-范数"><a href="#0-范数" class="headerlink" title="0-范数"></a>0-范数</h4><p>x 的 0 范数：x 到零点的汉明距离。表示向量 x 中非零元素的个数。</p><p>当P &#x3D; 0时，也就是L0范数，L0范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数。</p><img src="/da6b54c/12.svg"><p>对于L0范数，其优化问题为：</p><img src="/da6b54c/13.svg"><p>在实际应用中，由于L0范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。所以在实际情况中，L0的最优问题会被放宽到L1或L2下的最优化。</p><hr><h4 id="1-范数"><a href="#1-范数" class="headerlink" title="1-范数"></a>1-范数</h4><p>x 的 0 范数：x 到零点的汉明距离。表示向量 x 中非零元素的绝对值之和。</p><img src="/da6b54c/2.svg"><p>L1范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）：</p><img src="/da6b54c/14.webp"><p>对于L1范数，它的优化问题如下：</p><img src="/da6b54c/15.webp"> <img src="/da6b54c/16.webp"><p>由于L1范数的天然性质，对L1优化的解是一个稀疏解，因此L1范数也被叫做稀疏规则算子。通过L1可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有100个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用L1范数就可以过滤掉。</p><hr><h4 id="2-范数"><a href="#2-范数" class="headerlink" title="2-范数"></a>2-范数</h4><p>x 的 2 范数：x 到零点的欧氏距离。Euclid范数（欧几里得范数，常用计算向量长度），表示向量元素的平方和再开平方。</p><img src="/da6b54c/3.svg"><p>L2范数是我们最常见最常用的范数了，像L1范数一样，L2也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）:</p><img src="/da6b54c/17.webp"><p>对于L2范数，它的优化问题如下：</p><img src="/da6b54c/18.webp"> <img src="/da6b54c/19.webp"><p>L2范数通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</p><hr><h4 id="∞-范数"><a href="#∞-范数" class="headerlink" title="∞-范数"></a>∞-范数</h4><p>x 的无穷范数：x 到零点的切比雪夫距离。即所有向量元素绝对值中的最大值&#x2F;最小值</p><img src="/da6b54c/4.svg"> <img src="/da6b54c/5.svg"><hr><h3 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h3><h4 id="1-范数-1"><a href="#1-范数-1" class="headerlink" title="1-范数"></a>1-范数</h4><p>列和范数，即所有矩阵列向量绝对值之和的最大值</p><img src="/da6b54c/6.svg"><h4 id="2-范数-1"><a href="#2-范数-1" class="headerlink" title="2-范数"></a>2-范数</h4><p>谱范数，即AᵀA矩阵的最大特征值的开平方（λ为AᵀA的最大特征值）</p><img src="/da6b54c/7.svg"><h4 id="∞-范数-1"><a href="#∞-范数-1" class="headerlink" title="∞-范数"></a>∞-范数</h4><p>行和范数，即所有矩阵行向量绝对值之和的最大值</p><img src="/da6b54c/8.svg"><h4 id="F-范数"><a href="#F-范数" class="headerlink" title="F-范数"></a>F-范数</h4><p>Frobenius范数，即矩阵元素绝对值的平方和再开平方</p><img src="/da6b54c/9.svg"><h4 id="核范数"><a href="#核范数" class="headerlink" title="核范数"></a>核范数</h4><p>即奇异值之和（λᵢ是A的奇异值）</p><img src="/da6b54c/10.svg"><hr><h3 id="机器学习中的范数"><a href="#机器学习中的范数" class="headerlink" title="机器学习中的范数"></a>机器学习中的范数</h3><p>在很多机器学习相关的著作和教材中，我们经常看到各式各样的距离及范数，如：∥𝓍∥、∥𝑿∥，其中，𝓍 和 𝑿 分别表示向量和矩阵。</p><hr><h4 id="与L0范数相关"><a href="#与L0范数相关" class="headerlink" title="与L0范数相关"></a>与L0范数相关</h4><p>在诸多机器学习模型中，比如压缩感知（compressive sensing），我们很多时候希望最小化向量的L0范数。然而，由于L0范数仅仅表示向量中非0元素的个数，因此这个优化模型在数学上被认为是一个NP-hard问题，即直接求解它很复杂、也不可能找到解。需要注意的是，正是由于该类优化问题难以求解，因此压缩感知模型是将L0范数最小化问题转换成L1范数最小化问题。</p><hr><h4 id="与L1范数相关"><a href="#与L1范数相关" class="headerlink" title="与L1范数相关"></a>与L1范数相关</h4><p>L1范数优化问题比L0范数优化问题更容易求解，借助现有凸优化算法（线性规划或是非线性规划），就能够找到我们想要的可行解。鉴于此，依赖于L1范数优化问题的机器学习模型如压缩感知就能够进行求解了。</p><hr><h4 id="正则项与稀疏解"><a href="#正则项与稀疏解" class="headerlink" title="正则项与稀疏解"></a>正则项与稀疏解</h4><p>在机器学习的诸多方法中，假设给定了一个比较小的数据集让我们来做训练，我们常常遇到的问题可能就是<strong>过拟合</strong> (over-fitting) 了，即训练出来的模型可能将数据中隐含的噪声和毫无关系的特征也表征出来。</p><p>为了避免类似的过拟合问题，一种解决方法是在 (机器学习模型的) 损失函数中加入正则项，比如用<strong>L1范数</strong>表示的正则项，只要使得<strong>L1范数</strong>的数值尽可能变小，就能够让我们期望的解变成一个<strong>稀疏解</strong> (即解的很多元素为0)。</p><p>如果我们想解决的优化问题是损失函数 f(𝓍) 最小化，那么，考虑由L1范数构成的正则项后，优化目标就变成：</p><img src="/da6b54c/20.svg"><p>尽管类似的优化模型看起来很“简练”，在很多著作和教材中也会加上这样一句说明：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">只要优化模型的解 𝓍 的 L1范数比较小，那么这个解就是稀疏解，并且稀疏解可以避免过拟合。其中，“稀疏”一词可以理解为 𝓍 中的大多数元素都是0，只有少量的元素是非0的。</span><br></pre></td></tr></table></figure><p>可能比较难理解，接下来做简要说明，为了理解L1范数的正则项和稀疏性之间的关系，我们可以想想下面三个问题：</p><ul><li>为什么L1范数就能使得我们得到一个稀疏解呢？</li><li>为什么稀疏解能够避免过拟合？</li><li>正则项在模型中扮演者何种角色？</li></ul><hr><h5 id="什么是过拟合问题？"><a href="#什么是过拟合问题？" class="headerlink" title="什么是过拟合问题？"></a>什么是过拟合问题？</h5><p>假设我们现在买了一个机器人，想让它学会区分汉字，例如：</p><img src="/da6b54c/21.webp"><p>认定前5个字属于第一类，后5个字属于第二类。在这里，10个字是所有的训练“数据”。如果机器人很聪明，它能够把所有的字都“记住”，看过这10个字以后，机器人学会了一种<strong>分类</strong>的方式：它把前5个字的一笔一划都准确地记在心里。只要我们给任何一个字，如“揪”（不在10个字里面），它就会很自信地告诉你，非此即彼，这个字属于第二类。机器人没见过这个字，它将这个字归为第二类，这可能就错了。</p><p>因为我们可以明显看到，前5个字都带提手旁。所以，“揪”属于第一类。机器人的失败在于它太聪明，而训练数据又太少，不允许它那么聪明，这就是过拟合问题。</p><hr><h5 id="正则项是什么？为什么稀疏可以避免过拟合？"><a href="#正则项是什么？为什么稀疏可以避免过拟合？" class="headerlink" title="正则项是什么？为什么稀疏可以避免过拟合？"></a>正则项是什么？为什么稀疏可以避免过拟合？</h5><p>其实可以让机器人变笨一点，希望它不要记那么多东西。</p><p>还是给它前面测试过的那10个字，但现在机器人已经没办法记住前5个字的一笔一划了（存储有限），它此时只能记住一些简单的模式，于是，第一类字都带有提手旁就被它成功地发现了。</p><p>实际上，这就是L1范数正则项的作用。L1范数会让你的模型变傻一点，相比于记住事物本身，此时机器人更倾向于从数据中找到一些简单的模式。</p><p>机器人原来的解：[把, 打, 扒, 捕, 拉]<br>机器人变傻以后的解：[扌, 0, 0, 0, 0]</p><p>比较正式的解释如下：</p><p>假设我们有一个待训练的机器学习模型：A𝓍 &#x3D; b</p><p>其中， A 是一个训练数据构成的矩阵， b 是一个带有标签的向量，这里的 𝓍 是我们希望求解出来的解。</p><p>当训练样本很少 (training data is not enough)、向量 𝓍 长度很长时，这个模型的解就很多了。</p><img src="/da6b54c/22.webp"><p>如图，矩阵 A 的行数远少于向量 𝓍 的长度。</p><p>我们希望的是找到一个比较合理的解，即向量 𝓍 能够发现有用的特征 (useful features)。使用L1范数作为正则项，向量 𝓍 会变得稀疏，非零元素就是有用的特征了。</p><p>当然这里也有一个比较生动的例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Suppose you are the king of a kingdom that has a large population and an OK overall GDP, but the per capita is very low. Each one of your citizens is lazy and unproductive and you are mad. Therefore you command “be productive, strong and hard working, or you die!” And you enforce the same GDP as before. As a result, many people died due to your harshness, those who survived your tyranny became really capable and productive. [example]</span><br></pre></td></tr></table></figure><p>如果把总人口总量视作向量 𝓍 的长度，那么<strong>优胜劣汰其实相当于增加了一个正则项</strong>。在稀疏的结果中，我们能够保证向量 𝓍 的每个元素都是有用的！</p><p>到这里，我们知道了为什么稀疏可以避免过拟合。</p><hr><h5 id="为什么增加L1范数能够保证稀疏？"><a href="#为什么增加L1范数能够保证稀疏？" class="headerlink" title="为什么增加L1范数能够保证稀疏？"></a>为什么增加L1范数能够保证稀疏？</h5><p>根据L1范数的定义，向量的L1范数是所有元素的绝对值之和，以向量 [x, y]ᵀ 为例，其L1范数为 |x| + |y|</p><p>选取两个向量：x1 &#x3D; [0.1, 0.1]ᵀ 、 x2 &#x3D; [1000, 0]ᵀ</p><p>其中，x1很明显不是一个稀疏向量，但其L1范数∥x1∥ &#x3D; |0.1| + |0.1| &#x3D; 0.2 却远小于稀疏向量x2的L1范数∥x2∥ &#x3D; |1000| + |0| &#x3D; 1000</p><p>仅仅是看L1范数的数值大小，我们可能很难比较向量的稀疏程度，因此，需要结合损失函数。</p><p>再回到前面的问题：A𝓍 &#x3D; b，在平面直角坐标系上，假设一次函数 y &#x3D; ax + b 经过点(10, 5) ，则</p><img src="/da6b54c/23.webp"><p>由于 b &#x3D; 5 - 10a ，所以参数a, b的解有无数组 (在蓝线上的点都是解)。</p><img src="/da6b54c/24.webp"><p>怎样通过L1范数找到一个稀疏解呢？</p><p>我们不妨先假设向量的L1范数是一个常数c ，如下图：</p><img src="/da6b54c/25.webp"><p>它的形状是一个正方形 (红色线)，不过在这些边上只有<strong>很少的点是稀疏</strong>的，即<strong>与坐标轴相交的4个顶点</strong>。</p><img src="/da6b54c/26.webp"><p>把红色的正方形（L1范数为常数）与蓝色的线 (解) 放在同一个坐标系，于是，我们发现蓝线与横轴的交点恰好是满足稀疏性要求的解。同时，这个交点使得L1范数取得最小值。</p><hr><h4 id="最简单的最小二乘线性模型"><a href="#最简单的最小二乘线性模型" class="headerlink" title="最简单的最小二乘线性模型"></a>最简单的最小二乘线性模型</h4><p>最开始，最小二乘的loss（需优化的目标函数）如下：</p><img src="/da6b54c/27.webp"><p>式中，tn是目标变量，xn是观测量（自变量），Φ是基函数（后期推导与核化相关），是w是参数。此式有闭式解，解为：</p><img src="/da6b54c/28.webp"><p>但是我们都知道，矩阵求逆是一个病态问题，即矩阵并不是在所有情况下都有逆矩阵。所以上述式子在实际使用时会遇到问题。为了解决这个问题，可以求其近似解。可以用SGD(梯度下降法)求一个近似解，或者加入正则项（L2范数）。加入正则项是我们这里要说的。加入L2范数的正则项可以解决这个病态问题，并且也可以得到闭式解，在实际使用时要比用SGD快，并且加入正则化后的好处并不仅仅是这些。加入正则项（L2范数）的loss如下：</p><img src="/da6b54c/29.webp"><p>其闭式解为：</p><img src="/da6b54c/30.webp"><p>此式在 λ 不为零时，总是有解的，所以是一个非病态的问题，这在实际使用时很好。除了这一点，2范数的正则项还有其他好处，比如控制方差和偏差的关系，得到一个好的拟合，这里就不赘述了，毕竟这里讲的是范数，有兴趣可以参阅相关资料。</p><p>加入正则项后一般情况下的loss为：</p><img src="/da6b54c/31.webp"><p>好了，我们终于可以专注于范数了。不同范数对应的曲线如下图：</p><img src="/da6b54c/32.webp"><p>上图中，可以明显看到一个趋势，即q越小，曲线越贴近坐标轴，q越大，曲线越远离坐标轴，并且棱角越明显。q &#x3D; 0 和 q &#x3D; ∞ 时极限情况如下：</p><img src="/da6b54c/33.webp"><p>除了图形上的直观形象，在数学公式的推导中，q &#x3D; 0 和 q &#x3D; ∞ 时两种极限的行为可以简记为非零元的个数和最大项。即L0范数对应向量或矩阵中非零元的个数，无穷范数对应向量或矩阵中最大的元素。</p><img src="/da6b54c/34.webp"><p>上图中，蓝色的圆圈表示原问题可能的解范围，橘色的表示正则项可能的解范围。而整个目标函数（原问题+正则项）有解当且仅当两个解范围相切。从上图可以很容易地看出，由于L2范数解范围是圆，所以相切的点有很大可能不在坐标轴上，而由于L1范数是菱形（顶点是凸出来的），其相切的点更可能在坐标轴上，而坐标轴上的点有一个特点，其只有一个坐标分量不为零，其他坐标分量为零，即是稀疏的。</p><p>所以有如下结论，L1范数可以导致稀疏解，L2范数导致稠密解。那么为什么不用L0范数呢，理论上它是求稀疏解最好的规范项了。然而在机器学习中，特征的维度往往很大，解L0范数又是NP-hard问题，所以在实际中不可行。但是用L1范数解是可行的，并且也可以得到稀疏解，所以实际稀疏模型中用L1范数约束。</p><p>至此，我们总结一下，在机器学习中，以L0范数和L1范数作为正则项，可以求得稀疏解，但是L0范数的求解是NP-hard问题; 以L2范数作为正则项可以得到稠密解，并且由于其良好的性质，其解的定义很好，往往可以得到闭式解，所以用的很多。</p><p>另外，从距离的角度说一下范数。1范数对应街区距离，2范数对应大家熟知的欧式距离，无穷范数对应棋盘距离（切比雪夫距离）。</p><img src="/da6b54c/35.webp"> <img src="/da6b54c/36.webp"><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a493823882/article/details/80569888">https://blog.csdn.net/a493823882/article/details/80569888</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20473040">https://www.zhihu.com/question/20473040</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26884695">https://zhuanlan.zhihu.com/p/26884695</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/susanzhang1231/article/details/52127011">https://blog.csdn.net/susanzhang1231/article/details/52127011</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/1b50fbb9.html" rel="prev" title="MAE论文逐段精读-《Masked Autoencoders Are Scalable Vision Learners》"><i class="fa fa-chevron-left"></i> MAE论文逐段精读-《Masked Autoencoders Are Scalable Vision Learners》</a></div><div class="post-nav-item"><a href="/2a00db2e.html" rel="next" title="脑电情绪识别：脑功能连接网络与局部激活信息结合">脑电情绪识别：脑功能连接网络与局部激活信息结合 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%8C%83%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">什么是范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">向量范数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L-P%E8%8C%83%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">L-P范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-%E8%8C%83%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">0-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%8C%83%E6%95%B0"><span class="nav-number">2.3.</span> <span class="nav-text">1-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%8C%83%E6%95%B0"><span class="nav-number">2.4.</span> <span class="nav-text">2-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%88%9E-%E8%8C%83%E6%95%B0"><span class="nav-number">2.5.</span> <span class="nav-text">∞-范数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">矩阵范数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%8C%83%E6%95%B0-1"><span class="nav-number">3.1.</span> <span class="nav-text">1-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%8C%83%E6%95%B0-1"><span class="nav-number">3.2.</span> <span class="nav-text">2-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%88%9E-%E8%8C%83%E6%95%B0-1"><span class="nav-number">3.3.</span> <span class="nav-text">∞-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#F-%E8%8C%83%E6%95%B0"><span class="nav-number">3.4.</span> <span class="nav-text">F-范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E8%8C%83%E6%95%B0"><span class="nav-number">3.5.</span> <span class="nav-text">核范数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%8C%83%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">机器学习中的范数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8EL0%E8%8C%83%E6%95%B0%E7%9B%B8%E5%85%B3"><span class="nav-number">4.1.</span> <span class="nav-text">与L0范数相关</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8EL1%E8%8C%83%E6%95%B0%E7%9B%B8%E5%85%B3"><span class="nav-number">4.2.</span> <span class="nav-text">与L1范数相关</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8E%E7%A8%80%E7%96%8F%E8%A7%A3"><span class="nav-number">4.3.</span> <span class="nav-text">正则项与稀疏解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-number">4.3.1.</span> <span class="nav-text">什么是过拟合问题？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E9%A1%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A8%80%E7%96%8F%E5%8F%AF%E4%BB%A5%E9%81%BF%E5%85%8D%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">4.3.2.</span> <span class="nav-text">正则项是什么？为什么稀疏可以避免过拟合？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A2%9E%E5%8A%A0L1%E8%8C%83%E6%95%B0%E8%83%BD%E5%A4%9F%E4%BF%9D%E8%AF%81%E7%A8%80%E7%96%8F%EF%BC%9F"><span class="nav-number">4.3.3.</span> <span class="nav-text">为什么增加L1范数能够保证稀疏？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.4.</span> <span class="nav-text">最简单的最小二乘线性模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1006</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">145:57</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>