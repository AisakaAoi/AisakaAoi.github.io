<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CV双雄这里之所以是双雄，其实主要想讲的是MoCo和SimCLR"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读-对比学习论文综述-2CV双雄"><meta property="og:url" content="https://aisakaaoi.top/d37a9a38.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="CV双雄这里之所以是双雄，其实主要想讲的是MoCo和SimCLR"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/1.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/2.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/3.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/4.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/5.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/6.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/7.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/8.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/9.webp"><meta property="og:image" content="https://aisakaaoi.top/d37a9a38/10.webp"><meta property="article:published_time" content="2022-05-27T17:08:16.000Z"><meta property="article:modified_time" content="2023-06-18T20:02:00.561Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.top/d37a9a38/1.webp"><link rel="canonical" href="https://aisakaaoi.top/d37a9a38.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>论文阅读-对比学习论文综述-2CV双雄 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">逢坂葵葵</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">53</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">582</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.top/d37a9a38.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文阅读-对比学习论文综述-2CV双雄</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-05-28 01:08:16" itemprop="dateCreated datePublished" datetime="2022-05-28T01:08:16+08:00">2022-05-28</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/d37a9a38.html" class="post-meta-item leancloud_visitors" data-flag-title="论文阅读-对比学习论文综述-2CV双雄" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/d37a9a38.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/d37a9a38.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>13k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>33 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="CV双雄"><a href="#CV双雄" class="headerlink" title="CV双雄"></a>CV双雄</h3><p>这里之所以是双雄，其实主要想讲的是MoCo和SimCLR</p><span id="more"></span><hr><h4 id="《Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning》"><a href="#《Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning》" class="headerlink" title="《Momentum Contrast for Unsupervised Visual Representation Learning》"></a>《Momentum Contrast for Unsupervised Visual Representation Learning》</h4><p>MoCo</p><p>这次主要就讲和其它工作的区别和联系</p><p>MoCo 的主要贡献就是把之前对比学习的一些方法都归纳总结成了一个字典查询的问题，它提出了两个东西</p><ul><li>队列</li><li>动量编码器<br>从而去形成一个又大又一致的字典，能帮助更好的对比学习</li></ul><p>MoCo跟Inst Disc是非常相似的</p><ul><li>它用队列取代了原来的 memory bank 作为一个额外的数据结构去存储负样本</li><li>它用动量编码器去取代了原来 loss 里的约束项，从而能达到动量的更新编码器的目的，而不是动量的去更新特征，从而能得到更好的结果<br>但是整体的出发点以及一些实现的细节都是非常类似的</li></ul><p>MoCo 的这个实现细节：</p><ul><li>首先从模型的角度上来说，它用的是残差网络，它的基线模型都用的是 Res 50，其实 Inst Disc 也用的是 Res 50，模型上是一样的</li><li>最后每个图片的特征维度也沿用了128维</li><li>它也对所有的特征做了 L2 归一化</li><li>至于目标函数，MoCo 采用的是 info NCE，而不是像 Inst Disc 是 NCE 但是算 loss 用的温度也是0.07</li><li>数据增强的方式也是直接借鉴过来的</li><li>包括后面训练的学习率0.03，训练200个epochs这些也都是跟Inst Disc保持一致的</li></ul><p>所以，说 MoCo 是 Inst Disc 一个改进型工作也不为过，但是 MoCo 真正出色的地方其实有两点</p><ul><li>一个是它的改进真的是简单有效，而且有很大的影响力的，比如说它的动量编码器，在后面的 SimCLR、BYOL，一直到最新的对比学习的工作都还在使用。它提出的这个技术不仅在当时帮助 MoCo 第一次证明了无监督学习也能比有监督特征学习的预训练模型好，而且还能产生持续的影响力，帮助之后的工作取得更好的结果，所以它的改进很深刻而且很有效</li><li>另外一个可圈可点的地方就是 MoCo 的写作真的是高人一等非常不一样，其实如果是一个简单直白的写作方式，在语言里先介绍对比学习是什么，然后再介绍之前的工作有哪些，比如说有端到端的工作，然后有看 Inst Disc，这个 memory bank 的这个工作，然后它们各自都有各自的缺点和局限性，所以说提出 MoCo，用队列去解决大字典的问题，用动量编码器去解决字典特征不一致的问题，最后结果很好，第一次证明了在下游任务中用一个无监督训预训练的模型也会比有监督预训练的模型好，那这种写法也是一种很简洁直白明了的写作方式，大部分论文的写作都是按照这个套路来的。但是 MoCo 的作者明显就高了一个层次：引言上来先说这个 cv 和 nlp 之间的区别，以及到底为什么无监督学习在 cv 这边做的不好，然后第二段它才开始讲对比学习，但是它也不是细细地去讲对比学习，或者细细的去讲那些方法，而是直接把之前所有的方法都总结成了一个字典查找的问题，所以直接把问题给归纳升华了，然后在这个框架下，就是 cv 和 nlp 大一统的框架以及所有的对比学习也都大一统的框架之下，然后作者提出了 MoCo 这个框架，希望能用一个又大又一致的字典去整体地提高对比学习的性能，那论文的 scope 整体就扩大了，远不是之前的那种简单的写作方式可以比的，而且这样的写作风格呢还延续到了方法部分，在 3.1 里，作者没有先写一个模型总览图，也没有具体说是什么模型、什么任务，而是先从最后的目标函数入手，说是用 info NCE 来做的，先把正负样本定义了一下，然后再去讲网络结构然后再去讲实现细节和伪代码，而且在 3.1 里，为了让 MoCo 看起来更朴实，在这里没有直接定义输入是什么，也没有定义这个网络结构到底是什么样的，它是说什么样的输入都可以，比如说它可以是图片，也可以是图片块，或者是上下文的图片块（文献46其实就是cpc），至于网络，它说 query 的编码器和 key 的编码器既可以是相同的（invariant spread），也可以是部分共享的，还可以是完全不同的（文献56就是cmc，因为是多个视角嘛所以是多个编码器）</li></ul><p>所以说MoCo这种自顶向下的写作方式也是非常值得借鉴的，但这个真的是需要功力，稍有把握不慎别人可能就看不懂了。</p><hr><h4 id="《A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations》"><a href="#《A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations》" class="headerlink" title="《A Simple Framework for Contrastive Learning of Visual Representations》"></a>《A Simple Framework for Contrastive Learning of Visual Representations》</h4><p>SimCLR(simple contrastive learning)</p><p>这个方法真的是够简单，这就是为什么很多博客在介绍对比学习的时候都用 SimCLR 当例子，因为它概念上更容易理解，方法上也很容易解释，只不过 batch size 太大，一般人不好上手</p><img src="/d37a9a38/1.webp"><ul><li>图二里说，如果有一个 mini-batch 的图片，假如说是 x，对这个 mini-batch 里的所有图片做不同的数据增强就会得到 xi 和 xj，同一个图片延伸得到的两个图片就是正样本，也就是说如果 batch size 是 n 的话，正样本个数就是 n，负样本的个数就是这个 batch size 剩下所有的样本以及它们数据增强过后的样本，也就和 invariant spread 里讲的一样，是两倍的 n 减 1</li><li>然后当有了正负样本之后通过编码器 f 对它进行编码，这两 f 是共享权重，也就说其实只有一个编码器，如果把它想象成一个 res 50 的话，得到的 h（特征表示）是 2048 维了</li><li>SimCLR 的重大创新点其实是在特征之后又加了一个 projector，也就是上图中的 g 函数，它就是一个 mlp 层（只有一个全连接层，后面跟一个 relu 的激活函数），但是就这么简简单单的一层 mlp 能让最后学到的特征在 ImageNet 这个分类任务上直接提点将近 10 个点，这个效果在别的任何的任务里或者说在有监督学习里是很难观测到的，很少有说加一个全连接层就能直接提点 10 个点，所以说是一个非常有趣而且非常惊讶的结果</li><li>但是在这个框架里，可以想象出有一个特征之后再做一个非线性变化，就得到了另外一个特征，也就是最后去做对比学习的那个特征，一般这个特征 z，它的维度会小一点，为了跟之前的工作保持一致性，它也用了 128 维</li><li>最后要衡量一下正样本之间是不是能达到最大的一致性，它采用的是 normalized temperature-scaled 的交叉熵函数。normalized 就是说在这个特征后面进行了 L2 归一化，temperature-scaled 就是说在这个 loss 成了个 tao，所以说其实这个 loss 跟之前说的 infoNCE loss 也是非常接近的</li><li>g 函数只有在训练的时候才用，而在做下游任务的时候，是把 g 函数扔掉了，还是只用 h 这个特征去做下游任务，这样的话跟之前的工作也还是公平对比，因为之前它们如果用 res 50，还是用 res 50 并没有多加一层，加上这个 g 函数只是为了能让模型训练的更好</li></ul><p>和MoCo比起来确实很简单，这里只有一个编码器，既不需要 memory bank，也不需要队列和动量编码器；正负样本全都是从同一个 mini-batch 里来的；整个前向过程非常的直接，就是图片进入编码器编码然后 projector 降维，最后算个对比学习的 loss，非常符合大家对深度学习工作的期待</p><p><strong>前面说 invariant spread 可以看作是 SimCLR 的前身，为什么这么说呢？</strong>本文其实整体的这个思路和结构跟 SimCLR 是基本一致的，SimCLR 跟 Inva Spread 的区别其实都写在 SimCLR 的贡献列表里了</p><ul><li>首先第一个就是它用了更多的数据增强，它发现对比学习真的是需要很强的数据增强的技术</li><li>第二就是它加了一个 g 函数（一个可以学习的分线性的变换，就是一个 mlp 层）</li><li>第三就是它们用了更大的 batch size，而且训练的时间更久，它发现这两个策略都能让网络学到的特征变得更好</li></ul><p><strong>乍一看这些贡献有些读者可能就会觉得这些技术不都或多或少在之前的工作里被提出来过吗？</strong>所以说在这篇论文里，作者专门把相关工作放到了第七节，相当于文章的最后才去讲相关工作，它比较细致的跟之前手工的代理任务做了对比，然后跟最近的对比学习也做了对比，而且作者最后非常谦虚的写了这么一段话：就是说基本上所有的这些单个的这些贡献在之前的工作里都被提出来过了，虽然有的时候这个实现的形式可能不太一样，但是作者强调说 SimCLR 的优越之处就是在于它的成功不是由任何一个单一的设计决定，而是把所有的这些技术结合起来而得到的一个结果，而且作者把详细的消融实验，以及它们的设计选择全都放到了附录里，所以说作者团队真的是非常贴心，而且非常谦虚了</p><p>而事实上呢，SimCLR 这篇文章中提出来的很多技术都对后续的工作产生了长远的影响力，比如说在编码器之后加这么一个 mlp 层，在之后的 MoCo v2、BYOL 这些工作里全都有使用；它使用的数据增强的策略在之后的工作里也是被广泛的使用；它使用 lars 这个优化器去做大 batch size 的这个模型训练，之后 BYOL 也采用了同样的策略。总之 SimCLR 真的是够简单，而且为后续的很多研究铺平了道路</p><p>最后稍微讲一下SimCLR这篇论文里的贡献</p><p>第一个就是<strong>数据增强</strong></p><ul><li>如下图图4所示，SimCLR 这篇论文使用了这么多的数据增强的方法，从最开始的原始图片，到裁剪，到改变色彩，到旋转，使用 cut out，还有使用高斯的噪声和高斯 blur，以及最后使用 sobel 的这种滤波器。真的是把前人想到的这些数据增强的方式全都用了个遍，然后为了让读者知道，到底哪些数据增强有用，哪些数据增强没用，作者还做了详细的这个消融实验</li></ul><img src="/d37a9a38/2.webp"><ul><li>下图中除了最后一列是 average，剩下的数字就是这七种数据增强两两互相合并之后的这个效果如何，比如说中间的对角线其实就是使用一个数据增强，发现其实最有效的两个数据增强就是这个 crop 和这个 color，也就是随机的裁剪和随机的这种色彩变换，其它的数据增强其实最后都是锦上添花、可有可无的，但这两个是必须得有的</li></ul><p>另外一个就是说 SimCLR 这篇文章提出的非线性变换，也就是说在编码器后面加一层 mlp<br>如图8所示，如果 h 是一个 res 50 出来的特征，也就是 2048 维的话，那z就是经过了 projector 之后的维度，一般是 128</p><img src="/d37a9a38/3.webp"><ul><li>g 函数其实里面就包含了一个全连接层和一个 relu 激活函数</li><li>projection head 指的是 non-linear，之所以是 non-linear 是因为有 relu 的激活层（relu 就会把一个线性函数变成非线性）</li><li>linear 线性指的是不要 relu，只加一层全连接层就可以了</li><li>None 其实就是说像 Inva Spread 或者像 MoCo 一样，直接编码器出来的特征拿去做对比学习，不要 projection head</li></ul><p>然后会发现两个很有意思的现象</p><ul><li>第一个就是如果用 non-linear 的层，相比原来什么都不用，结果提了十几个点，所以是非常显著的</li><li>第二个就是说 z 最后的维度不论是 32、64 还是 2048 其实都没太大区别，这就是为什么对比学习现在一般都选一个比较低的特征维度，因为 128 就够了，再高再低其实最后的结果也没有太大的变化</li></ul><p>因为这里提升 10 个点实在是太过诡异，所以作者还做了很多实验<br>去验证这个想法，比如说在表 3 里就做了一些实验，但是也仅仅是一些实验，并不一定能真的证明这个事，至今好像也没有一个理论上的解释</p><hr><h4 id="《Improved-Baselines-With-Momentum-Contrastive-Learning》"><a href="#《Improved-Baselines-With-Momentum-Contrastive-Learning》" class="headerlink" title="《Improved Baselines With Momentum Contrastive Learning》"></a>《Improved Baselines With Momentum Contrastive Learning》</h4><p>因为 MoCo 和 SimCLR 的结果实在是太过惊艳，所以从 2020 年开始就掀起了一波对比学习的狂潮，基本上每天只要去刷 arxiv，都会有对比学习的论文，这波热浪一直到 20 年年底 Vision Transformer 出来以后才逐渐消退</p><p>MoCo v2 其实是一个只有两页的技术报告，严格意义上不算是一篇论文了，但即使只有两页，信息量也是相当大</p><p>MoCo v2 主要就是说，在看到 SimCLR 这个比较好的结果以后，它们发现 SimCLR 里的那些技术都是即插即用型的，所以说它们就把那些就拿过来了，它直接说就在 MoCo 上面做很简单的改动，引入了 mlp projection head 以及使用更多的数据增强，就又刷新 ImageNet 上的最好成绩，不仅比之前的 MoCo 高很多，而且比最新的 SimCLR 也要高很多</p><p>注意，SimCLR 是 2 月 13 号才放到 arxiv 上的，MoCo v2 是 3 月 9 号就放到 arxiv 上了，所以说这个节奏是相当快的</p><p>MoCo v2 具体进行了哪些改进？如下表表1所示</p><img src="/d37a9a38/4.webp"><p>准确的说就四个方面</p><ul><li>加了一个 mlp 层</li><li>加了更多的数据增强</li><li>训练的时候用了 cosine 的 learning rate schedule</li><li>训练更长的 epoch，从 200 变到了 800</li></ul><p>ImageNet 上结果</p><ul><li>灰色的结果 76.5 属于是有监督的这个基线模型</li><li>MoCo v1 只能达到 60.6，差的还是比较远的</li><li>就在上面加上这个 projection head mlp 层，一下准确率就提高到 66.2，就长了 6 个点，所以说加 projection head 不光是对 SimCLR 有用，对 MoCo 也有用，其实对其之后的很多方法都有用，像 SwAV 呢也用了，BYOL 也用了</li><li>如果使用更强的数据增强，就是也能提三个点，从 60 到 63，但是不如 mlp 提升的多</li><li>如果把这个 mlp 和 augmentation 一起用就已经到 67.3 了，就非常高了</li><li>再加上这个 cos 的这个学习率，就到 67.5 还能再提 0.2 个点，那这个提升就很小了可以忽略不计</li><li>最后如果训练更长的时间，训练 800 epochs，就能再提高到 71.1，SimCLR 结果也是这样，如果它训练更久的话，它的结果也会提升很多，一直到现在为止，就连凯明最新的 MAE 这个工作，也是训练了 1,600 个 epochs，而且的这个效果还在继续往上涨</li></ul><p>无监督学习真的是训练的越久或者模型越大，它的结果就会越好</p><p>接下来作者主要跟SOTA进行了比较，其实也就是MoCov1和 SimCLR这些工作，如下表表2所示</p><img src="/d37a9a38/5.webp"><ul><li>在只训练 200 epochs 的情况下，MoCo v2 比 SimCLR 高了大概一个点</li><li>如果训练更长的时间，在训练 800 个 epochs 的时候 MoCo v2 能到 71.1，比 SimCLR 训练了 1,000 个 epochs</li></ul><p>还要好将近 2 个点，所以就意味着 MoCo v2 能更好的利用数据，能在更短的时间内取得更好的结果</p><p>接下来作者又再次强调了一下为什么要用 MoCo 以及 MoCo 相比于 SimCLR 的优越性如上图中表3所示：</p><ul><li>其实这个优越性就在于硬件：机器的内存以及训练的时长</li><li>MoCo v2 的作者使用的机器配置是 8 张 v100 的显卡，MoCo 在普通这个 batch size 256 的情况下就能训练，内存只消耗 5 个 G，其实还有很多的剩余空间，还可以再加大 batch size 或者再增大模型都可以，它非常省内存，而且训练一个模型也只需要 53 个小时，在 ImageNet 这种规模的数据集上来说，两天多的时间已经算是很快了</li><li>如果这个时候换成 end-to-end 这种端到端的学习，也就之前说的 invariant spread 或者 SimCLR，这里主要指的就是 SimCLR 我们如果只用小 batch size 是 256 的时候，SimCLR 在小 batch size 的情况下只有 61.9 的结果</li><li>相对 MoCo v2 来说就差很多了，为什么呢？因为字典不够大、提供的负样本不够多，所以导致对比学习对比不是很有效，而且不光是效果低，它的内存占用 7.4G 也明显高，训练的时长也多了十几个小时，就是全方位呢都不划算</li></ul><p>如果想要端到端的这个学习走 4096 的这个 batch size 就是说让它的 performance 变好，变成 66.6，虽然说还没有 MoCo v2 好，但也差不多，性能上比较相近，那它对硬件的要求就太高了</p><ul><li>比如说对 gpu 的这个内存要求，它需要 93 个 g 的内存，这里画了个脚注，意思就是说这只是估计，因为现在也没有这么大内存的 gpu，所以说它只能估计一下，训练时长当然也就不得而知了</li></ul><p>因为这种端到端的学习方式，包括 SimCLR、BYOL、SwAV 默认都是用 8 台 8 卡机去做训练的,也就是有 64 张 gpu，才能在一两天这个合理的时间内把训练完成，而 MoCo 只需要一台 8 卡机就可以在两天的时间内完成</p><hr><h4 id="《Big-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners》"><a href="#《Big-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners》" class="headerlink" title="《Big Self-Supervised Models are Strong Semi-Supervised Learners》"></a>《Big Self-Supervised Models are Strong Semi-Supervised Learners》</h4><p>SimCLR v2</p><p>其实 SimCLR v2，只是这篇论文一个很小的部分，它只是说怎么从 v1 变到 v2，就是一个模型上的改进，而事实上都在讲如何去做半监督的学习</p><p>它主要想说的体现在它的这个标题里了：非常大的自监督训练出来的模型非常适合去做半监督学习</p><p>模型总览图如下图中图3所示</p><img src="/d37a9a38/6.webp"><p>这篇文章分了三个部分</p><ul><li>第一部分就是 SimCLR，怎样自监督或者说自监督的对比学习去训练一个大的模型出来</li><li>第二部分就是说，一旦有了这么好的一个模型，只需要一小部分有标签的数据，然后去做一下有监督的微调，一旦微调结束了，就相当于有一个 teacher 模型，就可以用这个 teacher 模型去生成很多伪标签，这样就可以在更多的无标签的数据上去做自学习了</li></ul><p>整个框架其实也是受启发于 google 的另外一篇工作（19年的一篇叫 noisy student 的工作）</p><ul><li>因为 noisy student 就是在 ImageNet 数据集上先训练了一个 teacher 模型，然后在 JFT 300M 那个数据集上生成了很多的伪标签，最后一起训练了一个 student 模型，而这个 student 的模型算是 ImageNet 上的 SOTA，大概是 88 点多的准确率，霸占了 ImageNet 上这个 sota 很长时间，大概有一年的时间</li><li>Vision Transformer 就跟这个 noisy student 比过，因为截止到那个时候，noisy student 还是 ImageNet 上的 SOTA</li></ul><p>作者其实就在第三页大概花了半页的篇幅来讲了一讲怎么把 v1 变成 v2 了，其实大概就是提高了这三个点：</p><ul><li>第一个就是大家其实都公认的一个事实，就是用更大的模型，无监督训练就会训练的更好，在这里就换了一个更大的模型，换了一个 152 层的残差网络，同时用了这个 selective kernels，也就是 SK net，这个骨干网络变得非常的强</li><li>第二点改进就是，之前 SimCLR 说 protection head 的 mlp 层特别有用，而且 MoCo v2 也证明了特别特别的有用，所以 SimCLR 的作者就想那一层都这么有用了，把它再变深点会不会更有用，所以它就试了试变成两层变成三层这个性能会不会继续提升，最后发现其实就是两层就够了，原来是 fc + relu，现在是 fc + relu + fc + relu，一层变成两层的 mlp，这个效果呢就最好了，就是加深了这个 projection head</li><li>第三点改进就是它们也使用了动量编码器（这里说 motivated by [29]，就是 MoCo v2 ，[20]就是 MoCo），就是 SimCLR 的作者发现 MoCo 的这个动量编码器真的很管用，所以也想试一试，事实上动量编码器真的管用，后面 BYOL 都用了动量编码器，但在这里作者说动量编码器在 SimCLR 里的提升并不是很大可能就提了一个点，具体原因它们解释说，因为它们已经有非常大的这个 mini-batch，要么是 4096，要么是 8192，所以它们的负样本已经相当多了，所以不论是从字典的大小，还是从字典里特征一致性来说，SimCLR v2 都已经做的很好了，所以说再加这种队列或者加这种动量编码器其实都不会带来很大的提升</li></ul><p>总的来说就是三点改进：</p><ul><li>使用了更大的模型</li><li>加深了 projection head</li><li>引入了动量编码器</li></ul><p>如果不算半监督学习的内容的话，SimCLR v2 也是一个 2 页的技术报告，而且不论是 SimCLR v1 还是 v2，都只做了分类这个任务，但是 MoCo 就广泛的很多了，至少做了四五个下游的任务，而且刷了很多的数据集，所以 MoCo 系列工作就更 cv friendly，所以它投的都是 cv 的会议，而 SimCLR v1 就是 ICML，而 SimCLR v2 就是 Neural IPS，所以说<strong>投对口的会议也很重要</strong></p><hr><h4 id="《Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignment》"><a href="#《Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignment》" class="headerlink" title="《Unsupervised Learning of Visual Features by Contrasting Cluster Assignment》"></a>《Unsupervised Learning of Visual Features by Contrasting Cluster Assignment》</h4><p>SwAV</p><ul><li>Swap</li><li>assignment</li><li>views<br>给定同样一张图片，如果生成不同的视角，不同的 views 的话，希望可以用一个视角得到的特征去预测另外一个视角得到的特征，因为所有这些视角的特征按道理来说都应该是非常接近的</li></ul><p>本文的具体的做法就是把对比学习和之前的聚类的方法合在了一起，当然这么想也不是偶然</p><ul><li>首先，聚类方法也是一种无监督的特征表示学习方式，而且呢它也是希望相似的物体都聚集在某一个聚类中心附近，不相似的物体尽量推开推到别的聚类中心，所以跟对比学习的目标和做法都比较接近</li><li>另外，这篇文章的一作其实之前一直也是做聚类的，它之前就做过 deep cluster 这篇工作，也是一篇非常好的无监督学习的论文</li></ul><p><strong>具体 SwAV 是怎么和聚类的方法融合起来的呢？</strong></p><img src="/d37a9a38/7.webp"><ul><li>上图图 1 把之前对比学习的方法总结了一下画到了左边，然后把 SwAV 的方法画到了右边，这样就比较好对比</li><li>左边当然很好理解了，就是同一个图片，做两次数据增强就得到了 x1、x2，然后所有的样本通过一个编码器，这个编码器有可能就是个 Res 50，也有可能是一个 Res 50 加了一个 projection head，它这里没有明说，反正就是所有的这些都属于一个模型，最后这个模型输出一个特征，一旦有了这个特征，用它做一个对比学习的 loss 就可以了</li><li>SwAV 说，这么做虽然比较简单，但是直接拿所有图片的特征跟特征做对比有点原始而且有点费资源，因为所有的图片都是自己的类，所以其实像 MoCo 一样，取了 6 万个负样本，这还只是个近似，因为其实所有的数据集，所有的负样本理应是 128 万个图片</li><li>SwAV 的作者就想，能不能不去做近似，能不能借助一些先验信息不去跟大量的负样本比，而去跟一些更简洁的东西比，然后 SwAV 的作者就想出来了，可以去跟聚类的中心比（聚类中心就是右图里的 c，也就是个 prototype，它其实就是个矩阵，它的维度是 d 乘以 k，d 是特征的维度，这里的 d 和特征的 d 是一样的，比如说就是之前说的 128 维，这个 k 就是有多少个聚类中心，在这篇文章中它们选的是 3,000，也就是说你有 3,000 个 cluster center，3,000 这个数字也是之前的一些聚类方法在 ImageNet 数据集上常用的一个参数）</li></ul><p>SwAV的前向过程</p><ul><li>前面还是都一样的：一个 mini-batch 的图片，做两次数据增强，得到 x1、x2 分别通过编码器得到最后的特征 z1、z2</li><li>有了 z1、z2 之后并不是直接在这个特征上去做对比学习的 loss，而是说先通过 clustering 让特征 z 和 prototype c 生成一个目标，也就是这里的 q1、q2</li><li><strong>q1、q2 就相当于 ground truth，那它真正要做的这个代理任务是什么呢？</strong>它的意思是说如果 x1、x2 是正样本的话，那 z1 和 z2 的特征就应该很相似，也就跟之前对比学习一样，z1 和 z2 要尽可能的相似</li><li>那如果两个特征非常相似，或者说含有等量的信息的时候，按道理来说应该是可以互相去做预测的，也就是说，如果拿 z1 这个特征去跟 c 去做点乘，按道理来说也是可以去预测 q2；反之亦然，z2 和这个 c 去做点乘也可以预测 q1，所以说点乘之后的结果就是预测，而 ground truth 就是之前按照 clustering 分类而得到的 q1 和 q2</li><li>所以通过这种 Swapped prediction，也就是换位预测的方法，SwAV 可以对模型进行训练</li></ul><p>用聚类的好处到底有哪些？</p><ul><li>首先，就像 SwAV 这篇论文里讲过的一样，如果要跟很多的负样本去做类比，可能就需要成千上万的负样本，而且即使如此也只是一个近似，而如果只是跟聚类中心做对比，则可以用几百或者最多 3,000 个聚类中心，就足以表示了，因为其实也并没有那么多类，ImageNet 也就 1,000 类，COCO 才 80 类，所以说 3,000 个聚类中心就足够用了，这相对于几万个负样本来说还是小了很多的</li><li>第二，这些聚类中心是有明确的语意含义的，如果之前只是随机抽样抽取负样本去做对比的话，那些负样本有的可能还是正样的，而且有的时候抽出来的负样本类别也不均衡，所以不如使用聚类中心有效。其实这就是 SwAV 的基本思想。<strong>（如果对聚类算法比较感兴趣，以先去看 deep cluster deep cluster two，然后再来看这篇 SwAV 的论文）</strong></li></ul><img src="/d37a9a38/8.webp"><ul><li>SwAV 的结果非常好，它不仅比我们之前讲过的方法效果好，其实比之后要讲的 BYOL、SimSiam 这些都好，算是卷积神经网络里用 Res 50 分刷的最高的一篇工作，达到了 75.3</li><li>上图表里的性能做的还是 ImageNet 的 linear evaluation，也就之前说的提前预训练好一个模型以后，把这个模型的 backbone 冻住，只训练最后的那个全连接层</li><li>表中之前不是对比学习的方法都还比较低，可能都是在 60 以下，有了对比学习以后，从 MoCo 开始基本上就上 60 了，然后 CPC v2 刷到 63.8，SimCLR 刷到 70，MoCo v2 刷到 71.1，之后要讲的 BYOL 其实 74点几，SimSiam 也是 74点几</li><li>所以说 75.3 就算是最高的了，而且这个 75.3 是你把 backbone 冻住的情况下去做的，如果跟有监督的基线模型去比的话，这个有监督的基线模型是从头到尾都在 ImageNet 上训练，最后的结果也就是 76.5，所以说 SwAV 已经是非常非常逼近这个结果</li><li>而且当使用更大的模型的时候，也就是像右图里说的一样，把一个 Res 50 变宽，而且就是这里的 2 倍、4 倍、5 倍这么宽的时候，SwAV 的结果还能不停地涨</li><li>当用最大的模型（5 倍的模型）的时候，SwAV 已经跟有监督的模型，差距非常的小，而且 SwAV 也是要比 SimCLR 2、SimCLR 4 要高的，所以说从性能上来讲，SwAV 是真的不错</li></ul><img src="/d37a9a38/9.webp"><p>但其实让 SwAV 有这么好的性能，不光是因为它和聚类的方法融合在了一起，它另外一个主要的性能提升点来自于一个叫 multi crop 的 trick：</p><ul><li>之前的那些对比的学习方法都是用的两个 crop，也就是说一个正样本对 x1、x2 两个图片，如上图左下角所示，本来我们有一个图片，先把它 resize 到 256 * 256，然后随机 crop 两个224 * 224的图片当成 x1 x2，因为这两张图片都非常大，所以它们重叠的区域也非常多，于是它们就应该代表一个正样本</li><li>但总之就是两个 crop，SwAV 的作者就想：用这么大的 crop 明显抓住的是整个场景的特征，如果更想学习这些局部物体的特征，最好能多个 crop，去图片里 crop 一些区域，这样就能关注到一些局部的物体了</li><li>但是增加 crop，也就是说增加 view，会增加模型的计算复杂度，因为相当于使用了更多的正样本</li><li>那如何能同时使用更多的正样本，而又不增加太多的这个计算成本呢？作者就想到了另外一个办法，就是说做点取舍，原来是取了两个 224 * 224 的 crop，现在把这个 crop 变得小一点，变成 160，也就是说取 2 个 160 的 crop 去争取学全局的特征，然后为了增加正样本的数量，为了学一些局部的特征，再去随机选 4 个小一点 crop，然而这 4 个 crop 的大小是 96 * 96，这样的话，就意味着现在有 6 个视角了，而不是像原来一样只有 2 个视角，所以正样本的数量增多了，但是通过这么一种取舍，整体的计算代价还是差不多的</li><li>别看这个想法很简单，这个 multi crop 的技术真的很有用而且它不光是对 SwAV 有用，对其它的对比学习的方法也有用</li></ul><p>作者在下图图3中就做了一些实验</p><img src="/d37a9a38/10.webp"><ul><li>基线模型就是 2 * 224，它用了 multi crop 的这个技术，就是 2 * 160 加上 4 * 96</li><li>如果现在把 multi crop 的技术用到 SimCLR 上会发现它涨了 2.4 个点，这个涨幅还是非常明显，所以说其实如果把 multi crop 这个技术用到 BYOL 上有可能 BYOL 会比 SwAV 的效果高</li><li>接下来作者又对比了一些聚类的方法，对于聚类的这些方法用 multi crop 的方式提点就更多了，对于这几个方式来说都提了 4 个多点，更是非常显著</li><li>所以我们可以看到，如果没有这个 multi crop 的这个技术，把这四个点拿掉，其实 SwAV 的性能也就跟 MoCo v2 是差不多的，也就是说一个纯聚类的方法，或者说聚类和对比学习结合的方法其实也并没有什么优势，真正提点的是 multi crop 的技术</li><li>multi crop 这个技术其实非常朴实了，它其实就是一种思想，就是说全局的和这个局部的特征都要关注，所以说接下来的很多工作，也都借鉴是 multi crop 的这个技术，而不是 SwAV 这篇工作本身</li></ul><p>这里简单提一下：</p><p>CPC v2 其实也是融合了很多的技巧，它用了更大的模型、用了更大的图像块、做了更多方向上的预测任务，把 batch norm 换成了 layer norm，而使用了更多的数据增强，所以这一系列操作下来，CPC v2 直接就把 CPC v1 之前在 ImageNet 上 40 多的准确率一下就拔到 70 多</p><p>informing 其实是 cmc 的作者做的一个分析型的延伸性工作，它论文本身的名字叫 What Makes for Good Views for Contrastive Learning（我们到底选什么样的视角才能对对比学习最好？）</p><ul><li>它主要是提出了一个 InfoMin 的原则，就是最小化互信息 minimi mutual information，那乍一听觉得可能有点奇怪，因为之前大家做的都是 maximize mutual information，都是想要两个视角之间的互信息达到最大，为什么作者这里就想让它达到最小呢？</li><li>其实这里也不是严格意义上的最小，作者其实想说的是，他想要不多不少的互信息，如果最大化互信息以后比所需要的互信息要多，也是一种浪费，而且有可能泛化做的不好，但如果互信息比所需求的这个互信息要少，有可能达不到最优的性能，所以这个才是作者的本意，就是不能一味的最大化这个互信息，而是要不多不少刚刚好</li><li>然后按照 Info Min 的原则选择合适的数据增强，然后拿到合适的对比学习的视角以后，作者发现对于很多的方法都有提升，它们最后在 ImageNet 上也有73，也是相当不错的</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>其实到了第二阶段很多细节都处于统一了，比如说</p><ul><li>目标函数都是用 infoNCE 或者 infoNCE 类似的目标函数去算的</li><li>模型最后也都归一到用一个编码器后面加一个 projection head</li><li>都采用了更强的数据增强</li><li>都想用这个动量编码器</li><li>都尝试着训练的更久</li><li>最后在 ImageNet 上的准确度也逐渐逼近于有监督的基线模型</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/66cd2d4.html" rel="prev" title="经验分享-2022年保研经验分享"><i class="fa fa-chevron-left"></i> 经验分享-2022年保研经验分享</a></div><div class="post-nav-item"><a href="/d2dbdbbb.html" rel="next" title="脑机接口与混合智能-新闻-结合虚拟现实和脑机接口来探测情绪">脑机接口与混合智能-新闻-结合虚拟现实和脑机接口来探测情绪 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#CV%E5%8F%8C%E9%9B%84"><span class="nav-number">1.</span> <span class="nav-text">CV双雄</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AMomentum-Contrast-for-Unsupervised-Visual-Representation-Learning%E3%80%8B"><span class="nav-number">1.1.</span> <span class="nav-text">《Momentum Contrast for Unsupervised Visual Representation Learning》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AA-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations%E3%80%8B"><span class="nav-number">1.2.</span> <span class="nav-text">《A Simple Framework for Contrastive Learning of Visual Representations》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AImproved-Baselines-With-Momentum-Contrastive-Learning%E3%80%8B"><span class="nav-number">1.3.</span> <span class="nav-text">《Improved Baselines With Momentum Contrastive Learning》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8ABig-Self-Supervised-Models-are-Strong-Semi-Supervised-Learners%E3%80%8B"><span class="nav-number">1.4.</span> <span class="nav-text">《Big Self-Supervised Models are Strong Semi-Supervised Learners》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AUnsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignment%E3%80%8B"><span class="nav-number">1.5.</span> <span class="nav-text">《Unsupervised Learning of Visual Features by Contrasting Cluster Assignment》</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.</span> <span class="nav-text">总结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">582</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">53</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">79:28</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>