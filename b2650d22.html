<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="原文  ViT 论文链接：https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id&#x3D;YicbFdNTTy"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读-ViT论文逐段精读-《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》"><meta property="og:url" content="https://aisakaaoi.top/b2650d22.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="原文  ViT 论文链接：https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id&#x3D;YicbFdNTTy"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/1.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/2.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/3.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/4.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/5.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/6.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/7.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/8.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/9.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/10.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/11.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/12.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/13.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/14.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/15.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/16.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/17.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/18.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/19.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/20.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/21.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/22.webp"><meta property="og:image" content="https://aisakaaoi.top/b2650d22/23.webp"><meta property="article:published_time" content="2022-01-29T09:28:10.000Z"><meta property="article:modified_time" content="2023-06-18T19:42:08.975Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.top/b2650d22/1.webp"><link rel="canonical" href="https://aisakaaoi.top/b2650d22.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>论文阅读-ViT论文逐段精读-《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">53</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">728</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.top/b2650d22.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文阅读-ViT论文逐段精读-《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-01-29 17:28:10" itemprop="dateCreated datePublished" datetime="2022-01-29T17:28:10+08:00">2022-01-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/%F0%9F%92%AB%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B-Networks-Model/" itemprop="url" rel="index"><span itemprop="name">💫网络模型 Networks Model</span></a> </span></span><span id="/b2650d22.html" class="post-meta-item leancloud_visitors" data-flag-title="论文阅读-ViT论文逐段精读-《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/b2650d22.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/b2650d22.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>17k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>43 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><div class="pdfobject-container" data-target="./file/paper/2020-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE.pdf" data-height="500px"></div><p><strong>ViT</strong> 论文链接：<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=YicbFdNTTy">https://openreview.net/pdf?id=YicbFdNTTy</a></p><span id="more"></span><hr><h3 id="速览"><a href="#速览" class="headerlink" title="速览"></a>速览</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>其实核心问题就是考虑如何把图像数据 H * W * C,序列化成一个一个词那种结构，自然就想到将图片crop成一个一个patch，假设有N个patch,维度为 p * p * C,reshape加concate一下就变成个N * p^2C,也就类似词向量。</p><hr><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>如下图所示：</p><img src="/b2650d22/1.webp"><h5 id="图像转序列"><a href="#图像转序列" class="headerlink" title="图像转序列"></a>图像转序列</h5><p>将图片 H * W * C,crop成Ｎ个patch,然后在转换成N * (p^2C),同时为了避免模型结构受到patch size的影响，采用Linear project将不同flatten patchs转换成D维向量。这样的话输入图片数据就成了N*D二维矩阵就和词向量矩阵对应上了。</p><img src="/b2650d22/2.webp"><h5 id="Position-embeddings"><a href="#Position-embeddings" class="headerlink" title="Position embeddings"></a>Position embeddings</h5><p>作者用一个可学习的embedding向量去将图像位置信息加入到序列中。</p><img src="/b2650d22/3.webp"><h5 id="learnable-embedding"><a href="#learnable-embedding" class="headerlink" title="learnable embedding"></a>learnable embedding</h5><p>上图中，带*号的粉色框是一个可学习的embedding，记住Xclass,经过encoder后的结果作为整张图像的表示。之所以不用其中一个patch的embedding是因为，这种embedding不可避免带有path的信息，而新增的这个没有语义信息，能更佳反映整张图片。</p><img src="/b2650d22/4.webp"><h5 id="输入transformer-encoder"><a href="#输入transformer-encoder" class="headerlink" title="输入transformer encoder"></a>输入transformer encoder</h5><p>整个公式如下：</p><img src="/b2650d22/5.webp"><hr><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>在中等数据集（例如ImageNet），效果不如resnet，但是在大规模数据集上，表现更佳。</p><img src="/b2650d22/6.webp"> <img src="/b2650d22/7.webp"><hr><p>ViT：过去一年，CV 最有影响力的工作</p><ul><li>推翻了 2012 Alexnet 提出的 CNN 在 CV 的统治地位</li><li>有足够多的预训练数据，NLP 的 Transformer 搬运到 CV，效果很好</li><li>打破 CV 和 NLP 的壁垒，给 CV、多模态 挖坑</li></ul><p>ViT效果有多好？</p><ul><li>CV 任务刷榜</li><li>paperwithcode网站 霸榜 ImageNet （基于 ViT）和 COCO ,目标检测（Swin Transformer ICCV 21 best paper：多尺度的 ViT ）的模型</li></ul><p>四种情况 ViT 都能处理：<br>遮挡、数据分布的偏移（纹理的去除）、鸟头部+对抗的patch、图片打散重新排列组合</p><hr><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p><strong>An image is worth 16 * 16 words</strong></p><p>每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches –&gt; an image is worth 16 * 16 words</p><p>Transformers for image recognition at scale</p><p>transformer 去做大规模的图像识别</p><p>作者来自 Google research 和 Google brain team</p><hr><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>Transformer 在 NLP 是基本操作，i.e., BERT, GPT3, T5, 但 transformer 在 CV 的应用有限。</p><p>CV 里的 attention 是怎么用的呢？<br>attention + CNN, or attention 替换 CNN components 但依然保持 CNN 整体结构。</p><p>如何理解 CNN 整体结构不变？<br>ResNet 50 有 4 个 stages (res2 res3 res4 res5), stage 不变，attention 取代 每一个 stage 每一个 block 里的这个操作。</p><p>本文怎么看 CV 里的 attention?<br>attention 不用和 CNN 绑在一起，和 我 transformer 组队，在 CV 领域 大杀四方。<br>esp, 大规模数据集做预训练，mid-sized or small 数据集做微调，ViT SOTA</p><p>ViT fewer computational resources to train, really?<br>少的训练资源 &#x3D;&#x3D; TPUv 3 + 2500 天。<br>“fewer” 相对来说</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>self-attention 架构， esp Transformers，是 NLP 必选模型。主流方式是 BERT 提出的，大规模数据集预训练，在 特定领域的小数据集 做微调。 Transformer 的 计算高效和可扩展性，1000亿参数都还没有 性能饱和 的现象。<br>i.e., MT-N;P 5300亿参数，SOTA，也没有性能饱和的现象。</p><p>Transformer 应用在 CV 有<strong>难点</strong>吗？<br>计算像素的 self-attention，序列长，维度爆炸<br>Trnasformer 的计算复杂度是 序列长度 n 的 平方 O（n^2）<br>224 分辨率的图片，有 50176 个像素点，（2d 图片 flatten）序列长度是 BERT 的近 100 倍。</p><img src="/b2650d22/8.webp"><p>呼应摘要+文献：<strong>CNN 在 CV 领域 火， Transformer, self-attention 在 NLP 领域 火。CV 如何用 attention 呢？</strong><br>CNN 结构 + self-attention or attention 替代卷积</p><p>CVPR Wang et al. 2018, Local Network, 网络中的特征图 输入 Transformer<br>ResNet 50 最后一个 stage, res4 的 feature map 14 * 14， 196</p><p>降低序列长度的方式：用特征图做 transformer 输入（Wang et al 2018）, replacing the convolutions entirely (Ramachandran et al., 2019 stand-alone attention 孤立自注意力; Wang et al., 2020 axial attention 轴注意力)</p><p>stand-alone attention 孤立自注意力<br>用 local window 局部小窗口 控制 transformer 的计算复杂度，有点像 卷积， 卷积也有 locality，局部窗口卷积。</p><p>axial attention 轴注意力 –&gt; 2 个 1d 顺序操作，降低计算复杂度<br>图片的序列长度 n &#x3D; H * W<br>2d 矩阵 拆分为 2个1d 向量，先在 H 高度 dimension 做一次 self-attention，再 W 宽度 dimension 做一次 self-attention</p><p><strong>replacing the convolutions entirely 好不好呢？</strong><br>理论高效，但硬件无法加速 –&gt; 此类模型都还没有太大。<br>本段（第二段）总结：在大规模的图像识别上，ResNet 还是效果最好的。</p><p><strong>本文 ViT 的工作是什么？</strong><br><strong>现状：</strong>attention 已经在 CV 领域有应用，甚至也有 attention 替代卷积的操作<br><strong>讲故事的角度：</strong>Inspired by the Transformer scaling 可扩展性 success in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.<br>标准 Transformer 直接应用于图片，做最少的修改，不做任何针对视觉任务的特定的改变。</p><p>The fewest possible modifications 是什么呢？<br>把图片划分成很多 patches，每个 patch 元素 是 16 * 16，序列长度 14 * 14 &#x3D; 196个元素</p><p>每一个 patch 经过一个 FC layer(fully connected layer)得到一个 linear embedding，patches 的 linear embeddings 是 Transformer 的输入。</p><p>一个 224 * 224 图片 变成一个 196 个的 16 * 16 图片块（words in NLP）。</p><p><strong>为什么 transformer 的训练是 supervised fashion？</strong><br>NLP 的 Transformer 无监督训练 by language model LM or mask language model MLM；CV 任务的 benchmark 使用有监督训练。</p><p>ViT 把 CV 任务当成 NLP 任务，模型使用的是 BERT, Transformer encoder 简洁框架。Transformer 在视觉也有很好的效果。</p><p><strong>Transformer in CV，之前有人做吗？</strong><br>ICLR 2020 从输入图片里抽取 2 * 2 patches。 2 * 2 size enough：CIFAR-10 32 * 32 图片，16 * 16 会过大。 抽好 patch 之后，在 patches 上 做 self-attention。 –&gt; 技术上的 Vision Transformer</p><p><strong>ViT 和 ICLR 2 * 2 patches 的区别？</strong></p><ul><li>ViT证明了 大规模数据集预训练 （NLP 常用）之后的 Transformer，不需要做 针对视觉任务的 修改，比最好的 CNNs 效果差不多 or 甚至更好。</li><li>2 * 2 patches applicable only to small-resolution images, ViT handles medium-resolution images as well.</li><li>ViT 告诉大家，Transformer 在 vision 领域能拓展到有多好。large 数据集 + large 模型，transformer 能否取代 CNN 地位？</li></ul><p><strong>引言的最后：</strong><br>最想说的结论 or 最想展示的结果：卖点，不用看完整篇论文，就知道此篇论文的贡献。</p><p><strong>ViT 任何情况都很强吗？</strong><br>No<br>mid-sized datasets ImageNet without strong regularization，ViT 比 ResNet of comparable size 弱几个点。</p><p><strong>Why 弱？ expected</strong><br>Transformer 比 CNN 少 inductive biases 归纳偏置<br>inductive biases 归纳偏置：先验知识 or 提前的假设<br>CNN 的 inductive biases 是 locality 和 平移等变性 translation equaivariance（平移不变性 spatial invariance）。</p><p>locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。i.e., 桌椅在一起的概率大，距离近的物品 相关性越强。</p><p>translation equaivariance：f(g(x)) &#x3D; g(f(x))<br>f 和 g 函数的顺序不影响结果。<br>f：卷积 g：平移; 无论先做平移 g 还是先做卷积 f , 最后结果一样。<br>CNN 的卷积核 像一个 template 模板，同样的物体无论移动到哪里，遇到了相同的卷积核，它的输出一致。</p><p>CNN 有 locality 和 translation equivariance 归纳偏置，–&gt; CNN 有 很多先验信息 –&gt; 需要较少的数据去学好一个模型。</p><p>Transformer 没有这些先验信息，只能 从图片数据里，自己学习对 视觉世界 的感知。</p><p><strong>怎么验证 Transformer 无 inductive bias 的假设？</strong><br>在 1400万(ImageNet-21K) - 3000 万(JFT-300)得到图片数据集上预训练 trumps inductive bias, ViT +足够训练数据，CV SOTA。</p><p>VTAB 融合了 19 个数据集，检测模型的稳健性，ViT的 robustness 也很好。</p><p><strong>引言总结：</strong><br><strong>第一段：</strong>Transformer 在 NLP 扩展的很好，没有因为大模型和大数据集而饱和，performance 一直有提升，Transformer 在 CV 里能不能也有大幅度的提升呢？<br><strong>第二段：</strong>前人工作。这么好的 idea 有哪些人做过呢？要讲清楚自己的工作和 related works 的区别<br>之前的工作是 CNN + attention 或者 attention 替代 convolutions，没有工作将 transformer 用到 CV 领域，没有得到很好的扩展效果。<br><strong>第三段：</strong>Vision Transformer 是 standard Transformer with the fewest possible modifications<br>对图片的最少修改是什么？<br>图片变成 16 * 16 的像素块 patches，经过 一个 fc layer 得到的 linear embeddings 输入 transformer<br>ViT 融合了 CV 和 NLP 领域。<br><strong>第四+五段：</strong>show 结果<br>足够多的数据集，ViT 能 SOTA</p><hr><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>explored the direct application of Transformers to image recognition. 直接 用 NLP 的 Transformer 来处理图片。</p><p><strong>和其它 self-attention in CV 的工作不同：</strong>除了将图片转成 16 * 16 patches + 位置编码 之外，没有额外引入 图像特有的 inductive bias</p><p><strong>没有图片的 inductive bias 的好处是什么？</strong><br>不需要对 vision 领域的了解，不需要 domain knowledge，直接把 图片理解成 a sequence of patches, i.e., 一个句子里的很多单词。<br>An image is worth 16 * 16 words.</p><p>直接用 NLP 里的 Transformer encoder， simple yet scalable，大规模预训练数据集，效果非常好。</p><p><strong>ViT 效果有多好？</strong><br>image classification SOTA, relatively cheap to pre-train</p><p><strong>ViT 没有解决的问题？</strong><br>文章挖坑：新问题 or 新模型<br>ViT 挖坑：新模型 ViT<br>**future directions: **新问题 —— CV 除了 image classfication 其他的任务，行不行呢？分割、检测</p><p>DETR (Carion et al. 2020) 目标检测的力作，改变了目标检测 出框的方式。ViT 做其它 CV 任务应该效果也很好。</p><p>2020年 12 月(ViT 1.5月之后)<br>ViT-FRCNN 检测 detection<br>SETR 分割 segmentation （CVPR 论文 11.15完成写作投稿）<br>（3个月后）Swin Transformer 融合 Transformer 和多尺度设计</p><p>Transformer 是 CV 领域的一个通用的骨干网络 backbone<br>另外一个未来工作方向，自监督的预训练方式。<br>NLP 大的 transformer 模型使用 自监督 预训练，ViT有 initial experiments 证明 自监督预训练也可以，但和有监督的训练有差距 still large gap。</p><p>把 ViT 变得很大，would likely lead to improved performance。scaling ViT, ViT-G, ImageNet 90 +%</p><p>ViT 挖坑：</p><ul><li>视觉领域 CV</li><li>多模态，一个 transformer 处理 CV 和 NLP</li></ul><hr><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>Transformer 在 NLP 领域的应用：BERT, GPT<br>Transformer 先在大规模语料库上做预训练，再根据具体的任务数据集进行微调。<br>BERT: denosing mask挖词、完形填空，把masked的词预测出来<br>GPT: language modelling, 预测下一个词 next word prediction<br>完形填空 or 预测下一个词，人为设定。语料句子是完整的，去掉某些词（完形填空） or 最后词（预测下一个词） –&gt; 自监督的训练方式。</p><p>self-attention 在视觉领域的应用</p><p>self-attention to each pixel：❌<br>224 * 224 image: O(n^2 &#x3D; 50176)<br>1k, 4k image: 维度爆炸</p><p>self-attention to each image with approximations：</p><ul><li>不用整张图，只用 local neighborhoods，降低序列长度</li></ul><p>sparse transformer</p><ul><li>全局注意力的近似</li><li>只对 稀疏的点 做注意力</li></ul><p>scale attention by applying attention in blocks of varying size</p><ul><li>把自注意力用到不同大小的 blocks</li><li>in the extreme case only along individual axes 极端情况，只关心轴， axial self-attention，横轴 + 纵轴</li></ul><p>小结：以上 self-attention + CV 效果不错，但工程实现加速很难。可在 cpu gpu跑，但大规模训练不行。</p><p><strong>和 ViT 最相似的工作：</strong><br>ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片<br><strong>ViT 胜在哪里：</strong>更大的 patches 16 *16 + 更大的训练数据集</p><p>CV 中 检测、分类、视频处理、多模态 self-attention with CNNs</p><p><strong>另一个相似工作：image GPT</strong><br>GPT 是 NLP 的生成模型，image GPT 无监督预训练，生成模型。</p><p>image GPT 也用了 transformer 图片（降低分辨率和 color space）。用训练好的 image GPT or 直接把 image GPT 当成特征提取器， ImageNet 准确率 72%；ViT ImageNet 准确率 88.5%</p><p><strong>ps：最近爆火的 MAE</strong><br>在 BEiT 或 MAE 论文之前，生成式网络 在 CV 比 判别式网络 弱很多。<br>MAE 生成式模型 在 ImageNet-1k 做训练，比判别式模型好。分类 ✔，目标检测 ✔ （transfer learning）</p><p>还有其它和 ViT 相似的工作吗？</p><ul><li>用比 ImageNet 还大的数据集做预训练，大力出奇迹</li><li>Sun et al 2017 JFT-300M 数据集，CNN 的效果随数据集增加而提升</li><li>Djolonga et al 2020 研究大数据集预训练迁移到小数据集的效果</li><li>在 ImageNet-21K 或 JFT-300M 数据集做预训练，迁移到 ImageNet 或 CIFAR-100 效果怎么样</li></ul><p>本文 ViT 和这些相似论文的关系？<br>ViT 关注 ImageNet-21K 或 JFT-300M 数据集， 不训练 ResNet，训练 Transformer</p><p>Related work 写作总结：<br>方方面面相关的都写到了，也列举了非常相似的工作 ICLR 2020 2*2 patches, image GPT 生成模型，大数据集 BIT 相关的文章</p><p>Related work 目的：<br>让读者知道在你的工作之前，别人做了哪些工作，你跟他们的区别在哪里</p><p>related work 章节的详细不会降低论文的创新性，反而加分，让整个文章变得更简单易懂。</p><hr><h3 id="ViT模型"><a href="#ViT模型" class="headerlink" title="ViT模型"></a>ViT模型</h3><p>ViT 尽可能使用 original Transformer，享受 Transformer efficient implementations。<br>NLP 中 Transformer 很火，有很多 Transformer 的高效实现</p><h4 id="Vision-Transformer-模型图"><a href="#Vision-Transformer-模型图" class="headerlink" title="Vision Transformer 模型图"></a>Vision Transformer 模型图</h4><p>Model overview<br>好图：以图读论文，讲解 ViT 直接复制</p><img src="/b2650d22/9.webp"><p><strong>Input:</strong> 1 张图<br><strong>Process:</strong> 九宫格 9 patches –&gt; Flattened Patches (3 * 3 –&gt; 1 * 9 拍平) –&gt; Linear Projections —&gt; Patch embedding<br><strong>Why need position embedding?</strong><br>self-attention 所有元素两两算自注意力，和顺序位置无关。但图片的 patches 是有顺序的，+ position embedding</p><p>Patch embedding + position embedding &#x3D;&#x3D; token 包含 图片 patch 信息 和 patch 在原图中的位置信息。</p><p><strong>ViT 对 图片的操作：</strong> 划分 patches，flatten patches 的线性投影 + patches 的位置信息，得到输入 transformer 的 tokens</p><p>得到 tokens 之后，对 visual tokens 进行 NLP 操作：<br>tokens 传入 Transformer encoder，得到很多输出。</p><p><strong>Q: 每一个 token 都有输出，用哪个输出分类呢？</strong><br>借鉴 BERT， extra learnable {class} embedding –&gt;, a special classification token, * in figure 1.<br>也有 position embedding, 0(永远是0)</p><p><strong>Q: Why works？self-attention O(n^2)</strong><br>self-attention in transformer encoder，所有的 tokens 在做两两的交互信息。因此，也会和所有的 图片 patches 的 token 交互，从而从图片 patches + position 的 embedding 学到有用信息，最后用做分类判断。</p><p><strong>Q: 从怎么得到最后的分类？ 通用MLP Head</strong><br>输入 一个通用的 MLP Head，得到 Class，cross-entropy 损失函数训练模型。</p><p><strong>Q: ViT 用了标准的 transformer 结构，ViT的特点是什么？</strong><br>图片 patches 化 + position embedding 转化为 tokens</p><p><strong>ViT 前向过程</strong><br><strong>Vision 问题 变成 NLP 问题</strong></p><p>图片 X： 224 * 224 * 3 (RGB, 3 channels)<br>patches 数 N： 224 ^ 2 &#x2F; 16 ^ 2 &#x3D; 14 ^ 2 &#x3D; 196<br>每一个 patch 的维度：16 * 16 * 3 (RGB, 3 channels) &#x3D; 768<br>Linear Projection 全连接层 E: 768( 不变，patch 计算而来 ) * D(embedding_dim) 768 或 更大<br>图片 X * E &#x3D; patches (196 patches 个数 * 768 每个 patch 的维度) * E ( 768 * D ) &#x3D; 196 * D (768)</p><img src="/b2650d22/10.webp"><p>**Vision to NLP done! **<br>a 2d image –&gt; a sequence 1d tokens</p><p><strong>Q: 进入 transformer encoder 的序列长度？</strong><br>196 * 768(图片对应的 tokens) 拼接 concatenate [CLS] token (1 * 768) &#x3D; 197 * 768</p><p><strong>Q: position embedding 怎么加 patch embedding？sum()</strong><br>图1 的 1-9 不是真正使用的 position embedding，实际的 position embedding 表，1 - 5 行 代表 图1 的 1 - 5 值。<br>每行向量的维度是 1 * 768<br><strong>相加 sum：</strong><br>patch embedding（197 * 768） + position embedding （(1 CLS + 196 patches) * 768）&#x3D; （197 * 768）</p><img src="/b2650d22/11.webp"><p>ViT base: 12 heads<br>MLP：放大 4 倍，再缩小到原维度大小<br>Transfomer encoder 输入输出维度一致，可以直接叠加 L 个</p><img src="/b2650d22/12.webp"><hr><h4 id="Vision-Transformer正文"><a href="#Vision-Transformer正文" class="headerlink" title="Vision Transformer正文"></a>Vision Transformer正文</h4><p>公式的具体值计算，参考上一小节。<br>有了具体含义的公式字符，也不那么可怕了呢 o(<em>￣▽￣</em>)ブ</p><p>ViT 用的是 BERT 1d position embedding，图片 2d aware position embedding 结果也差不多。</p><p><strong>D.3 Head type and class token 作者的消融实验</strong><br>ViT 除了标准的 transformer，关键部分是 怎么对图片进行预处理 和 怎么对图片最后的输出进行后处理。</p><p><strong>class token：证明 标准的 transformer 做视觉，没问题！</strong><br>控制和 NLP 的差异：使用 BERT 的 CLS，CLS 在 NLP 理解为 一个全局的对句子理解的特征；ViT 的 CLS 理解为 一个图像的整体特征。</p><p>CLS token + MLP (tanh acitvation) &#x3D;&#x3D; 分类</p><p>CV 通常的 全局特征：i.e., Res50<br>feature map (14 * 14) –&gt; GAP globally average-pooling 全局平均池化 –&gt; a flatten vector 全局的图片特征向量 –&gt; MLP 分类</p><p>类似的，Transformer 的 输出元素 + GAP 可以用做全局信息 + 分类吗？ Ok</p><p><strong>CV 的 CLS GAP 和 NLP 的 CLS 效果差异不大。</strong></p><p>CLS-Token 和 GAP 的 适用参数 不一样。</p><p><strong>位置编码： 1d 2d relative 无所谓</strong></p><p><strong>1d：</strong>NLP 1, 2, 3, …, 9 D<br><strong>2d：</strong>D &#x2F; 2 * D &#x2F; 2<br>11 12 13<br>21 22 23<br>31 32 33</p><img src="/b2650d22/13.webp"><p><strong>relative: offset</strong><br>绝对距离转相对距离，1 - 9 和 -4, …, 0, …, 4</p><p><strong>为啥都是 0.64 左右，无所谓？</strong><br>ViT 直接作用于 14 * 14 patches，而不是 224 * 224 像素。较少数量的 patches 之间的相对位置信息，容易学到。</p><hr><h4 id="ViT-正文-CLS-continued"><a href="#ViT-正文-CLS-continued" class="headerlink" title="ViT 正文 CLS continued"></a>ViT 正文 CLS continued</h4><p>CLS 可用 GAP global average pooling 替换<br>1d position embedding 可用 2d or relative 替换</p><p>ViT 对齐 标准的 transformer，选用 NLP 里常用的 CLS 和 1d position embedding</p><p>Appendix: Transformer multi-head 解释，i.e., 卷积解释 in CNN papers</p><p><strong>公式总结 ViT 的前向传播过程</strong></p><img src="/b2650d22/14.webp"><p><strong>Inductive bias</strong></p><p>CNN 的 inductive bias: locality 局部性, translation equivalence 平移等变性。在 CNN 模型每一层都有所体现，&#x3D;&#x3D;》模型的先验知识从头到尾，贯穿整个模型。</p><p><strong>ViT 比 CNN 的 inductive bias 少, only MLP</strong><br>In ViT, only MLP layers are local and translationally equivariant, <strong>while the self-attention layers are global.</strong></p><p>ViT 的 inductive bias in images：<br>图片 切成 patches；+ position embedding（随机初始化，没有携带 2d 位置信息）</p><p>ViT 的 patches 块的 2d 位置信息 + spatial relations 图像块之间的场景信息，都需要重新学。 &#x3D;&#x3D;》 <strong>ViT 没有很多 inductive bias</strong> &#x3D;&#x3D;》中小型数据集训练 ViT 效果不如 CNN</p><p><strong>Hybrid architecture</strong><br>Transformer: 全局建模能力强<br>CNN: data-efficient 不用那么多训练数据</p><p>前 CNN + 后 Transformer –&gt; Hybrid archtecture<br><strong>不同的图片预处理方式：</strong>不划分 patches，采用 CNN (Res50 的 feature map 14 * 14 &#x3D; 196)，过全连接层 <strong>E</strong> Linear projections 得到图片的 embedding</p><img src="/b2650d22/15.webp"><p><strong>ViT 的图片预处理方式：</strong><br>把一张图划分成 patches，直接过全连接层 fc</p><hr><h4 id="Fine-tuning-and-higher-resolution"><a href="#Fine-tuning-and-higher-resolution" class="headerlink" title="Fine-tuning and higher resolution"></a>Fine-tuning and higher resolution</h4><p>微调时用大图片尺寸 i.e., 256 * 256， 320 * 320 而不是 224 * 224，效果更好</p><p><strong>Q: 预训练好的 ViT 可以在更大尺寸的图片上为条码？</strong><br>if patch size 不变 16 * 16，更大尺寸的图片 –&gt; 序列长度的增加 i.e., 14 * 14 –&gt; 20 * 20 in 320 * 320 image</p><p>Transformer 理论上，可以处理任意长度。<br><strong>But，提前训练好的 position embedding 可能失效</strong></p><p>1 - 9 的九宫格 图片 patches 位置编码 –&gt; patches 增多，1 - 25 位置编码</p><p><strong>Q: patches 数增多，如何使用 已预训练好的 位置编码呢？</strong><br>2d 插值，torch 的 interpolate 函数实现；但也不是任意长度增加都能保持效果。<br>256 –&gt; 512 –&gt; 768 长度的增加，直接使用差值，最后效果掉点。（采样定理）</p><p>插值 interpolate 临时解决方案，ViT 微调时的一个局限。</p><p>ViT 用了图片 2d 结构 的 inductive bias 地方：resolution adjustment 尺寸改变 和 patch extraction 抽 patches</p><hr><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>对比 ResNet, ViT, Hybrid ViT (CNN 特征图，不是图片直接 patch 化) 的 representation learning capabilities 表征学习能力。</p><p>为了了解每个模型预训练好 到底需要多少数据，在不同大小的数据集上预训练，然后在很多 benchmark tasks 做测试。</p><p>考虑模型预训练的计算成本时，ViT performs very favourably 表现很好， SOTA + fewer resource 训练时间更少</p><p>ViT 的自监督训练，可行，效果也还不错，有潜力；一年之后，MAE 用自监督训练 ViT 效果很好。</p><p><strong>4.1 Setup</strong><br><strong>datasets:</strong><br>ImageNet-1K: 1000 classes, 1.3M images<br>ImageNet-21K: 21000 classes, 14M images<br>JFG-300: 303M images Google 不开源</p><p>下游任务：分类 CFIAR etc.</p><p><strong>Model variants</strong></p><p>ViT Base, Large, Huge<br>Layers, Hidden size D, MLP size, Heads 相应增加</p><p>模型变体 &#x3D; (Base, Large, Hugh) + (patch size 表示)<br>ViT-L&#x2F;16 使用 Large 参数 和 patch 16 * 16 输入</p><p>Q: Why patch size in name of model variants?<br>ViT 模型的 patch size 变化时, i.e., 16 * 16 –&gt; 32 * 32 or 8 * 8, 模型的位置编码会变化</p><ul><li>transformer 输入的序列长度 与 patch size 成反比</li><li>patch size 越小，一张图片的 patches 数越多，训练越贵 because of 序列长度的增加</li></ul><p><strong>结果</strong></p><p>ViT-H&#x2F;4 秀肌肉 刷榜</p><img src="/b2650d22/16.webp"><p>和 CNN 的工作 BiT-L, Noisy Student 做对比<br><strong>BiT-L:</strong> CNN比较大的模型，ViT论文作者团队自己的工作<br><strong>Noisy Student:</strong> ImageNet 当时表现最好的方法。用 伪标签 pseudo-label 去 self-training</p><p>ViT-H&#x2F;14 训练比 ViT-H&#x2F;16 贵，效果和 BiT-L 差不多，优势不明显。怎么突出 ViT 的好呢？</p><p>ViT 训练更便宜。TPUv3 天数：ViT-H&#x2F;14 2.5K, BiT-L 9.9K, Noisy Student 12.3K</p><p>ViT 优点：效果好 + 训练快</p><p><strong>结果分析</strong></p><p>Vision Transformer 到底需要多少数据才能训练好？<br>图3 灰色区域 ResNet 的效果，圆圈 ViT 的效果</p><p><strong>Take home message: 图 3</strong></p><p>如果想用 ViT，至少需要 ImageNet-21K 14M 大小的数据集</p><ul><li>小于整个数据量，CNN 更合适，更好的利用 inductive bias，ViT 没有特别多 inductive bias 需要更多数据训练。</li></ul><p>数据集规模比 ImageNet-21K 更大时，Vision Transformer 效果更好，因为可扩展性 scaling 更好。</p><img src="/b2650d22/17.webp"><p><strong>图 4 Linear few-shot evaluation</strong></p><p>图 3 ViT 和 ResNet 比，加了强约束：dropout、weight decay、label smoothing，约束了 ViT 的发挥</p><p>linear evalution: 把 ViT 预训练好的模型 直接作为 特征提取器，不 fine-tune，+ 一个 logistic regression 得到分类结果。</p><p>Few-shot：5-shot，在 ImageNet 做 linear evaluation 时，每类图片随机选取 5 个 samples，evaluation 很快，做 消融实验。</p><p>linear few-shot evaluation 采用 JFT 数据集 10M, 30M, 100M, 300M。来自同一个数据集，数据没有 distribution gap，模型的效果更能体现 Vision Transformer 本身特质。</p><p>ViT 图4 效果 和 图3 差不多。<strong>如何用 ViT 做小样本学习，未来研究方向之一。</strong></p><p><strong>图 5 用 ViT 比 CNNs 便宜 的实验支持</strong><br>大家的印象：Transformer 又大又贵，很难训练</p><img src="/b2650d22/18.webp"><p>average-5：ImageNet-real, Pets, Flower, CIFAR10, CIFAR100 平均<br>ImageNet 单独的对比</p><p>同等计算复杂度：ViT 比 ResNet 效果好，印证了 ViT 训练更便宜</p><p><strong>Q: Hybrid 模型，CNN 抽取出来的特征，能不能帮助 Transformer 更好的学习呢？</strong></p><ul><li>小模型，Hybrid 模型吸收 CNN 和 Transformer 的优点，效果好。不需要很多的数据预训练，达到 Transformer 的效果</li><li>大模型，Hybrid 模型 和 Transformer 差不多，甚至不如 Transformer 模型。<strong>Why？</strong><br><strong>如何 预处理图像，如何做 tokenization 很重要</strong>，后续论文有研究</li></ul><p>整体趋势：模型增加，除了 Hybrid 模型有点饱和（饱和：增加到一个平台值后，不增加了）。ResNet 和 Transformer 都没有饱和。</p><p><strong>4.5 Inspecting Vision Transformer</strong></p><p>可视化分析 ViT 内部表征 internal representations: <strong>Patch embedding, position embedding</strong><br><strong>ViT 第一层 Linear projection E 学到了什么？</strong></p><p>Figure 7 (left) embed RGB value 前 28 个主成分</p><img src="/b2650d22/19.webp"><p>Vision Transformer 和 CNN 学到的很像，类似 gabor filter 有颜色、纹理， 可以做 plausible basis functions，可以描述每个图像块的底层信息 a low-dimensional representation of the fine structure within each patch.</p><p><strong>Position embedding 能学到一些表示位置距离的信息</strong></p><ul><li>patch 自己本身 相似度高 黄色 1</li><li>学到了距离的概念<ul><li>(4, 4) 黄色中心点，越边缘，相似度越低，颜色越蓝</li></ul></li><li>学到了 行 和 列 的距离规则<ul><li>同行同列，颜色条 的表示<br>虽然是 1d 的 position embedding，但已经学到了 2d 的图像位置概念；所以换成 2d position 提升不多。</li></ul></li></ul><img src="/b2650d22/20.webp"><p><strong>Self-attention 有没有起作用？</strong></p><p>用 Transformer 的原因：自注意力 能模拟长距离的关系。</p><ul><li>NLP 一个很长的句子里，开头的一个词和结尾的一个词 可能互相有关联。</li><li>CV 里 很远的两个像素点之间 也能做自注意力。</li></ul><p><strong>ViT 的 self-attention 是不是 很远的像素点也能有交互？</strong><br>ViT-L&#x2F;16 有 24 层（横坐标值），五颜六色的点：transformer 每层 multi-head 的heads，ViT-L 16 heads, 每一列有 16 个点</p><img src="/b2650d22/21.webp"><p>纵轴是 mean attention distance<br>d_ab &#x3D; l_ab * A_ab &#x3D; ab 两点 pixel 之间的距离差 * ab 两点之间的attention weights<br>d_ab 的大小，反映模型能不能注意到很远的 2 个 pixels</p><ul><li>self-attention 刚开始能注意到 10 - 110 pixels</li><li>self-attention 刚开始就注意到全局的信息；CNN 刚开始第一层的感受野 receptive filed 很小，只能看到附近的 pixel</li></ul><img src="/b2650d22/22.webp"><p>网络加深，模型学到的特征越来越 high level，越来越有语义信息，像素的自注意力距离 越来越远，不是靠邻近的像素点做判断。</p><p><strong>证明 自注意力 有学到 很远距离的 pixel 信息，证明 by 图6</strong></p><p>ViT 最后一层 output 的 token 的 self-attention 折射（逆向映射）回 原来的输入图片。ViT 真的学到了一些概念：狗、飞机</p><img src="/b2650d22/23.webp"><p>Globally 全局来说，输出的 token 是融合全局的特征信息，ViT 模型可以关注到 和 classfication 分类相关的图像区域。</p><p><strong>4.6 self-supervision</strong></p><p>如何用 自监督 的方式 训练一个 vision transformer？</p><p>很重要，22页全文，别的结果都在 appendix，自监督的结果在正文。</p><p>因为 NLP 的 transformer 都是用 large scale self-supervised pre-training <strong>大规模、自监督</strong> 的方式预训练的。</p><p><strong>NLP 的 自监督：</strong>BERT 完形填空 Mask language model，GPT 生成，<strong>预测下一个词</strong> by language model</p><p>ViT 借鉴 BERT，创建一个专属于 vision 的目标函数，masked patch prediction。一张图片的某些 patches 随机抹掉，ViT 重建缺失的patches</p><p>Note：从 模型、目标函数上，CV 和 NLP 的大一统。</p><p>但是，ViT-B&#x2F;16 with masked patch prediction 在 ImageNet <del>80% 准确率。</del>80% 比 从头训练 ViT 好 2%，比 supervised pre-training 低 4%。</p><p><strong>ViT 和 contrastive pre-training 的结合： future work</strong> i.e., MOCOv3, DINO</p><p><strong>contrastive learning:</strong> 2020 年 CV 最火的 topic，是所有 自监督学习方法表现最好的。</p><hr><h3 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h3><p>写作：简洁明了、有轻有重（重要结果放正文），图表清晰。</p><p>内容：Vision Transformer 挖了一个大坑：各个角度的分析，提升 or 推广</p><p>task 任务角度：ViT 只做了分类，检测、分割、其它领域的任务 future work</p><p>ViT 结构的角度：</p><ul><li>改刚开始的 tokenization</li><li>改 transformer block, i.e., self-attention 换成 MLP works<ul><li>MetaFormer 认为 transformer work 的原因是 transformer 的架构，不是 transformer 某些特殊的算子</li><li>MetaFormer，self-attention 改成 （不能学习的）pooling 池化操作；甚至改成 Identity，不用注意力</li></ul></li><li>改 目标函数：有监督、or 不同的自监督训练方式</li></ul><p>ViT 的大坑：</p><ul><li>打通了 CV 和 LP 之间的鸿沟</li><li>挖了一个更大的<strong>多模态</strong>的坑<ul><li>视频、音频、基于 touch 的信号</li><li>各种 modality 的信号都可以拿来用</li></ul></li></ul><p><strong>CNN, self-attention, MLP 鹿死谁手？</strong><br>犹未可知，期待下一个改进的 vision transformer</p><ul><li>一个简洁、高效、通用的视觉骨干网络 CV backbone，甚至完全不用任何标注信息</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/4004cc9d.html" rel="prev" title="脑机接口与混合智能-新闻-最新CNN反超Transfromer之作：ConvNeXt"><i class="fa fa-chevron-left"></i> 脑机接口与混合智能-新闻-最新CNN反超Transfromer之作：ConvNeXt</a></div><div class="post-nav-item"><a href="/78187af6.html" rel="next" title="CS231n: Convolutional Neural Networks for Visual Recognition [2019中文] - Lecture 1 Introduction">CS231n: Convolutional Neural Networks for Visual Recognition [2019中文] - Lecture 1 Introduction <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">原文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9F%E8%A7%88"><span class="nav-number">2.</span> <span class="nav-text">速览</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3"><span class="nav-number">2.1.</span> <span class="nav-text">思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%BD%AC%E5%BA%8F%E5%88%97"><span class="nav-number">2.2.1.</span> <span class="nav-text">图像转序列</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Position-embeddings"><span class="nav-number">2.2.2.</span> <span class="nav-text">Position embeddings</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learnable-embedding"><span class="nav-number">2.2.3.</span> <span class="nav-text">learnable embedding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5transformer-encoder"><span class="nav-number">2.2.4.</span> <span class="nav-text">输入transformer encoder</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">2.3.</span> <span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">标题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">4.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">5.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">6.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">7.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ViT%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.</span> <span class="nav-text">ViT模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vision-Transformer-%E6%A8%A1%E5%9E%8B%E5%9B%BE"><span class="nav-number">8.1.</span> <span class="nav-text">Vision Transformer 模型图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vision-Transformer%E6%AD%A3%E6%96%87"><span class="nav-number">8.2.</span> <span class="nav-text">Vision Transformer正文</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT-%E6%AD%A3%E6%96%87-CLS-continued"><span class="nav-number">8.3.</span> <span class="nav-text">ViT 正文 CLS continued</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-tuning-and-higher-resolution"><span class="nav-number">8.4.</span> <span class="nav-text">Fine-tuning and higher resolution</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">9.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E8%AE%BA"><span class="nav-number">10.</span> <span class="nav-text">评论</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">728</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">53</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">2.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">102:10</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haru02.model.json"},"display":{"position":"right","width":208,"height":520},"mobile":{"show":false},"log":false});</script></body></html>