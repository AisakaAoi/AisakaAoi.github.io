<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="原文  MoCo 论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1911.05722"><meta property="og:type" content="article"><meta property="og:title" content="MoCo论文逐段精读-Momentum Contrast for Unsupervised Visual Representation Learning"><meta property="og:url" content="https://aisakaaoi.github.io/da364ed.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="原文  MoCo 论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1911.05722"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/da364ed/19.webp"><meta property="article:published_time" content="2021-12-15T16:08:23.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:39.648Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/da364ed/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/da364ed.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>MoCo论文逐段精读-Momentum Contrast for Unsupervised Visual Representation Learning | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1006</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/da364ed.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">MoCo论文逐段精读-Momentum Contrast for Unsupervised Visual Representation Learning</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-12-16 00:08:23" itemprop="dateCreated datePublished" datetime="2021-12-16T00:08:23+08:00">2021-12-16</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/da364ed.html" class="post-meta-item leancloud_visitors" data-flag-title="MoCo论文逐段精读-Momentum Contrast for Unsupervised Visual Representation Learning" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/da364ed.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/da364ed.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>7.8k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>19 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><div class="pdfobject-container" data-target="./file/paper/2020-Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning.pdf" data-height="500px"></div><p><strong>MoCo</strong> 论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.05722">https://arxiv.org/abs/1911.05722</a></p><span id="more"></span><iframe src="//player.bilibili.com/player.html?aid=422340209&bvid=BV1C3411s7t9&cid=461830701&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen></iframe><p>这次论文精读李沐博士继续邀请了亚马逊计算机视觉专家朱毅博士来精读 <strong>Momentum Contrast（MoCo）</strong>，强烈推荐大家去看本次的论文精读视频。朱毅博士和上次一样讲解地非常详细，几乎是逐词逐句地讲解，在讲解时把 <strong>MoCo</strong> 相关领域的研究也都介绍了，听完之后收获满满。</p><p><strong>MoCo</strong> 获得了 CVPR2020 最佳论文提名，是视觉领域使用<strong>对比学习</strong>的一个里程碑工作。<strong>对比学习</strong>目前也是机器学习领域最炙手可热（？）的一个研究方向，由于其简单、有用、强大，以一己之力盘活了从2017年开始就卷的非常厉害的计算机视觉领域。<strong>MoCo 是一个无监督表征学习的工作，其不仅在分类任务，在检测、分割和人体关键点检测任务上都逼近或超越了有监督学习模型</strong>；<strong>MoCo</strong> 的出现证明我们可能<strong>并不需要大量标注好的数据去预训练</strong>。下图中 <strong>Yann LeCun</strong> 将机器学习比作一块蛋糕，强化学习是蛋糕上的樱桃、有监督学习是蛋糕上的奶油、无监督学习才是那块大蛋糕，才是问题的本质，目前很多的大模型都是通过自监督学习得到的。</p><img src="/da364ed/1.webp"><hr><h3 id="对比学习介绍"><a href="#对比学习介绍" class="headerlink" title="对比学习介绍"></a>对比学习介绍</h3><p>在开始精读论文之前，朱毅博士首先介绍了什么是<strong>对比学习</strong>。如下图所示，有三张图片，图1、2为同一个人不同的表情，图3为dog，<strong>在训练时不会为这三种图片去标注</strong>。将三种图片输入到模型中，模型会得到三张图片各自特征。由于图1、2为同一个人、对比学习就是让特征 f₁ 、 f₂ 比较接近，而特征 f₃ 与另外两个特征在特征空间相距较远，这就是对比学习需要达到的目的。</p><img src="/da364ed/2.webp"><p>虽然在对比学习中并不需要为图片进行标注，但是仍然需要知道哪些图片是相似的，哪些图片是不相似的，在计算机视觉中通常使用代理任务来完成。举一个具体的例子 instance discrimination，假设有 n 张图片，选取一张图片 xᵢ，经过裁剪和数据增强后得到两张新的图片 xᵢ¹ 和 xᵢ²，则这两张图片和原来的图片就是相似的，也被称为正样本，其余图片即 j ≠ i，则为负样本。对比学习的灵活之处就在于正负样本的划分，例如同一张图片不同视角可看作为正样本，视频中同一段视频任意两帧可以看为正样本，RGB和深度图也可看作为正样本等等。正是由于其灵活性，对比学习的应用才如此之广。</p><img src="/da364ed/3.webp"><hr><h3 id="标题、摘要、引言、结论"><a href="#标题、摘要、引言、结论" class="headerlink" title="标题、摘要、引言、结论"></a>标题、摘要、引言、结论</h3><p>先是论文<strong>标题</strong>，论文标题的意思是：使用动量对比去做无监督视觉表征学习，<strong>MoCo</strong> 就来自于论文前两个单词前两个字母。简单介绍什么是<strong>动量</strong>，<strong>动量</strong>在数学上就是加权移动平均。例如 yₜ &#x3D; m × yₜ₋₁ + (1 − m) × xₜ，yₜ₋₁ 为上一时刻的输出，xₜ 为当前输入，m 为动量参数；当 m 很大时，yₜ 就取决于上一时刻输出，其更新就很缓慢；当 m 很小时，yₜ 就取决于当前时刻输入。</p><p>作者团队来自于<strong>FAIR</strong>，就不过多介绍了，五个人谷歌学术引用数达到了50万+。</p><img src="/da364ed/4.webp"><hr><p>下面是论文<strong>摘要</strong>，摘要写的很简洁，总共只有7句话。</p><ul><li>第1句话直接介绍主题，我们提出了 <strong>MoCo</strong> 用于无监督视觉表征学习。第2句话意思是我们把对比学习看作是<strong>字典查询</strong>，我们建立了一个动态字典，使用到了<strong>队列</strong>和<strong>移动平均编码器</strong>。</li><li>第3句话意思是使用队列和移动平均编码器，我们可以建立一个很大且一致的字典，有助于对比无监督学习。</li><li>第4-6句话是模型效果，<strong>MoCo</strong> 在 <strong>ImageNet</strong> 分类上取得了很有竞争力的结果，其中 <strong>linear protocol</strong> 的意思是说将主干网冻结，只训练分类头。更重要的，将 MoCo 学到的特征迁移到下流任务时，在7个检测和分割任务上，<strong>MoCo</strong> 都超过它的有监督预训练对手，<strong>counterpart</strong> 的意思是有监督和无监督训练都使用同一个网络，例如ResNet-50。</li><li>最后一句话的意思是，在许多视觉任务上，无监督和有监督特征学习之间的鸿沟被大幅度的填上了。</li></ul><img src="/da364ed/5.webp"><hr><p>下面是论文<strong>引言</strong>部分，总共有6段。</p><ul><li>第1段说无监督学习在自然语言处理任务中取得了很大的成功，但是在计算机视觉中，仍然是有监督预训练占统治定位。原因可能是各自信号空间的不同。语言任务有着离散的信号空间（单词、词根、词缀等）；但是计算机视觉，原始信号往往是连续、高维的，在构建字典时会有很多问题。</li><li>第2段说最近的无监督学习研究都使用了<strong>对比学习</strong>。这些方法可以看作是<strong>构建动态字典</strong>。使用编码器网络将图像或图像块表示成 <strong>key</strong>。无监督学习训练编码器时是这样进行字典查询：<strong>一个编码后的 query 应该和它匹配的 key 相似，而和其它 key 不相似</strong>。这样一个学习就变成了最小化对比损失的问题。</li><li>第3段说我们想构建这样的一个字典：<strong>（1）大（2）在训练时保持一致</strong>。大的字典可以让我们采样到想要的连续、高维视觉空间；字典中的 key 应该尽可能使用相同或相似的编码器来表示，这样它们和 <strong>quary</strong> 的比较才能一致。如何让字典又大又一致，作者在后面会详细介绍。</li></ul><img src="/da364ed/6.webp"><p>第4段说我们提出了 <strong>MoCo</strong> 模型，如下图所示，通过比较 <strong>query</strong> 和 <strong>key</strong> 地相似性来训练编码器。我们用<strong>队列</strong>来存储字典数据，当前时刻数据编码后新的特征入队，最老的数据特征出队，这样字典大小和mini-batch大小就解耦了，就能保证构建一个大的字典；同时使用动量去更新编码器参数，能保证字典中的特征尽可能地一致。使用数学公式表达的话就是：θₖ &#x3D; mθₖ₋₁ + (1 − m)θq。</p><img src="/da364ed/7.webp"><p>第5段介绍了<strong>代理任务</strong>，作者使用个体判别作为代理任务，即同一张图片不同视角的 <strong>query</strong> 和 <strong>key</strong> 看作是相似的。在 <strong>ImageNet</strong> 数据线性分类问题上 <strong>MoCo</strong> 显示出了很有竞争力的结果。第6段作者说无监督学习最主要的目的是将预训练好的特征迁移到下游任务中。在7个检测和分割任务上，<strong>MoCo</strong> 都有着很好的效果。无论是百万张图片还是10亿张图片，<strong>MoCo</strong> 都工作地很好。意味着无监督学习和有监督学习之间的差距越来越小了，在许多应用中逐渐可以替代有监督预训练模型。</p><img src="/da364ed/8.webp"><hr><p>下面是<strong>结论和讨论</strong>部分，结论就1句话，我们的无监督学习方法在许多计算机视觉数据和任务上都有着很好的结果。<strong>MoCo</strong> 从百万数据到十亿数据性能提升是有的，但是很小，可能是这些大规模数据未充分利用，希望有更高级的代理任务来提高模型性能；有可能将 <strong>MoCo</strong> 调整到像 <strong>masked auto-encoding</strong> 这样的代理任务上（最近作者就提出了大火的 <strong>MAE</strong>）。最后作者希望 <strong>MoCo</strong> 能在其它对比学习研究中有帮助。</p><img src="/da364ed/9.webp"><hr><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>下面是论文<strong>相关工作</strong>部分，无监督学习通常包含两方面：<strong>代理任务</strong>和<strong>损失函数</strong>。代理任务通常不是大家实际感兴趣的任务 (如检测、分类、分割)，而是为了学习一个好的数据特征表示；损失函数可以和代理任务分开研究，<strong>MoCo</strong> 关注的就是损失函数研究。</p><p>损失函数是为了衡量模型的预测输出和固定目标之间的差异，如通过 <strong>L1、L2</strong> 损失重构像素或通过交叉熵对输入进行分类。对比学习的损失测量的是样本对在特征空间的相似性，在训练过程中，目标通常是不固定的。对抗学习的损失衡量的是概率分布的差异，经常用在无监督数据生成。</p><p>各种各样的代理任务被提出来，如重构整张图、重构某个 <strong>patch</strong>、给图片绘上颜色。不同的代理任务可以和对比学习损失函数结合使用，如 <strong>CPC、CMC</strong>。</p><img src="/da364ed/10.webp"><hr><h3 id="MoCo方法、实验"><a href="#MoCo方法、实验" class="headerlink" title="MoCo方法、实验"></a>MoCo方法、实验</h3><p>下面是论文<strong>方法</strong>部分，对比学习以及最新它的一些最新进展，都可以看作成是训练一个编码器，从而去做一个<strong>字典查询</strong>的任务。假设有一个编码好的查询 <strong>q</strong>，以及一系列已经编码好的样本，也就是 {undefined k₀、k₁、k₂}等，这些可以看作是字典中的 <strong>key</strong>。</p><p>这里存在一个假设：在字典中只有一个 <strong>key</strong> 是跟 <strong>query</strong> 匹配的，两个互为正样本对，这个 <strong>key</strong> 叫做 <strong>key positive</strong>（k₊）。定义好了正样本和负样本，接下来就需要一个对比学习的目标函数，这个对比学习的目标函数最好能满足以下要求：（1）当 <strong>q</strong> 和唯一的正样本 k₊ 相似的时候，它的loss值应该比较低；（2）当 <strong>q</strong> 和其他所有的 <strong>key</strong> 都不相似的时候，这个loss的值也应该比较低。通过点积计算相似性，我们使用 <strong>InfoNCE</strong> 当作对比学习损失函数，形式为：</p><img src="/da364ed/11.webp"><p>公式中的 τ 是一个温度超参数，是一个标量，如果忽略掉它，就会发现，其实这个 <strong>InfoNCE</strong> 损失就是交叉熵损失，唯一的区别就在于在<strong>交叉熵损失中k指代的是数据集里类别的多少</strong>，但是在对比学习的 <strong>InfoNCE</strong> 损失中，k 指的是<strong>负样本的数量</strong>。</p><p>通常来说，查询 <strong>q</strong> 是一个输入 x_q 通过一个编码器 f_q 得到的，同理所有的 <strong>k</strong> 的表示也都是输入 xᵏ 通过一个编码器 fₖ 得到，输入和模型具体的实现是由具体的代理任务决定。既可以是图片，也可以是图片块，或者是含有上下文的一系列的图片块；对于模型，<strong>q</strong> 的编码器和 <strong>key</strong> 的编码器既可以是一样的、也可以是部分共享的，还可以是不一样的。</p><img src="/da364ed/12.webp"><hr><p>下面介绍<strong>动量对比学习</strong>，对比学习是一种在高维的连续的输入信号上去构建字典的一种方式。高维和连续指的是图片，字典是动态的，之所以是动态的是因为这个字典中的 <strong>key</strong> 都是随机选取的，而且 <strong>key</strong> 的编码器在训练的过程中也是在不停的变化。如果想学一个好的特征，这个字典就必须拥有两个特性（<strong>大</strong>，大的字典能够包含很多语义丰富的负样本，从而有助于学到更有判别性的特征；<strong>一致性</strong>主要是为了模型的训练方便）基于以上动机，作者就提出了动量对比学习。</p><ul><li><p>首先就是<strong>把一个字典用队列的形式表现出来</strong>。队列其实就是一种数据结构，一般被称作是一个<strong>fifo</strong>（先进先出）的数据结构。作者在这里是用一个队列去代表一个字典，也就是说整个队列就是一个字典，里面的元素就是放进去的 <strong>key</strong>。在模型训练的过程中，每一个 <strong>mini-batch</strong> 就会有新的一批 <strong>key</strong> 被送进来，同时也会有一批老的 <strong>key</strong> 移出去，所以用队列的好处是可以重复使用那些已经编码好的 <strong>key</strong>，而这些 <strong>key</strong> 是从之前的那些 <strong>mini-batch</strong> 中得到的。这样使用了队列之后，<strong>就可以把字典的大小和mini-batch的大小彻底剥离开了</strong>，就可以在模型的训练过程中使用一个比较标准的mini-batch size，一般是128或者是256，但是字典的大小可以变得非常大，它的大小非常灵活，而且可以当作一个超参数一样单独设置。同时在算对比学习目标函数的时候只是取一个近似，而不是在整个数据集上算loss，使用队列的数据结构，可以让维护这个字典的计算开销非常小。</p></li><li><p>用队列的形式可以使这个字典变得很大，但是也因为使用了非常大的字典，也就是非常长的队列，导致没办法给队列中所有的元素进行梯度回传，也就是说，<strong>key</strong> 的编码器无法通过反向传播的方式去更新它的参数。如果想更新这个 <strong>key</strong> 的编码器，其实有一个非常简单的方法：就是每个训练迭代结束后，将更新好的 f_q 编码器参数直接复制过来给 fₖ 编码器就可以了。这个想法简单确实是简单，但是作者紧接着说这种方式的结果并不好，原因是一个快速改变的编码器降低了这个队列中所有 <strong>key</strong> 的特征的一致性。因此作者提出了<strong>动量更新</strong>的方法，如果将 <strong>key</strong> 编码器参数设为 θₖ，<strong>q</strong> 编码器的参数设为 θ_q，那 θₖ 就是以下面公式进行更新：</p></li></ul><img src="/da364ed/13.webp"><ul><li>上式中 m 是动量参数，它是一个0到1之间的数。<strong>q</strong> 的编码器 ，是通过梯度反向回传来更新模型参数，θₖ 除了刚开始是用 θ_q 初始化以外，后面的更新大部分主要是靠自己。如果动量参数 m 设的很大，那么 θₖ 更新就非常缓慢，所以作者接下来说，因为使用了这种动量更新的方式，虽然在队列里的 <strong>key</strong> 都是由不同的编码器产生得到的，但是因为这些编码器之间的区别极小，所以产生的 <strong>key</strong> 的一致性都很强。</li></ul><img src="/da364ed/14.webp"><ul><li><p>下面作者还介绍了 <strong>MoCo</strong> 和之前研究的对比。之前的对比学习研究都可以看作是<strong>字典查找</strong>，但是它们都或多或少受限于<strong>字典的大小</strong>和<strong>字典的一致性</strong>问题，这里作者将之前的方法总结了一下，归纳成了两种结构。第一种就是比较直接的<strong>端到端学习</strong>，如下图所示编码器可以通过梯度回传来更新模型参数。由于模型的正负样本都是从同一个 <strong>mini-batch</strong> 里来的，也就是 x_q 和 xₖ 都是从同一个 <strong>batch</strong> 中来的，它做一次前向传播就能得到所有样本的特征，而且这些样本是高度一致的，因为都是来自一个编码器。听起来确实很美好，编码器能用反向回传学习，特征也高度一致了，但是它的<strong>局限性就在于字典的大小</strong>，因为在端到端的学习框架中，<strong>字典的大小和 mini-batch size 的大小是等价的</strong>，如果想要一个很大的字典，里面有成千上万个 <strong>key</strong> 的话，也就意味着 <strong>mini-batch size</strong> 的大小必须也是成千上万的，这个难度就比较高了。端到端学习的优点在于编码器是可以实时更新的，所以导致它字典里的那些 key 的一致性是非常高的，但是它的缺点在于因为它的字典大小（就是batch-size的大小），导致这个字典不能设置的过大，否则硬件内存吃不消。</p></li><li><p>在 <strong>memory bank</strong> 中其实就只有一个编码器，<strong>query</strong> 的编码器是可以通过梯度回传来进行更新学习。但是对于字典中的 <strong>key</strong> 是没有一个单独的编码器，<strong>memory bank</strong> 把整个数据集的特征都存到了一起，对于 <strong>ImageNet</strong> 来说，<strong>memory bank</strong> 中就有128万个特征（看上去好像很大，但是每一个特征只有128维，所以即使整个 <strong>memory bank</strong> 中有128万个 <strong>key</strong> ，最后也只需要600M的空间就能把所有的这些key存下来了）。一旦有了这个 <strong>memory bank</strong>，在每次模型做训练的时候，只需要从 <strong>memory bank</strong> 中随机抽样很多的 <strong>key</strong> 出来当作字典就可以了。这里也有一个问题：因为这里的特征都是在不同时刻的编码器得到的，而且这些编码器都是通过梯度回传来进行快速更新的，这也就意味着这里得到的<strong>特征都缺乏一致性</strong>。</p></li><li><p>显然，无论是<strong>端到端的学习</strong>还是 <strong>memory bank</strong> 的方法，都和作者说的一样，<strong>受限于字典大小</strong>和<strong>特征一致性</strong>这两方面中的至少一个，所以为了解决之前这两种做法的局限性，作者就提出了 <strong>MoCo</strong>。<strong>MoCo</strong> 采用队列的形式去实现字典，从而使得它不像端到端的学习一样受限于 <strong>batch-size</strong> 的大小，同时为了提高字典中特征的一致性，MoCo使用了<strong>动量编码器</strong>。</p></li></ul><img src="/da364ed/15.webp"><p>到这里，其实 <strong>MoCo</strong> 的主要贡献旧已经讲完了，但是如果对<strong>对比学习</strong>不是很熟的人来说可能还是不太理解 <strong>MoCo</strong> 的前向过程到底是什么样子的，可惜的是这篇论文并没有提供一个很直观、形象的模型总览图，取而代之的是伪代码，写得相当简洁明了，理解和复现都比较容易。</p><img src="/da364ed/16.webp"><hr><p>下面是论文<strong>实验</strong>部分，作者分别在 <strong>ImageNet-1K</strong> 和 <strong>Instagram-1B</strong> 数据集上进行了模型训练，使用的主干网是 <strong>ResNet-50</strong>。首先是<strong>线性分类</strong>结果的展示，在完成了无监督学习的预训练之后，将模型的 <strong>backbone</strong> 冻住，只将它作为一个特征提取器，然后在上面训练一个全连接层去充当分类器，是在 <strong>ImageNet</strong> 验证集上测试，<strong>top-1</strong> 分类准确率。</p><ul><li><p>如左图所示，三种对比学习方法结果对比：黑色的线表示的是端到端的学习，它的结果只有三个点，因为受限于显卡内存，蓝色的线表示的是 <strong>memory bank</strong> 的形式，它可以用很大的字典，所以它可以随着字典增大一直训练，但是它的效果整体上要比端到端学习和 <strong>MoCo</strong> 的结果都要差一截。作者说这可能是因为特征的不一致性导致的。橙色的线表示 <strong>MoCo</strong>，<strong>MoCo</strong> 确实可以有很大的字典，之所以停在65536这个数字，从图中可以看到，从16384到65536性能也已经比较饱和了，所以再大也可能不会带来更多的性能提升了。如果拿 <strong>MoCo</strong> 和端到端学习的方法做比较，可以发现它们的曲线在刚开始的时候的重合度还是比较高的，但是作者说，因为没有实验的支撑，不知道黑线是否能继续按照现有的趋势继续增长下去，有可能结果会更高，也有可能结果会更低，但是因为做不了实验，所以无从得知。</p></li><li><p>如下图表格所示，动量使用一个相对较大的值（0.999或者0.9999）的时候性能是最好的，差不多都是59，这就说明了一个变化非常缓慢的编码器是对对比学习有好处的，因为它能够提供一个一致性的特征。但是当把动量逐渐变小，变到0.99或者是0.9的时候，性能的下降就比较明显了，尤其是当直接去掉动量，直接将 <strong>query</strong> 的编码器拿过来当 <strong>key</strong> 编码器用的时候，就会发现不光是性能下降的问题，整个模型甚至都不能收敛。</p></li><li><p>右下图是和其它分类方法的比较，首先可以发现，对比学习的效果还是不错的，因为准确率要比没有使用对比学习得到得结果要好。作者还强调：在无监督学习中，模型的大小还是非常关键的（模型越大，一般效果就会越好），所以只是比较最后的准确率，而不去关注模型大小的话，就不太公平了，从图中可以看到 <strong>MoCo</strong> 既能在小模型上得到最好的效果，也能在大模型的比较中得到最好的结果。</p></li></ul><img src="/da364ed/17.webp"><p>无监督学习最重要的目标是学习到可迁移的特征，作者用视觉领域中最常见、应用最广的检测任务来做无监督的<strong>MoCo</strong> 预训练模型和 <strong>ImageNet</strong> 的有监督预训练模型之间的比较。表2种第一行使用的是<strong>随机初始化</strong>的模型再做微调，所以它是一个基线网络，分数比较低；第二行使用的是 <strong>有监督ImageNet</strong> 的预训练的模型做初始化然后再做微调，也就是一个比较强的基线结果；最后两行分别是 <strong>MoCo</strong> 在 <strong>ImageNet-1M</strong> 上和在 <strong>Instagram-1</strong> 上做无监督预训练当作模型的初始化，然后再做微调。可以看到大多数结果显示 <strong>MoCo</strong> 在 <strong>ImageNet-1M</strong> 上的预训练就已经超过了有监督的预训练模型。当换成更大的数据集的时候还会有进一步的提升。</p><p>接下来作者又再次比较了三种对比学习的检测结果，从表3可以看到 <strong>MoCo</strong> 和前面两种方式比起来确实是好了很多，而且最主要的是之前的两种方法都没有超越有监督预训练模型的结果，只有MoCo是真的超越了。</p><img src="/da364ed/18.webp"><p>作者又在 <strong>COCO</strong> 数据上进行了对比。除了在设置a里面 <strong>MoCo</strong> 的模型稍显逊色，在剩下的三个设置下，<strong>MoCo</strong> 预训练的模型都比 <strong>ImageNet</strong> 有监督预训练模型得到的效果要好。</p><img src="/da364ed/19.webp"><hr><p>最后简单总结下，<strong>MoCo</strong> 在很多的视觉任务上，已经大幅度的把无监督和有监督之间的坑给填上了。<strong>MoCo</strong> 在 <strong>Instagram</strong> 数据集中是要比 <strong>ImageNet</strong> 训练出来的模型要好的，而且是在所有任务上普遍表现的都很好，这说明了 <strong>MoCo</strong> 的可扩展性很好，也就是说如果有更多的数据，<strong>MoCo</strong> 有可能就能学到更好的模型，这和 <strong>NLP</strong> 中得到的结论是一样的，这也符合了无监督学习的终极目标。</p><p><strong>MoCo</strong> 这篇论文以及它高效的实现，能让大多数人有机会用普通的GPU就能跑对比学习的实验、做研究。因为 <strong>MoCo</strong> 在各个视觉任务上取得了更好的性能，也激发了很多后续分析性的工作，去研究 <strong>MoCo</strong> 学出来的特征到底和有监督学出来的特征有什么不同，还能从别的什么方向去提高对比学习。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1C3411s7t9/">https://www.bilibili.com/video/BV1C3411s7t9/</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/2b3c20c7.html" rel="prev" title="深度学习-关于BN和LN的一些补充"><i class="fa fa-chevron-left"></i> 深度学习-关于BN和LN的一些补充</a></div><div class="post-nav-item"><a href="/928002b2.html" rel="next" title="深度学习-归一化层BN、LN、IN、GN、SN">深度学习-归一化层BN、LN、IN、GN、SN <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">原文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">对比学习介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%A2%98%E3%80%81%E6%91%98%E8%A6%81%E3%80%81%E5%BC%95%E8%A8%80%E3%80%81%E7%BB%93%E8%AE%BA"><span class="nav-number">3.</span> <span class="nav-text">标题、摘要、引言、结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">4.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MoCo%E6%96%B9%E6%B3%95%E3%80%81%E5%AE%9E%E9%AA%8C"><span class="nav-number">5.</span> <span class="nav-text">MoCo方法、实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1006</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">145:57</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haru02.model.json"},"display":{"position":"right","width":208,"height":520},"mobile":{"show":false},"log":false});</script></body></html>