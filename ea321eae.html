<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇学习报告来源:《Inter-subject Transfer Learning with End-to-end Deep Convolutional Neural Network for EEG-based BCI》，作者针对深度学习在认知脑机接口特别是跨被试分类问题上还有较大提升空间，提出了一个基于深度卷积神经网络（CNN）的端到端框架用于在单通道原始脑电图数据中检测被试者的注意力状态。"><meta property="og:type" content="article"><meta property="og:title" content="基于端到端深度神经网络的跨被试注意力检测方法"><meta property="og:url" content="https://aisakaaoi.github.io/ea321eae.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本篇学习报告来源:《Inter-subject Transfer Learning with End-to-end Deep Convolutional Neural Network for EEG-based BCI》，作者针对深度学习在认知脑机接口特别是跨被试分类问题上还有较大提升空间，提出了一个基于深度卷积神经网络（CNN）的端到端框架用于在单通道原始脑电图数据中检测被试者的注意力状态。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/ea321eae/8.webp"><meta property="article:published_time" content="2022-10-12T14:47:57.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:42.263Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/ea321eae/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/ea321eae.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>基于端到端深度神经网络的跨被试注意力检测方法 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1009</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/ea321eae.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">基于端到端深度神经网络的跨被试注意力检测方法</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-10-12 22:47:57" itemprop="dateCreated datePublished" datetime="2022-10-12T22:47:57+08:00">2022-10-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/ea321eae.html" class="post-meta-item leancloud_visitors" data-flag-title="基于端到端深度神经网络的跨被试注意力检测方法" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/ea321eae.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/ea321eae.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>15 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇学习报告来源:《Inter-subject Transfer Learning with End-to-end Deep Convolutional Neural Network for EEG-based BCI》，作者针对深度学习在认知脑机接口特别是跨被试分类问题上还有较大提升空间，提出了一个基于深度卷积神经网络（CNN）的端到端框架用于在单通道原始脑电图数据中检测被试者的注意力状态。</p><span id="more"></span><hr><h3 id="研究背景与内容"><a href="#研究背景与内容" class="headerlink" title="研究背景与内容"></a>研究背景与内容</h3><p>深度学习首先在语音识别和计算机视觉等领域获得了成功的应用，随后在脑机接口等其他研究领域也获得了关注，但是深度学习技术应用于认知BCI的研究目前还较少。基于脑电图的认知BCI是本研究的范围，旨在评估和增强注意力等认知功能。在这类以被试注意力水平为控制信号的BCI系统中，如何从脑电图中准确检测被试注意力状态至关重要。作者在先前工作[1]的基础上,引入了一个新的框架来解决单通道脑电图的注意检测问题,该技术可以显著的提高被试间注意力检测任务的准确性。</p><hr><h3 id="研究数据与方法"><a href="#研究数据与方法" class="headerlink" title="研究数据与方法"></a>研究数据与方法</h3><h4 id="研究数据"><a href="#研究数据" class="headerlink" title="研究数据"></a>研究数据</h4><p>共有120名健康受试者进行了Stroop颜色测试[2,3]，并从这些受试者中收集的脑电图数据作为该研究的研究数据。具体方法：在测试过程中，一个彩色的单词出现在屏幕上，受试者被要求说出这个单词的颜色。实验框图如图1所示：每位参与者重复进行40次实验，每次实验包括约6秒钟的Stroop测试(注意)，以及同等时长的休息阶段(不注意)。Stroop测试界面如图2所示，界面上呈现的单词颜色和其含义不一致，要求受试者集中注意力，根据单词的颜色选择答案。在此过程中受试者经历了一种精神状态的变化(专注&#x2F;不专注)。每个实验大约需要10分钟。为了更便捷的记录长时间的脑电数据，实验使用双极性、单通道的干电极头环设备，电极放置于额区(Fp1-Fp2)处，采样频率为256Hz。</p><img src="/ea321eae/1.webp"><div align="center">图1 实验框图。（每个Stroop测试后有等时长的休息时间）</div><img src="/ea321eae/2.webp"><div align="center">图2 测试演示示例</div><hr><h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>采用一个长度为2s的滑动窗口（窗与窗之间数据重叠率为50%）对连续脑电图时间序列进行分割, 经目视筛选以及阈值法去除有噪声的片段, 阈值为±100µv, 最后数据经过0.5Hz的高通滤波。序列分割示意图如图3所示：</p><img src="/ea321eae/3.webp"><div align="center">图3 连续脑电图时间序列分割图</div><hr><h4 id="数据输入神经网络特征构造"><a href="#数据输入神经网络特征构造" class="headerlink" title="数据输入神经网络特征构造"></a>数据输入神经网络特征构造</h4><p>基于为网络提供一维输入的单通道脑电图数据，作者定义了三种未预提取特征的网络输入表示，分别为：<br>（1）数据表示1 (DR1):原始脑电图数据。<br>（2）数据表示2 (DR2):原始脑电图片段在0.5-40Hz带通滤波。<br>（3）数据表示3 (DR3):对原始脑电图片段进行5个经典波段的滤波：δ(0.5-4Hz),θ(4-8Hz),α(8-12 Hz),β(12-30Hz),和较低的γ(30-40 Hz)。</p><p>注：以上数据均经过了2.1中所述的数据预处理。</p><hr><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>早期的卷积神经网络是由一系列卷积层和池化层组成。为了加速训练、避免过拟合和更好地保存信息，研究者多次尝试通过批处理归一化和dropout[4]等扩展对深度卷积神经网络进行升级。在本次研究中，也利用了其中的一些技术。将2.2中所述的三种脑电图数据表示形式作为输入导入网络。考虑到输入数据是时间序列，因此采用了跨时间的一维滤波器进行卷积。为了生成更高层次的特征，作者为网络插入了三个具有一维滤波的卷积层。第一层有60个过滤器，内核大小为1×4，后面是池大小为1×2的最大池化层。maxpooling的输出通过第二个卷积层，其中包含40个过滤器，内核大小为1×3。最后，在使用20个过滤器和内核大小1×2的第三个卷积层后，生成的特征映射被扁平成一个向量。这个向量以20%的概率通过dropout层，然后被馈送到第一个大小为100的全连接层。接着作者在第二个全连接层(Softmax)之前以30%的概率插入第二个dropout层，以克服过拟合。最后，将特征输入Softmax层进行分类。在每个卷积层和第一个全连通层之后都采用了型整流线性单元(ReLU)的激活函数。优化算法采用ADAM方法[5]。用于认知BCI的基于CNN的端到端深度分类框架的示意图如图4所示。</p><img src="/ea321eae/4.webp"><div align="center">图4 基于端到端CNN的迁移学习分类框架示意图</div><hr><h3 id="研究结果"><a href="#研究结果" class="headerlink" title="研究结果"></a>研究结果</h3><h4 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h4><p>为了为所提出的技术提供一个公平的基线，作者实现了[6]中引入的分类框架用于单通道数据在注意和非注意之间进行分类。为了与提出的数据表示一致，作者使用数据DR3中描述的相同频带执行传统的特征提取和分类方法。第一个基线：根据[6]中的方法，利用快速furrier变换(FFT)提取delta (0.5-3Hz)、theta (4-7Hz)、alpha (8-13Hz)、beta (14-30Hz)和alpha - beta比值等频段能量，送入具有多项式核函数的支持向量机(SVM)进行分类。第二个基线：作者使用Chebyshev II型带通过滤了随后5个频带的数据，包括δ，θ，α，β和低γ。然后计算出波段功率(平方值的平均值)，并送入LDA进行分类。其中在两种基线方法中都执行了主题间分类方法(删除一个主题)，以提供与深度CNN结果的公平比较。结果显示基线1的平均准确率仅为50.70%。</p><p>为了提高精度，作者对基线1的特征进行了归一化处理。平均准确率提高到了67.90%。如表1所示，基线1和基线2的平均准确率分别为67.90%和68.23%，两者之间无统计学差异，超过50%的受试者准确率低于70%。</p><div align="center">表1 基线和端到端深度CNN方法的平均精度</div><img src="/ea321eae/5.webp"><hr><h4 id="留一交叉验证结果（LOO）"><a href="#留一交叉验证结果（LOO）" class="headerlink" title="留一交叉验证结果（LOO）"></a>留一交叉验证结果（LOO）</h4><p>LOO方法：使用来自主体(源)池的数据学习一个广义网络，然后将所学到的知识转移到新的主体(目标),可看作是一种主题间迁移学习。</p><p>在本研究中，作者使用除目标受试者外的所有其他受试者的数据对网络进行训练，并用目标受试者数据来测试模型精度。该方法的精度明显优于基线，平均提高7.92%。如表1所示，DR1、DR2、DR3的平均准确率分别为76.20%、75.07%、76.68%，差异无统计学意义。该方法的准确率低于70%的受试者比例也显著下降，DR1、DR2和DR3的准确率在120个受试者中分别为26.67%、24.17%和23.34%。</p><hr><h4 id="自适应迁移学习"><a href="#自适应迁移学习" class="headerlink" title="自适应迁移学习"></a>自适应迁移学习</h4><p>虽然零次学习方法避免了对新被试数据的长时间训练，但这种方法在将知识从源转移到目标时可能会遇到信息变化&#x2F;移位的问题。为了解决这一问题，作者采用了自适应方法，对新受试者数据的小样本量进行再训练。此方法既解决了对新数据进行再培训时间过长问题，又解决了信息转移问题。在研究中作者使用了一半的新受试者样本进行适应(2倍)。实验结果如表1所示，该策略对DR1、DR2和DR3的平均准确率分别为79.26%、78.12%和79.86%，超过了基线和LOO方法。与基线相比平均增加11.02%，与LOO相比平均增加3.10%。在DR1、DR2和DR3的120名受试者中，表现不佳的受试者数量分别下降到15.83%、17.50%和15.83%。</p><p>基线、LOO和自适应迁移学习三种不同方法的性能直观比较如图5所示：</p><img src="/ea321eae/6.webp"><div align="center">图5 比较基线和端到端深度CNN方法在注意检测中的性能</div><p>从图5中可以看出，采用自适应迁移学习的CNN取得了最佳性能。</p><hr><h4 id="多通道公共数据集上的结果"><a href="#多通道公共数据集上的结果" class="headerlink" title="多通道公共数据集上的结果"></a>多通道公共数据集上的结果</h4><p>为了研究该框架的可泛化性，作者将该网络应用于一个多通道数据集。该数据集为研究受试者注意力转移问题，招募了8名健康受试者(18-27岁)参加实验，采用64通道帽，电极按国际10-10系统放置，并记录被试者的脑电图。在记录过程中采样频率被设置为1000Hz，后来它被下采样到200Hz。实验包括注意、反应和休息的顺序。作者对注意力和休息阶段的脑电图进行了分割，以完成分类任务。基于对数据集的原始研究，包括PO3、4、7-10、Oz、O1和O2在内的9个电极是研究注意力的最佳电极。在此研究中也使用了这9种推荐的电极。</p><p>作者使用的第一个基线：基于流行的滤波器组公共空间模式(FBCSP)[8]方法，采用了与[8]相同的基于互信息的最佳个体特征(MIBIF)和Naïve贝叶斯帕森窗口 (NBPW)方法分别进行特征选择和分类。它提供了与端到端框架公平比较的结果，作者还使用10倍交叉验证进行了被试内分类。</p><p>第二个基线是[7]中介绍的浅层CNN方法。它有两个隐藏层，分别执行时间卷积和空间滤波，用于波段功率特征解码。此方法通过单个网络[7]联合优化了所有的计算步骤。实验结果如图表2所示。</p><div align="center">表2 多通道数据集上的结果</div><img src="/ea321eae/7.webp"><p>从表2中我们可以看出：基浅层卷积网络优于FBCSP方法，端到端深度CNN优于两种基线方法。</p><hr><h3 id="总结讨论"><a href="#总结讨论" class="headerlink" title="总结讨论"></a>总结讨论</h3><p>为了增强提出的端到端CNN模型学习到的内容的可解释性，作者使用激活最大化技术来可视化来自网络的感知输入。每个类对网络的感知如图6所示。图6(a)为第一类(注意)的网络感知，图6(b)为第二类(不注意)的网络感知。从图中我们看出注意类表现为高频振荡，而在非注意模式中这些分量消失。(a)和(b)中信号的功率谱密度，在包括alpha、beta1、beta2、高beta和低gamma在内的多个频带上，分别显示在(c)和(d)中。可以看到，随着精神状态从不专心(class2)到专心(class1)的变化:<br>（1）Beta活动增加。<br>（2）Beta2增加更多。<br>（3）Theta活动降低。<br>（4）被称为注意力指标的Theta&#x2F;beta比值（TBR）下降。这可以从（1）和（3）推论得出。</p><img src="/ea321eae/8.webp"><div align="center">图6 可视化结果</div><p>从上图可以看出，观察结果与注意诱导频率振荡[9][10]的研究结果一致。这些结果表明，端到端CNN能够直接从原始脑电图中学习有意义的信息，能够自动检测到注意检测中的重要频带。</p><p>本研究提出的一个端到端的深度CNN框架，将脑电图分类为注意&#x2F;非注意精神状态，解决了以下三个问题，分别是: 1）由于特征提取导致的信息丢失从而使分类精度下降 ; 2）跨被试迁移学习 ; 3）CNN学习内容的可解释性。</p><p>这项研究表明，通过CNN进行深度学习是一种有前途的脑电分类技术，其性能优于LDA，SVM和FBCSP等其他技术。实验结果表明，通过使用Deep CNN，可以从原始的EEG中学习，并成功地将所学的知识迁移到新的目标被试中。该研究的成果可以应用于基于注意力的BCI系统，并可以扩展到其他类型的基于EEG的BCI系统。</p><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]. Fahimi F, Zhang Z, Lee T S, et al. Deep convolutional neural network for the detection of attentive mental state in elderly[C]&#x2F;&#x2F;7th Int. BCI Meeting, Sacramento, CA, USA. 2018.</p><p>[2]. MacLeod C M 1991 Half a century of research on the Stroop effect: an integrative review Psychological bulletin 109 163-203.</p><p>[3]. MacLeod C M and MacDonald P A 2000 Interdimensional interference in the Stroop effect: uncovering the cognitive and neural anatomy of attention Trends in cognitive sciences 4 383-91.</p><p>[4]. Srivastava N, Hinton G, Krizhevsky A, Sutskever I and Salakhutdinov R 2014 Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 15 1929-58.</p><p>[5]. Diederik P. Kingma and Ba J 2015 Adam: A Method for Stochastic Optimization. In: 3rd International Conference for Learning Representations, (San Diego, USA.</p><p>[6]. Liu N-H, Chiang C-Y and Chu H-C 2013 Recognizing the Degree of Human Attention Using EEG Signals from Mobile Sensors Sensors (Basel, Switzerland) 13 10273-86.</p><p>[7]. Schirrmeister R T, Springenberg J T, Fiederer L D J, Glasstetter M, Eggensperger K, Tangermann M, Hutter F, Burgard W and Ball T 2017 Deep learning with convolutional neural networks for EEG decoding and visualization Human brain mapping 385391-420.</p><p>[8]. Ang K K, Chin Z Y, Wang C, Guan C and Zhang H 2012 Filter Bank Common Spatial Pattern Algorithm on BCI Competition IV Datasets 2a and 2b Frontiers in Neuroscience 6.</p><p>[9]. Kamiński J, Brzezicka A, Gola M and Wróbel A 2012 Beta band oscillations engagement in human Alertness process International Journal of Psychophysiology 85 125-8.</p><p>[10]. Martijn A, Conners C K and Helena C K 2012 A Decade of EEG Theta&#x2F;Beta Ratio Research in ADHD: A Meta-Analysis Journal of Attention Disorders 17 374-83.</p><hr><h3 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=12426">https://www.scholat.com/teamwork/showPostMessage.html?id=12426</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/ebc4aeea.html" rel="prev" title="人脸识别-DeepID3 face recognition"><i class="fa fa-chevron-left"></i> 人脸识别-DeepID3 face recognition</a></div><div class="post-nav-item"><a href="/727e6209.html" rel="next" title="算法设计与分析-第5章">算法设计与分析-第5章 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E5%86%85%E5%AE%B9"><span class="nav-number">1.</span> <span class="nav-text">研究背景与内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">研究数据与方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.</span> <span class="nav-text">研究数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">研究方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0"><span class="nav-number">3.2.</span> <span class="nav-text">数据输入神经网络特征构造</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">3.3.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E7%BB%93%E6%9E%9C"><span class="nav-number">4.</span> <span class="nav-text">研究结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%BA%BF"><span class="nav-number">4.1.</span> <span class="nav-text">基线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%95%99%E4%B8%80%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E7%BB%93%E6%9E%9C%EF%BC%88LOO%EF%BC%89"><span class="nav-number">4.2.</span> <span class="nav-text">留一交叉验证结果（LOO）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.3.</span> <span class="nav-text">自适应迁移学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E9%80%9A%E9%81%93%E5%85%AC%E5%85%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="nav-number">4.4.</span> <span class="nav-text">多通道公共数据集上的结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E8%AE%A8%E8%AE%BA"><span class="nav-number">5.</span> <span class="nav-text">总结讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87%E9%93%BE%E6%8E%A5"><span class="nav-number">7.</span> <span class="nav-text">原文链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1009</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">146:54</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>