<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文将分 3 期进行连载，共介绍 15 个在图像分类任务上曾取得 SOTA 的经典模型。  第1期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt 第2期：DenseNet、MobileNet、SENet、DPN、IGC V1 第3期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFNet  （您"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-图像分类必备经典模型，一文打尽知多少？（一）"><meta property="og:url" content="https://aisakaaoi.github.io/39e6514a.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本文将分 3 期进行连载，共介绍 15 个在图像分类任务上曾取得 SOTA 的经典模型。  第1期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt 第2期：DenseNet、MobileNet、SENet、DPN、IGC V1 第3期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFNet  （您"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/20.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/23.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/24.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/25.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/26.webp"><meta property="og:image" content="https://aisakaaoi.github.io/39e6514a/27.webp"><meta property="article:published_time" content="2022-11-30T18:17:50.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:47.583Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/39e6514a/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/39e6514a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度学习-图像分类必备经典模型，一文打尽知多少？（一） | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1030</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/39e6514a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习-图像分类必备经典模型，一文打尽知多少？（一）</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-12-01 02:17:50" itemprop="dateCreated datePublished" datetime="2022-12-01T02:17:50+08:00">2022-12-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span></span><span id="/39e6514a.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-图像分类必备经典模型，一文打尽知多少？（一）" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/39e6514a.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/39e6514a.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>12k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>30 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文将分 3 期进行连载，共介绍 <strong>15 个</strong>在<strong>图像分类</strong>任务上曾取得 SOTA 的经典模型。</p><ul><li>第1期：AlexNet、VGG、GoogleNet、ResNet、ResNetXt</li><li>第2期：DenseNet、MobileNet、SENet、DPN、IGC V1</li><li>第3期：Residual Attention Network、ShuffleNet、MnasNet、EfficientNet、NFNet</li></ul><p>（您正在阅读的是其中的第 1 期。前往 SOTA！模型资源站（sota.jiqizhixin.com）即可获取本文中包含的模型实现代码、预训练模型及 API 等资源。）</p><span id="more"></span><hr><p>本期收录模型速览</p><table><thead><tr><th align="left">模型</th><th align="left">SOTA！模型资源站收录情况</th><th align="left">模型来源论文</th></tr></thead><tbody><tr><td align="left">AlexNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/d07f8019-2816-4ce4-af44-ff9c1310e6ee">https://sota.jiqizhixin.com/models/models/d07f8019-2816-4ce4-af44-ff9c1310e6ee</a></td><td align="left">Imagenet Classification with Deep Convolution Neural Network</td></tr><tr><td align="left">VGG</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/9d4c5378-8f0d-43aa-b64f-976a3c87273b">https://sota.jiqizhixin.com/models/models/9d4c5378-8f0d-43aa-b64f-976a3c87273b</a></td><td align="left">Very Deep Convolutional Networks for Large-Scale Image Recognition</td></tr><tr><td align="left">GoogleNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/5b5254d4-c27d-4014-85b0-c1cb48488995">https://sota.jiqizhixin.com/models/models/5b5254d4-c27d-4014-85b0-c1cb48488995</a></td><td align="left">Going Deeper with Convolutions</td></tr><tr><td align="left">ResNet</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/51e81484-f784-4b1d-9b4a-723d9c18a76f">https://sota.jiqizhixin.com/models/models/51e81484-f784-4b1d-9b4a-723d9c18a76f</a></td><td align="left">Deep Residual Learning for Image Recognition</td></tr><tr><td align="left">ResNeXt</td><td align="left"><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/0e3dac75-f35b-4813-9355-c4c9f5cb0f78">https://sota.jiqizhixin.com/models/models/0e3dac75-f35b-4813-9355-c4c9f5cb0f78</a></td><td align="left">Aggregated Residual Transformations for Deep Neural Networks</td></tr></tbody></table><p>图像分类是计算机视觉领域最经典的任务之一，目的是将输入的图像对应到预定义的语义类别中，即打上类别标签。传统的图像分类方法由底层特征学习、特征编码、空间约束、分类器设计、模型融合等步骤组成。</p><p>首先，从图像中提取特征，经典的特征提取方法包括HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)、SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换)等，也可以将多种特征融合以保留更多有用信息。然后，对特征进行编码后去除冗余和噪声，生成特征编码，经典方法包括稀疏编码、局部线性约束编码、Fisher向量编码等。再然后，经过空间特征约束后实现特征汇聚，例如经典的金字塔特征匹配方法。最后，利用分类器进行分类，经典分类器包括SVM、随机森林等等。</p><img src="/39e6514a/1.webp"><p>Alex Krizhevsky 在 2012 年 ILSVRC 提出的 CNN 模型首次将深度学习应用于大规模图像分类任务，其效果远超传统的图像分类方法，一举获得 ILSVRC 2012 的冠军，开启了深度学习模型在图像分类中的应用历程。这个模型就是著名的 AlexNet。自此，图像分类改变了传统的致力于提取有效的特征、改进分类器的有效性的研究和应用路径，转为研究不同的深度学习模型架构。</p><p>深度卷积神经网络为图像分类带来了一系列突破。深度神经网络集成了低&#x2F;中&#x2F;高层次的特征，可以被端到端训练，特征的层次可以通过网络的深度来丰富。甚至有研究人员称，正是在图像分类任务中应用的巨大成功，带动了深度学习的方法。</p><hr><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet 首次发表在 NIPS 2012，获得了 ILSVRC 2012 的冠军，开启了深度学习模型在图像分类中的应用历程。<strong>AlexNet 实际上就是一个 CNN</strong>，如下图，包含 8 个学习层：5 个卷积层和 3 个全连接层。在现在看来，AlexNet 的架构并不复杂。</p><img src="/39e6514a/2.webp"><div align="center">图 1 AlexNet 架构</div><p>AlexNet 首次在 CNN 中成功应用了 ReLU、Dropout 和 LRN 等。具体如下：<br>（1）AlexNet 中使用 Relu 作为 CNN 的激活函数，成功解决了 Sigmoid 在网络较深时的梯度弥散问题：</p><img src="/39e6514a/3.webp"><p>（2）AlexNet 在训练时使用 Dropout 随机忽略一部分神经元，以避免模型过拟合。在AlexNet 中主要是最后几个全连接层使用了 Dropout。<br>（3）AlexNet 中引入了局部响应归一化（Local Response Normalization，LRN）。对于第 i 个特征图 (x,y) 上的神经元活性 (a_x,y)^i ，应用 ReLU 非线性，响应归一化的活动是</p><img src="/39e6514a/4.webp"><p>通过向局部神经元的活动引入竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。<br>（4）AlexNet 使用重叠的最大池化。此前 CNN 中普遍使用平均池化，AlexNet 全部使用最大池化，避免平均池化的模糊化效果。并且 AlexNet 中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。<br>（5）使用 CUDA 加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。受限于当时的显卡显存条件，AlexNet 使用两卡并行训练，在最终的卷积层中，单 GPU 网络实际上可以具有与双 GPU 网络相同数量的内核。<br>（6）优化器是 SGD + Momentum。</p><p>当前 SOTA！平台收录 AlexNet 共 10 个模型实现资源，支持的主流框架包含 PyTorch、CANN、MindSpore、caffe、TensorFlow、PaddlePaddle 等。</p><img src="/39e6514a/5.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">AlexNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/d07f8019-2816-4ce4-af44-ff9c1310e6ee">https://sota.jiqizhixin.com/models/models/d07f8019-2816-4ce4-af44-ff9c1310e6ee</a></td></tr></tbody></table><hr><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>VGG 是指用于大规模图像识别的极深度卷积网络（VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION），重点研究的是深度对CNN 性能的影响，提出用堆叠的小卷积核来代替大卷积核，论文发表在 ICLR 2015。VGG 的核心思想是<strong>使用具有非常小的卷积核的架构，增加网络的深度，然后进行全面评估</strong>。实验表明将深度推至 16-19 层可以实现对现有技术的提升。具体的，VGG16、VGG19 中的数字都是指有参数层的数量。</p><img src="/39e6514a/6.webp"><div align="center">表 1 ConvNet 的配置（以列显示）</div><p>如表 1 所示，ConvNet 的配置的深度从左（A）到右（E）增加，卷积层的参数表示为 “conv<receptive field size>-<number of channels>“。其中，忽视了 Relu 层。</number></receptive></p><p>在训练阶段，ConvNets 的输入是一个固定大小的 224×224 的 RGB 图像。我们所做的唯一预处理是减去每个像素的 RGB 平均值，该值是在训练集上计算出来的。将图像传递到卷积层的堆栈中，使用具有非常小的感受野（receptive field，RF）的过滤器：3×3。在其中一个配置中，还利用了 1x1 卷积核，这可以看作是输入通道的线性变换。卷积 stride 固定为 1 ，padding 是为了卷积前后分辨率不变。池化由 5 个 max-pooling 执行，大小为 2，stride 为 2。引入 1×1 卷积层（配置C，表1）是增加决策函数的非线性而不影响卷积层的感受野的一种方法。</p><p>卷积层的堆栈（在不同的架构中具有不同的深度）之后是三个全连接（FC）层：前两个层各有 4096 个通道，第三层进行 1000 维度的 ILSVRC 分类，因此包含 1000 个通道（每个对应一个类别）。最后一层是 softmax 层。在所有网络中，全连接层设置都是相同的。</p><p>VGG 使用 ReLu 作为激活函数，网络（除了一个）均不包含 LRN。文中实验证明这种归一化不能提高 ILSVRC 数据集的性能，还会导致内存消耗和计算时间增加。此外，VGG 使用了 Dropout 来优化网络结构。VGG 的优点是结构简单，易于推广应用。缺点则是参数量和计算量都很大。</p><p>当前 SOTA！平台收录 VGG 共 16 个模型实现资源，支持主流框架包括 CANN、PyTorch、TensorFlow、MindSpore、caffe、PaddlePaddle 等。</p><img src="/39e6514a/7.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">VGG</td><td align="left">前往 SOTA！模型资源站获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/9d4c5378-8f0d-43aa-b64f-976a3c87273b">https://sota.jiqizhixin.com/models/models/9d4c5378-8f0d-43aa-b64f-976a3c87273b</a></td></tr></tbody></table><hr><h3 id="GoogLeNet（Inception-系列）"><a href="#GoogLeNet（Inception-系列）" class="headerlink" title="GoogLeNet（Inception 系列）"></a>GoogLeNet（Inception 系列）</h3><p>Inception系列主要包含 Inception V1、Inception V2、Inception V3、Inception V4、Inception-Resnet。Inception 系列模型提出的初衷主要为了解决 CNN 分类模型的两个问题，一是如何使得网络深度增加的同时能使得模型的分类性能随着增加，而非像简单的 VGG 网络那样达到一定深度后就陷入了性能饱和的困境；二是如何在保证分类网络分类准确率提升或保持不降的同时使得模型的计算开销与内存开销充分地降低。</p><p>GoogLeNet 是 Inception v1，相关论文发表在 CVPR 2015。改善深度神经网络性能的最直接方法是增加其大小。这既包括增加深度（网络级别的数量），也包括增加其宽度：每个级别的单位数量。但是这些方法带来了过拟合和高资源消耗的问题。解决这两个问题的基本方法是引入稀疏性，用稀疏的层替换全连接的层。GoogLeNet 与 AlexNet&#x2F;VGGNet 这类依靠加深网络结构的深度的思想不完全一样。GoogLeNet <strong>在加深度的同时做了结构上的创新，引入了一个叫做 Inception 的结构来代替之前的卷积加激活的经典组件。</strong></p><p>Inception 架构的主要思想是基于发现卷积视觉网络中的最佳局部稀疏结构如何能够被现成的密集组件所近似和覆盖。我们要做的是通过不同的转换<strong>找到最佳的局部结构，并在空间上重复它</strong>。假设来自较早层的每个单元对应于输入图像的某些区域，并且这些单元被分组为滤镜组。在较低的层（靠近输入层），相关单元将集中在局部区域。因此，我们最终会得到很多聚类，它们集中在一个区域中，并且可以在下一层中被 1×1 卷积层覆盖。为了避免分块对齐问题，Inception 体系结构的当前化身被限制为 1×1、3×3 和5×5 的滤波器大小，这也意味着建议的体系结构是所有这些层的组合，它们的输出滤波器组被串联到单个输出矢量中，形成下一级的输入。另外，由于池化操作对于当前卷积网络的成功至关重要（加padding，步长为1），因此建议在每个这样的阶段添加替代并行池化路径也应具有额外的有益效果，如图 2a。</p><img src="/39e6514a/8.webp"><div align="center">图 2 Inception 模块</div><p>由于这些 “初始模块” 是相互堆叠的，它们的输出相关统计数据必然会有所不同：随着更高的抽象特征被更高的层所捕获，它们的空间集中度预计会降低，这表明随着我们向更高的层移动，3×3 和 5×5 卷积的比例应该增加。上述模块的一个大问题是，即使是数量不多的 5×5 的卷积，在具有大量过滤器的卷积层之上也会变得过于昂贵。一旦池化单元被添加到组合中，这个问题就会变得更加明显：它们的输出过滤器的数量等于前一阶段的过滤器的数量。合并层的输出与卷积层的输出的合并将导致不可避免地逐级增加输出数量。</p><p>尽管此体系结构可能涵盖了最佳的稀疏结构，但这样做的效率非常低，导致在几个阶段内出现了计算爆炸。这就引发了第二个想法：在计算需求大大增加的情况下，应用降维和投影以减小尺寸。这种思想是基于嵌入的成功：即使是低维的嵌入也可能包含许多有关较大图像分块的信息。但是，嵌入以密集、压缩的形式表示信息，并且压缩的信息更难处理。这种表示应在大多数地方保持稀疏，并且仅在必须将其汇总时才压缩信号。也就是说，在昂贵的 3×3 和 5×5 卷积之前，使用 1×1 卷积来计算减少量。除了用作减少量之外，它们还包括使用 Relu 激活使其具有双重用途，如图 2b。</p><p>“GoogLeNet” 特指在参加 2014 年 ILSVRC 竞赛时所使用的 Inception 架构，如表 2 所示。</p><img src="/39e6514a/9.webp"><div align="center">表 2 Inception 架构的 GoogLeNet 化身</div><p>所有卷积，包括 Inception 模块内部的那些卷积，都使用 Relu。在 RGB 色彩空间中，网络中输入大小为 224×224，均值为零。#3×3 reduce 和 #5×5 reduce 表示在 3×3 和 5×5 卷积之前使用的缩小层中 1×1 滤波器的数量。在 pool proj 列中内置最大池化之后，可以看到投影层中 1×1 滤波器的数量。所有这些缩小&#x2F;投影层也都使用 Relu。如果只计算有参数的层，该网络有 22 层深（如果我们也计算池化，则有 27 层）。用于构建网络的总层数（独立构件块）约为 100，当然这个数字取决于所使用的机器学习基础系统。</p><p>鉴于 Inception 的网络的深度相对较大，如何以有效方式将梯度传播回所有层是一个问题。浅层网络在此任务上的强大性能表明，网络中间各层所产生的特征应当非常有区别。通过添加连接到这些中间层的辅助分类器（采用较小的卷积网络的分类器），可以在分类器的较低阶段进行区分：这是在提供正则化的同时解决了梯度消失问题。这些辅助分类器位于 Inception(4a) 和 (4d) 模块的输出之上。在训练阶段，以权重方式将损失添加到网络的总损失中（辅助分类器的损失加权为 0.3）。在推理阶段，则丢弃这些辅助网络。</p><p>侧面的额外网络（包括辅助分类器）的确切结构如下：</p><ol><li>平均池化层的卷积核大小为5×5，stride 为 3，4a 的输出为 4×4×512，4d 的输出为 4×4×528；</li><li>具有 128 个滤波器的 1×1 卷积，用于减小尺寸和 Relu；</li><li>具有1024个单位的全连接层，并具有Relu；</li><li>dropout&#x3D;0.7；5. 一个具有softmax损失的线性层作为分类器（预测与主分类器相同的1000个分类，在推理时将其删除）。</li></ol><p>2015 年提出的 Inception V2、V3 版本主要目的是<strong>减少特征的表征性瓶颈</strong>，即过度减少输入维度可能会造成信息损失。所以尝试提高特征内部的相关性和子特征之间的独立性，加速网络收敛。此外，压缩特征维度数以减少计算量，对于整个网络结构，尽量保持深度和宽度平衡。</p><p>Inception V2（《Rethinking the Inception Architecture for Computer Vision》<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.00567.pdf%EF%BC%89%E4%B8%BB%E8%A6%81%E5%81%9A%E4%BA%86%E8%BF%99%E4%BA%9B%E5%B7%A5%E4%BD%9C%EF%BC%9A%E5%B0%86%E5%A4%A7%E5%B0%BA%E5%BA%A6%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%88%86%E8%A7%A3%E6%88%90%E5%A4%9A%E4%B8%AA%E5%B0%8F%E5%B0%BA%E5%BA%A6%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%BC%95%E5%85%A5%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8D%B7%E7%A7%AF%E6%9D%A5%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97%E9%87%8F%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%B9%B6%E8%A1%8C%E7%BB%93%E6%9E%84%E4%BC%98%E5%8C%96">https://arxiv.org/pdf/1512.00567.pdf）主要做了这些工作：将大尺度的卷积分解成多个小尺度的卷积和引入非对称卷积来减少计算量，使用并行结构优化</a> Pooling，以及使用 Label Smoothing 来对网络输出进行正则化处理。Inception V2 的结构如下表：</p><img src="/39e6514a/10.webp"><div align="center">表 3 Inception 网络结构</div><img src="/39e6514a/11.webp"><div align="center">图 3 Inception V2 结构</div><p>Inception V2 结构如图 3 所示，左：第一级结构（对应于表 2 中的 figure 5），中：第二级结构（对应于表 2 中的 figure6），右：第三级结构（对应于表 2 中的 figure 7），每个模块的输出尺寸是下一个模块的输入尺寸。</p><p>Inception-v3 架构的主要思想是 factorized convolutions (分解卷积) 和 aggressive regularization (激进的正则化)（《Rethinking the Inception Architecture for Computer Vision》 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.00567.pdf%EF%BC%89%E3%80%82%E4%B8%80%E8%88%AC%E8%AE%A4%E4%B8%BA">https://arxiv.org/pdf/1512.00567.pdf）。一般认为</a> Inception-v2 (BN 技术的使用) 和 Inception-v3 (分解卷积技术) 网络是一样的，只是配置上不同。</p><p>在《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》这篇文章中，作者提出了 Inception v4 和 Inception-ResNet v1、Inception-ResNet v2。Inception-v4 网络<strong>对于 Inception 块的每个网格大小进行了统一。</strong></p><p>图 4 给出 Inception-v4 的结构：所有图中没有标记 “V” 的卷积使用 same 的填充原则，即其输出网格与输入的尺寸正好匹配。使用 “V” 标记的卷积使用 valid 的填充原则，意即每个单元输入块全部包含在前几层中，同时输出激活图（output activation map）的网格尺寸也相应会减少。</p><img src="/39e6514a/12.webp"><div align="center">图 4 Inception-v4 网络的整体架构</div><p>其中 stem 的机构如图 5，作用是对进入 Inception 模块前的数据进行预处理。stem 部分其实就是多次卷积加上 ２ 次 pooling 的处理，pooling 采用了 Inception-v3 论文里提到的卷积＋pooling 并行的结构，来防止 bottleneck 问题。stem 后用了 ３ 种共 14 个 Inception 模块。</p><img src="/39e6514a/13.webp"><div align="center">图 5 Inception-v4 和 Inception-ResNet-v2 网络架构</div><p>作者进一步把 Inception 和 ResNet 混合，设计了多种 Inception-ResNet 结构，在这篇论文中作者重点描述了 **Inception-ResNet-v1 (在 Inception-v3 上加入 ResNet)和 Inception-ResNet-v2 (在 Inception-v4 上加入 ResNet)**。残差连接是指浅层特征通过另外一条分支加到高层特征中，达到特征复用的目的，同时也避免深层网路的梯度弥散问题。Inception-ResNet 结构如图 6 所示。</p><img src="/39e6514a/14.webp"><div align="center">图 6 Inception-ResNet-v1 和 InceptionResNet-v2 网络模式</div><p>如图 6 所示，该模式适用于这两个网络，但其基本组成部分有所不同。</p><p>当前 SOTA！平台收录 GoogleNet 共 10 个模型实现资源，支持主流框架包括 CANN、TensorFlow、PyTorch、MindSpore、PaddlePaddle 等。</p><img src="/39e6514a/15.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">GoogLeNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/5b5254d4-c27d-4014-85b0-c1cb48488995">https://sota.jiqizhixin.com/models/models/5b5254d4-c27d-4014-85b0-c1cb48488995</a></td></tr></tbody></table><hr><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet 是 CVPR 2016 的 best paper。ResNet 是一个学习残差的框架，以简化比以前使用的网络更深的网络的训练。通过归一化初始化层和中间归一化层很大程度上解决了极深度网络梯度爆炸、收敛性问题，但却导致了退化问题：随着网络深度的增加，准确度会达到饱和，急速退化。ResNet 的思路是：<strong>通过引入深度残差学习框架来解决退化问题，令每个堆叠的层拟合一个残差映射，而不是让这些层去拟合隐含的映射。</strong></p><p>将要拟合的隐含关系的映射看做 H(x)，从而让堆叠的层去学习其残差 F(x)&#x3D;H(x)+x。那么最开始要学习的隐含映射就可以重写为 F(x)+x。假设优化残差映射比优化原先未知的映射要容易，极端情况下，如果一个恒等映射是最佳的，那么就可以推出拟合残差要比拟合一个堆叠层的恒等变化要来得容易。在前馈神经网络中可以通过跳接（shortcut connections）来实现 F(x)+x：</p><img src="/39e6514a/16.webp"><div align="center">图 7 残差学习的一个构件</div><p>将图7所示确定为一个 residual block（残差块），定义为：</p><img src="/39e6514a/17.webp"><p>其中，x、y 分别为这个 blok 的输入和输出。F(x,W_i) 表征学习的残差映射。图 3 中为一个 2 层 residual block，即 F&#x3D;W_2 σ (W_1 x)，σ 为 Relu，忽视 bias。F+x 是一个跳接并且按位相加，然后再进行非线性操作。这种操作既没有引入参数也没有带来额外的计算复杂度。</p><p>当 x 和 F 维度不同时，可以引入一个线性投影 W_s 匹配到相同的维度：</p><img src="/39e6514a/18.webp"><p>F 形式灵活，在实验中，可以有 2-3 层，也可以有更多层，当只有一层就相当于线性层。</p><p>文中给出了两个模型，plain network 和 residual network。plain network 的卷积层为 3x3 大小的卷积核（图 8 中），遵循两个原则：（i）对于相同的 feature map 大小，具有相同数量的过滤器；（ii）如果 feature map 的大小减半，则 channel 的数量将增加一倍，以保持每层的时间复杂度。</p><p>网络以全局平均池化层和带有 softmax 的 1000 路全连接层结束，权重层的总数为 34。plain network 比 VGG 有更少的过滤器和更低的复杂性。34 层的基线网络有 36 亿 FLOPs（乘法加成），这只是 VGG-19（196 亿 FLOPs）的 18%。</p><p>进一步，在 plain network 中引入跳接就得到了 Residual Network。当输入和输出的尺寸相同时，直接相加（图 8 右图中的实线部分）。</p><p>输入维度增加时，考虑 2 种解决策略：（1）仍直接相加，多余的维度全部用 0 填充，这样就不会引入额外的参数；（2）使用 F(x,{w_i})+w_s x，引入 1x1 大小的卷积进行维度变换。Residual Block 是 2 层的。</p><img src="/39e6514a/19.webp"><div align="center">图 8 ImageNet 的网络结构实例</div><p>ImageNet 的网络结构实例如图 8 所示，左图：VGG-19 模型（196 亿 FLOPs ）作为参考。中间：一个有 34 个参数层的 plain network（36 亿 FLOPs）。右图：一个有 34 个参数层的 residual network（36 亿 FLOPs）。</p><p>普通网络与深度残差网络的最大区别在于：<strong>深度残差网络有很多旁路的支线将输入直接连到后面的层，使得后面的层可以直接学习残差，这些支路就叫做 shortcut。传统的卷积层或全连接层在信息传递时，或多或少会存在信息丢失、损耗等问题。</strong>ResNet 通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络则只需要学习输入、输出差别的那一部分，简化学习目标和难度。</p><p>当前 SOTA！平台收录 ResNet 共 47 个模型实现资源，支持主流框架包括 CANN、TensorFlow、MindSpore、PyTorch、PaddlePaddle、JAX 等。</p><img src="/39e6514a/20.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">ResNet</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/51e81484-f784-4b1d-9b4a-723d9c18a76f">https://sota.jiqizhixin.com/models/models/51e81484-f784-4b1d-9b4a-723d9c18a76f</a></td></tr></tbody></table><hr><h3 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h3><p>ResNeXt 发表在 CVPR 2016。VGG 的思想是大量堆叠构建块，ResNet 是堆叠相同的拓扑结构块。简单地说 ResNeXt 可以看做是 VGG、ResNet 和 Inception 的结合体：通过重复多个块（如在 VGG 中的堆叠），每个块聚合了多种转换（如 Inception中找到最佳结构），同时引入跨层连接（ResNet 中的跳接）。ResNeXt 重点研究<strong>基数对CNN 性能的影响，即对普通卷积做分组的分组数量，它是深度和宽度维度之外的一个重要因素。</strong></p><img src="/39e6514a/21.webp"><div align="center">图 9 ResNeXt</div><p>左图：ResNeXt 的一个模块。右图。ResNeXt 的一个模块，cardinality &#x3D; 32，复杂度大致相同。</p><p>遵循 VGG&#x2F;ResNets 采用高度模块化的设计，ResNeXt 网络由一堆残差块组成，这些块具有相同的拓扑。</p><p>拓扑块遵循两个简单规则：（1）如果生成相同大小的空间图，则这些块共享相同的超参数（宽度和过滤器大小） ；（2） 每次当空间特征图降采样2倍时，块的通道数乘以系数2。</p><p>使用这两个规则，只需要设计一个模板模块就可以确定网络中的所有模块。因此，这两个规则极大地缩小了设计空间，使研究人员可以专注于一些关键因素。如表 4 所示结构。</p><img src="/39e6514a/22.webp"><div align="center">表 4 (左)ResNet-50。（右）ResNeXt-50与32×4d的模板。括号内是 residual block 的形状，括号外是一个阶段上的堆积块的数量。"C=32" 表示有 32 组的分组卷积</div><p><strong>重温简单神经元。</strong>人工神经网络中最简单的神经元执行内积（加权和），这是全连接和卷积层所做的基本转换。内积可以被认为是一种聚合转换的形式：</p><img src="/39e6514a/23.webp"><p>其中，x 为 D 通道向量，w_i 表征第 i 个通道的权重，这种操作（通常包括一些输出非线性）被称为 “神经元”。可以将上述操作重组为拆分、转换和聚合的组合。（1）拆分：将向量x切片为低维嵌入向量，它是一维子空间 x_i ；（2）变换：对低维表示进行变换，对其进行了缩放：w_i x_i；（3）聚合：所有嵌入向量中的转换都进行聚合。</p><p><strong>聚合的转变。</strong>鉴于以上对简单神经元的分析，作者考虑用更通用的函数代替基本变换（w_i x_i），该函数本身也可以是网络。本文提出了一种聚合方法：</p><img src="/39e6514a/24.webp"><p>其中，T_i(x) 可以是任意的函数，类似于简单的神经元， T_i 将 x 投射到（可选的低维）嵌入中，然后对其进行转换。C 是要聚合的转换集的大小，称为基数。<strong>基数是一个基本维度，并且比宽度和深度的维度更有效。</strong></p><p>本文考虑了一种设计转换函数的简单方法：所有 T_i 都具有相同的拓扑，使用 VGG 的堆叠策略重复使用相同形状。我们将单个变换设置为 Bottleneck layer 形的体系结构。Bottleneck layer 又称为瓶颈层，使用的是 1x1 的卷积神经网络。使用的网络结构的目的是把高维特征映射到低维空间去。在这种情况下，每个 T_i 中的第一个 1×1 层都会产生低维嵌入。将聚合函数写作残差模式为：</p><img src="/39e6514a/25.webp"><p>图10 给出的一些张量操纵表明，图10（a）中的模块与图 10（b）等效。图10（b）看起来与 Inception-ResNet 块相似，因为它涉及到残差函数的分支和连接。但是与所有Inception或 Inception-ResNet 模块不同，它在多个路径之间共享相同的拓扑。ResNeXt 的模块需要最少的额外精力来设计每个路径。使用分组卷积的符号，模块会变得更加简洁。图10（c）给出了这种重新表示形式。所有低维嵌入（前1×1层）都可以由单个较宽的层替换，例如图10（c）中的1×1、128 d。当分组卷积层将其输入通道分为几组时，它实质上是由分组卷积层完成的。图10（c）中的分组卷积层执行 32 组卷积，其输入和输出通道为 4 维。分组的卷积层将它们连接起来作为该层的输出。</p><img src="/39e6514a/26.webp"><div align="center">图10 ResNeXt的等效构件</div><p>ResNeXt 的等效构件如图 10 所示，(a)：聚合的残差转换。(b)：相当于 (a) 的模块，以早期串联的方式实现；(c)：相当于 (a,b) 的一个块，以分组卷积的方式实现。</p><p>当前 SOTA！平台收录 ResNet 共 55 个模型实现资源，支持主流框架包括 PyTorch、TensorFlow、CANN、MindSpore、Torch、PaddlePaddle、MXNet、Caffe2 等。</p><img src="/39e6514a/27.webp"><table><thead><tr><th align="left">项目</th><th align="left">SOTA！平台项目详情页</th></tr></thead><tbody><tr><td align="left">ResNeXt</td><td align="left">前往 SOTA！模型平台获取实现资源：<a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/0e3dac75-f35b-4813-9355-c4c9f5cb0f78">https://sota.jiqizhixin.com/models/models/0e3dac75-f35b-4813-9355-c4c9f5cb0f78</a></td></tr></tbody></table><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/jRizjfgjha6ALuJNL8KEog">https://mp.weixin.qq.com/s/jRizjfgjha6ALuJNL8KEog</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/a9e22ac5.html" rel="prev" title="CSIG云上微表情-第34期-IEEE TAC 2022中的智能微表情识别"><i class="fa fa-chevron-left"></i> CSIG云上微表情-第34期-IEEE TAC 2022中的智能微表情识别</a></div><div class="post-nav-item"><a href="/cf26f90d.html" rel="next" title="深度学习-DenseNet、MobileNet、DPN…你都掌握了吗？一文总结图像分类必备经典模型（二）">深度学习-DenseNet、MobileNet、DPN…你都掌握了吗？一文总结图像分类必备经典模型（二） <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG"><span class="nav-number">2.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GoogLeNet%EF%BC%88Inception-%E7%B3%BB%E5%88%97%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">GoogLeNet（Inception 系列）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">4.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNeXt"><span class="nav-number">5.</span> <span class="nav-text">ResNeXt</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1030</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">4.1m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">170:44</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haru02.model.json"},"display":{"position":"right","width":208,"height":520},"mobile":{"show":false},"log":false});</script></body></html>