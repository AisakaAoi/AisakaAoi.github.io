<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="原文题目为A Novel Graph-TCN with a Graph Structured Representation for Micro-Expression Recognition，收录于2020年ACM International Conference on Multimedia (ACM MM)。本文将会对论文中所提出的方法进行讲解，以及分享自己对于这篇论文的思考与感悟。  图1 论文"><meta property="og:type" content="article"><meta property="og:title" content="一种用于微表情识别的基于图结构表示的新型“图-时序卷积网络”"><meta property="og:url" content="https://aisakaaoi.github.io/89d847a0.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="原文题目为A Novel Graph-TCN with a Graph Structured Representation for Micro-Expression Recognition，收录于2020年ACM International Conference on Multimedia (ACM MM)。本文将会对论文中所提出的方法进行讲解，以及分享自己对于这篇论文的思考与感悟。  图1 论文"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/89d847a0/12.webp"><meta property="article:published_time" content="2021-01-22T17:58:44.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:36.204Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/89d847a0/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/89d847a0.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>一种用于微表情识别的基于图结构表示的新型“图-时序卷积网络” | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1014</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/89d847a0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">一种用于微表情识别的基于图结构表示的新型“图-时序卷积网络”</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-01-23 01:58:44" itemprop="dateCreated datePublished" datetime="2021-01-23T01:58:44+08:00">2021-01-23</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/89d847a0.html" class="post-meta-item leancloud_visitors" data-flag-title="一种用于微表情识别的基于图结构表示的新型“图-时序卷积网络”" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/89d847a0.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/89d847a0.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.9k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>原文题目为A Novel Graph-TCN with a Graph Structured Representation for Micro-Expression Recognition，收录于2020年ACM International Conference on Multimedia (ACM MM)。本文将会对论文中所提出的方法进行讲解，以及分享自己对于这篇论文的思考与感悟。</p><img src="/89d847a0/1.webp"><div align="center">图1 论文截图</div><span id="more"></span><hr><h3 id="研究背景与内容"><a href="#研究背景与内容" class="headerlink" title="研究背景与内容"></a>研究背景与内容</h3><p>微表情是一种自发的面部动作，这一组动作仅涉及面部局部区域且区域内的肌肉运动强度极低。微表情识别方法与一般的视频分析方法工作流程类似，需要对视频序列进行预处理、特征提取，并最终完成识别分类工作。然而，微表情面部肌肉运动强度低的特性会影响到微表情识别算法的实现效果。</p><p>为了解决该问题，一般会在预处理工作中加入视频动作放大的方法。视频动作放大方法顾名思义，就是将视频中的动作增强到更明显的效果。当前，最常被使用的动作放大方法有欧拉视频放大、基于相位的放大和全局拉格朗日运动放大等。但是，上述方法都是使用手工设计进行实现的，以致这些方法的适应性很差，例如欧拉视频放大就只是单纯的对像素进行线性修改，这个过程甚至还增大了图像的模糊度。</p><p>其次，另外一种解决方案就是在特征提取工作上进行改进，使特征的关注点尽量集中在面部的关键位置。在该方案里，如果使用的是传统机器学习的方法，则侧重于使用手工特征方法将人脸按照器官划分为多个子区域，但是微表情只涉及到部分器官区域，因此该类方法会包含很多冗余信息。如果使用深度学习方法来对特征进行学习，则主要集中在对微表情的空间与时序特征进行学习，这类方法同样也规避不了图像噪声或冗余信息所带来的影响。</p><p>而近期，Zhong等人[1]将图结构用在了静态人脸情绪识别上，即使用图结构表示不同情绪下的表情特征。图结构将人脸上的相关标志位相互连接起来，不仅能避免很多冗余信息和不相关噪声的影响，而且还能表达出复杂的面部结构信息，与其他方法比具有更强的判别性。</p><p>本文作者借鉴了图结构在表示面部特征上的优势将其应用在微表情识别当中，同时，考虑到文献[1]中使用的图结构依旧需要人工设置中节点信息和边信息的缺陷，作者提出了可以自动学习图结构特征的时序卷积网络（Temporal Convolutional Network，TCN）——基于图结构表示的新型“图-时序卷积网络（Graph-TCN）”。作者提出的方法主要做出以下几点贡献：</p><ol><li>为解决微表情面部肌肉运动强度低带来的影响，<strong>利用迁移学习技术实现视频动作放大来增强低强度的面部运动</strong>，并且解决了基于手工设计的动作放大方法所带来的适应性限制；</li><li><strong>基于统计分析提出了一种基于形状表征融合的微表情人脸图结构</strong>；</li><li><strong>设计了一种新型的Graph-TCN网络来提取微表情局部肌肉运动的特征</strong>，实现了对图结构的自主学习而不需要手工设计。</li></ol><hr><h3 id="方法的整体框架"><a href="#方法的整体框架" class="headerlink" title="方法的整体框架"></a>方法的整体框架</h3><p>文章提出方法的整体设计框架如图2所示。框架的大致过程如下：</p><ol><li>将微表情序列中的起始帧（即视频中微表情开始发生时对应的帧，Onset frame）和峰值帧（即视频中微表情达到最显著时对应的帧，Apex frame）图像作为输入；</li><li>紧接着，通过图2(a)所示的基于迁移学习的视频动作放大方法提取到以形状特征表示的放大图像；</li><li>然后对图像中的人脸区域进行标点，将眉毛和嘴部区域制成图结构（Graph structure）,如图2(b)所示；</li><li>最后将图结构输入到Graph-TCN网络，对图结构中的边和节点特征进行提取，如图2(c)所示，完成识别和分类任务。</li></ol><img src="/89d847a0/2.webp"><div align="center">图2 文章方法的整体框架图</div><hr><h3 id="具体方法的实现"><a href="#具体方法的实现" class="headerlink" title="具体方法的实现"></a>具体方法的实现</h3><h4 id="基于迁移学习的视频动作放大"><a href="#基于迁移学习的视频动作放大" class="headerlink" title="基于迁移学习的视频动作放大"></a>基于迁移学习的视频动作放大</h4><p>前面我们有提到，现在大多使用的动作放大方法都是通过手工设计实现的。为了解决这一限制，文章中引用了Oh等人[2]所提出的<strong>基于迁移学习的视频动作放大方法</strong>。这一方法不再需要人工去设计，而是通过模型自动学习微表情从起始帧到峰值帧的运动规律来进行放大。但是，文章作者并不是直接使用文献[2]中的方法，而是根据微表情的特点对结构进行了部分修改，并且还运用了迁移学习的方法来进一步改进效果，具体步骤如图3所示。</p><img src="/89d847a0/3.webp"><div align="center">图3 基于迁移学习的视频动作放大方法工作原理(虚线部分未使用)</div><p>图3(a)所示是文献[2]中提供的基本结构，包含编码器（Encoder）、解码器（Decoder）和控制器（Manipulator）。该模型的输入是两张图像，文章所选择的图像是微表情的起始帧和峰值帧（选择这两帧的原因是文献[3]中证明<strong>微表情的峰值帧所体现出来的表情特征最为明显，且足以超过使用整个序列进行特征提取的效果）</strong>，具体的工作流程如下：</p><ol><li>首先，编码器会提取起始帧和峰值帧的形状表征和纹理表征，控制器通过计算两张图像在形状表征上的变化，乘上放大因子（Amplification factor）来达到对形状表征进行放大的目的，解码器则是要将起始帧的纹理表征和放大后的形状表征进行解码生成完整放大后的图像。</li></ol><p>但是，在这个步骤里有一个关键的信息——已有相关文献证明了形状表征对表现肌肉运动关系的贡献更大，纹理表征为微表情提供的有用信息非常少，甚至比噪声的影响还小，因此文章在使用到该模型时摒弃了解码器的工作（也就是图3(a)中的虚线部分未使用）。</p><ol start="2"><li><p>紧接着，从控制器输出的放大后的形状表征由32个通道进行表示（图3(b)所示的就是将32个通道可视化后的效果）。</p></li><li><p>最后，对32个通道进行均质化得到放大的形状表征（如图3(c)所示），并把其作为构建人脸图结构的原始特征输入。</p></li></ol><hr><h4 id="构建面部图结构"><a href="#构建面部图结构" class="headerlink" title="构建面部图结构"></a>构建面部图结构</h4><p><strong>要构建一个可以适用于微表情的面部图结构，就要首先找到合适的面部区域。</strong>文章为了找到一个判别性更高、受噪声影响更低的区域，使用统计分析的方法——皮尔逊相关系数（皮尔逊相关系数是为了计算两个事物的相关性，如果相关系数的绝对值越大，则两者相关性越大；如果越接近0，那么两者的相关性越低）。</p><p>在选择特征点区域的实验上，文章首先在起始帧和峰值帧上设定大小为64*64的疑似特征区域，然后将每个区域级联成一个特征向量，计算每个对应区域在起始帧和峰值帧上的皮尔逊相关系数。图4所示的是每类微表情上排名Top13的特征点区域，也就是面部肌肉运动强度最大的13个区域（皮尔逊相关系数越小排名越高，意味着起始帧和峰值帧之间的差异越大，代表该区域的肌肉运动强度越大）。实验统计结果表明，微表情所涉及的面部肌肉主要集中在眉毛和嘴部区域。</p><img src="/89d847a0/4.webp"><div align="center">图4 在五种微表情上运动强度排在Top13的区域</div><p><strong>在确定过微表情的面部肌肉运动位置后，就要构建这些区域之间的面部图结构。</strong>面部图结构中的<strong>每个节点代表一个肌肉群的内部运动关系</strong>，而连接节点之间的<strong>边则代表两个肌肉群之间的运动关系</strong>。对于不同情绪类型的微表情序列，面部肌肉组也具有不同的运动模式，基于此理论基础可以得知使用面部图结构是可以适用于微表情识别的。</p><p>文章在构造面部图结构时，在眉毛和嘴部区域一共选择了28个节点来定位特征，如图5(c)中标记的红点所示，每个节点都是7*7大小的特征矩阵（如图5(a)所示），随后要将节点特征矩阵压缩成长度为49的节点特征向量（如图5(b)所示），以方便之后用于学习和构造节点之间的边特征。</p><img src="/89d847a0/5.webp"><div align="center">图5 面部图结构的构造过程</div><hr><h4 id="Graph-TCN"><a href="#Graph-TCN" class="headerlink" title="Graph-TCN"></a>Graph-TCN</h4><p>Graph-TCN的本质就是将上述所构建的图结构输入到TCN网络当中，通过TCN网络来对图结构中节点和边的特征分别进行学习。</p><p>文章考虑到眉毛和嘴部两个区域是相对独立的两个肌肉群，因此面部图结构也被分为两部分，每个部分都会使用一个双通道的TCN网络来训练各自的图结构特征，如图6所示。</p><img src="/89d847a0/6.webp"><div align="center">图6 Graph-TCN的框架图</div><p>双通道TCN网络的输入是由所有节点特征向量级联而成的向量。紧接着，网络设置了两个通道：通道1中所使用的TCN残差块称为TBN，用来卷积一个节点特征向量内的元素，提取该节点的特征；通道2中所使用到的TCN残差块称为TBE，对来自多个节点序列的元素进行卷积，提取边的特征。</p><p>图6中举的例子是一个拥有4个节点的图结构，那么在该案例中为了提取节点的特征，就应该对应设置4个TBN。而每个节点的长度都是长度为49的一维向量，那么为了探究节点与节点之间的关系，TBE所使用的膨胀系数（膨胀系数的概念是TCN网络中的膨胀卷积结构中的参数）也设为49。</p><p>最后，网络将两个通道的特征叠加在一起，并将眉毛和嘴部这两部分的图特征进行级联后放入两个全连接层、BatchNorm层、Dropout层和ReLU层中进行进一步的特征训练，最终使用一个softmax层来进行分类。</p><hr><h3 id="相关实验"><a href="#相关实验" class="headerlink" title="相关实验"></a>相关实验</h3><h4 id="数据集及预处理"><a href="#数据集及预处理" class="headerlink" title="数据集及预处理"></a>数据集及预处理</h4><p>文献使用了CASMEⅡ和SAMM两个微表情数据集，原因在于这两个数据集中的视频样本帧率都比较高（200fds）并且都标记了微表情的起始帧和峰值帧。但是值得注意的是，CASMEⅡ数据集的受试者都是中国人，因此具有同质性，而SAMM数据集的受试者则包含了多种族人群。文献的相关实验在CASMEⅡ数据集中选择了五个类别，分别是快乐(32个样本)、厌恶(63个样本)、压抑(27个样本)、惊讶(25个样本)和其他(99个样本)，总共246个样本。在SAMM数据集中同样选择了五个类别，分别是愤怒(57个样本)、快乐(26个样本)、轻视(12个样本)、惊讶(15个样本)和其他(26个样本)，共136个样本。</p><p>在预处理工作上，文献在对样本进行人脸对齐后将图像调整为256*256大小，该尺寸满足了基于迁移学习的视频动作放大的输入要求。由于样本存在类别不平衡和数量太小的问题，文献作者对两个数据集进行了数据增强。数据增强的策略是使用固定剪裁的方法，即以图像的四角和中心为中心按照一定比例进行剪裁来达到数据的扩充，并使每个类别的样本量扩充到相同。</p><hr><h4 id="放大因子的相关实验"><a href="#放大因子的相关实验" class="headerlink" title="放大因子的相关实验"></a>放大因子的相关实验</h4><p>在3.1节中介绍的基于迁移学习的视频动作放大网络里有一个关键参数­­——放大因子（Amplification factor），这个参数影响到视频动作放大的效果。为了评估放大因子对视频动作放大网络效果的影响，作者在CASME II数据集上进行了视觉对比实验。</p><p>为了使得更明显地看出动作放大后的效果，实验直接使用了重建后的放大图像（如图7所示），而不再是像图3那样只展示形状表征。图7中每一行代表的是一种微表情（D、H、O、S、R分别表示厌恶、快乐、其他、惊讶和压抑），每一列表示图像在放大因子分别设置为2.5、3、3.5上的放大效果。由图7展示的结果可以很清晰地观察到随着放大因子的增加，微表情的肌肉运动强度也逐渐增大。其中，眉毛和嘴部区域的放大效果更明显，但是当放大因子大于3时面部肌肉运动开始过度变形。因此，实验在之后使用时会选择小于等于3的放大因子。</p><img src="/89d847a0/7.webp"><div align="center">图7 在不同放大因子下的动作放大效果</div><hr><h4 id="Graph-TCN的相关实验"><a href="#Graph-TCN的相关实验" class="headerlink" title="Graph-TCN的相关实验"></a>Graph-TCN的相关实验</h4><p>在评估文献提出的Graph-TCN方法效果的实验上，作者使用到CASME II和SAMM两个数据集，使用了最常用的跨数据集评估方法——leave-one-subject-out (LOSO)协议，同时采用的评估指标有两个，一个是分类的准确率（Accuracy），另一个是F1-Score（该评估指标本质上是对分类的精确率与召回率进行平均的一个结果）。</p><p>表1展示了Graph-TCN网络其它参数的设置情况，在3.3节我们介绍到Graph-TCN网络包含了TBN和TBE两种TCN残差块，而这两种残差块层数的设置会影响到网络的效果，因此表2展示了网络在两种残差块不同设置情况下的对比实验，并选择准确率最佳的设置——TBN层数为4,TBE层数为1的情况下来与其它方法做对比。</p><img src="/89d847a0/8.webp"><div align="center">表1 Graph-TCN网络的参数设置</div><img src="/89d847a0/9.webp"><div align="center">表2 Graph-TCN网络在TBN和TBE层数设置下的实验</div><p>在与其它方法的比较实验中，首先作者与对CASME II和SAMM两个微表情数据被分为5类情况下的一些重要研究进行对比（如表3、表4所示）。在表中，MDMO、Bi+WOOF+Phase、DSSN三种方法是基于光流的改进方法，STLBP-IP是基于LBP改进的方法，HIGO+Mag、ME-Booster、MagGA是使用欧拉运动放大法对低强度的面部动作进行放大，MagGA，DSSN、CNN+LSTM是使用卷积神经网络识别微表情。</p><p>从图表中展示的结果来看，文献提出的方法在CASMEⅡ数据集上的准确率是明显高于其它方法的，比SSSN方法提升了2.79%，在F1-Score指标下效果虽然不是最好，但是也名列前茅。在SAMM数据集上的效果，无论是在准确率上还是F1-Score上都优于其它方法。</p><img src="/89d847a0/10.webp"><div align="center">表3 在CASMEⅡ数据集（五类）下与其它方法的对比实验</div><img src="/89d847a0/11.webp"><div align="center">表4 在SAMM数据集（五类）下与其它方法的对比实验</div><p>同时，对比实验考虑到其他方法将SAMM划分为5类时的情况比较少，所以实验又增加了SAMM在4类情况下的比较实验。实验根据文献[4]中的分类设计将数据集分为四类，仍然使用LOSO协议。比较结果如表5所示，本文献提出的方法在准确率和F1-Score两个指标下都优于其他方法。</p><p>综上所述，文献提出的方法目前在这两个数据库中具有最高的准确率。</p><img src="/89d847a0/12.webp"><div align="center">表5 在SAMM数据集（四类）下与其它方法的对比实验</div><hr><h3 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h3><p>在学习完整篇文章后，最大的感受是作者所提出的方法都是尽可能使用深度学习的方法去取代人工设计所带来的限制和麻烦。在数据预处理部分，作者所使用到的基于迁移学习的视频动作放大方法虽然不是创新，但是其结合了当前流行的迁移学习方法实现了该方法效果的增强。同时，作者在运用TCN网络的部分也充分考虑了所选取面部位置的特点，实验设计完整，作者对其所提出的方法总结出了以下优点：</p><ol><li>提出的网络是基于TCN结构的改进，因此可以借助TCN中的膨胀卷积结构，通过修改膨胀系数来使网络具备灵活的感受野；</li><li>由于TCN残差块可以跨层共享卷积滤波器，训练所需的内存比基于RNN的网络少；</li><li>由于TCN残差块的反向传播路径与序列的时间方向不同，因此不会发生RNN中的爆炸梯度问题。</li></ol><p>但本人在阅读这篇论文时，也产生了一些疑惑及思考：</p><ol><li>数据集的选择问题。作者在构建人脸图结构时所做的统计实验只使用了CASMEⅡ数据集，该数据集只包含中国人的数据，人脸肌肉的运动规律会不会受到不同种族的行为习惯不一样的影响而导致不严谨？是否会影响算法的泛化性？</li><li>人脸区域的选择问题。作者仅选择了眉毛和嘴部区域构建人脸图结构，是否有其他区域对微表情局部肌肉运动的表征也具有较大贡献？</li><li>Graph-TCN网络的输入问题。除了图结构，是否还有其他更鲁棒的结构用于网络的输入？</li></ol><p>以上问题还有待进一步研究。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=9236">https://www.scholat.com/teamwork/showPostMessage.html?id=9236</a><br>[1]Lei Zhong, Changmin Bai, Jianfeng Li, Tong Chen, Shigang Li, and Yiguang Liu. 2019. A Graph-Structured Representation with BRNN for Static-based Facial Expression Recognition. In Proceedings of IEEE International Conference on Automatic Face &amp; Gesture Recognition. 1-5.<br>[2] Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fr’edo Durand, William T. Freeman, and Wojciech Matusik. 2018. Learning-Based Video Motion Magnification. In ECCV. 663–679.<br>[3] Yante Li, Xiaohua Huang, Guoying Zhao. 2018. Can Micro-Expression be Recognized Based on Single Apex Frame? In Proceedings of IEEE International Conference on Image Processing. 3094-3098.<br>[4] Zhaoqiang Xia, Xiaopeng Hong, Xingyu Gao, Xiaoyi Feng, and Guoying Zhao. 2020. Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions. IEEE Transactions on Multimedia. 22, 3 (2020), 626-640.</p></blockquote><p>论文下载地址：<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3394171.3413714">https://dl.acm.org/doi/abs/10.1145/3394171.3413714</a></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/d3e0084b.html" rel="prev" title="设计模式-一句话归纳设计模式"><i class="fa fa-chevron-left"></i> 设计模式-一句话归纳设计模式</a></div><div class="post-nav-item"><a href="/f505fb1f.html" rel="next" title="通过神经网络图稳定的即插即用脑机接口">通过神经网络图稳定的即插即用脑机接口 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E5%86%85%E5%AE%B9"><span class="nav-number">1.</span> <span class="nav-text">研究背景与内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E7%9A%84%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"><span class="nav-number">2.</span> <span class="nav-text">方法的整体框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.</span> <span class="nav-text">具体方法的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E6%94%BE%E5%A4%A7"><span class="nav-number">3.1.</span> <span class="nav-text">基于迁移学习的视频动作放大</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E9%9D%A2%E9%83%A8%E5%9B%BE%E7%BB%93%E6%9E%84"><span class="nav-number">3.2.</span> <span class="nav-text">构建面部图结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-TCN"><span class="nav-number">3.3.</span> <span class="nav-text">Graph-TCN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.</span> <span class="nav-text">相关实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%8A%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">4.1.</span> <span class="nav-text">数据集及预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%BE%E5%A4%A7%E5%9B%A0%E5%AD%90%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.2.</span> <span class="nav-text">放大因子的相关实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-TCN%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.3.</span> <span class="nav-text">Graph-TCN的相关实验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">总结与思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1014</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">3.7m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">153:20</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/haru02.model.json"},"display":{"position":"right","width":208,"height":520},"mobile":{"show":false},"log":false});</script></body></html>