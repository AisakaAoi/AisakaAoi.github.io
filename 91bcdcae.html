<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="åŸæ–‡  ViT è®ºæ–‡é“¾æ¥ï¼šhttps:&#x2F;&#x2F;openreview.net&#x2F;pdf?id&#x3D;YicbFdNTTy"><meta property="og:type" content="article"><meta property="og:title" content="æ·±åº¦å­¦ä¹ ViTè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleã€‹"><meta property="og:url" content="https://aisakaaoi.github.io/91bcdcae.html"><meta property="og:site_name" content="é€¢å‚è‘µçš„ä¸ªäººåšå®¢"><meta property="og:description" content="åŸæ–‡  ViT è®ºæ–‡é“¾æ¥ï¼šhttps:&#x2F;&#x2F;openreview.net&#x2F;pdf?id&#x3D;YicbFdNTTy"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/20.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/91bcdcae/23.webp"><meta property="article:published_time" content="2022-01-29T09:28:10.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:40.121Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/91bcdcae/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/91bcdcae.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>æ·±åº¦å­¦ä¹ ViTè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleã€‹ | é€¢å‚è‘µçš„ä¸ªäººåšå®¢</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="é€¢å‚è‘µçš„ä¸ªäººåšå®¢" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ "><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoiå­¦é™¢</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£<span class="badge">1011</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="æœç´¢..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/91bcdcae.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="é€¢å‚è‘µçš„ä¸ªäººåšå®¢"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">æ·±åº¦å­¦ä¹ ViTè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleã€‹</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">å‘è¡¨äº</span> <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-29 17:28:10" itemprop="dateCreated datePublished" datetime="2022-01-29T17:28:10+08:00">2022-01-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">åˆ†ç±»äº</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">ğŸŒ™è¿›é˜¶å­¦ä¹ </span></a> </span>ï¼Œ <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">â­äººå·¥æ™ºèƒ½ Artificial Intelligence</span></a> </span></span><span id="/91bcdcae.html" class="post-meta-item leancloud_visitors" data-flag-title="æ·±åº¦å­¦ä¹ ViTè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleã€‹" title="é˜…è¯»æ¬¡æ•°"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valineï¼š</span> <a title="valine" href="/91bcdcae.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/91bcdcae.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="æœ¬æ–‡å­—æ•°"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span> <span>17k</span> </span><span class="post-meta-item" title="é˜…è¯»æ—¶é•¿"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span> <span>43 åˆ†é’Ÿ</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="åŸæ–‡"><a href="#åŸæ–‡" class="headerlink" title="åŸæ–‡"></a>åŸæ–‡</h3><div class="pdfobject-container" data-target="./file/paper/2020-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE.pdf" data-height="500px"></div><p><strong>ViT</strong> è®ºæ–‡é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=YicbFdNTTy">https://openreview.net/pdf?id=YicbFdNTTy</a></p><span id="more"></span><hr><h3 id="é€Ÿè§ˆ"><a href="#é€Ÿè§ˆ" class="headerlink" title="é€Ÿè§ˆ"></a>é€Ÿè§ˆ</h3><h4 id="æ€æƒ³"><a href="#æ€æƒ³" class="headerlink" title="æ€æƒ³"></a>æ€æƒ³</h4><p>å…¶å®æ ¸å¿ƒé—®é¢˜å°±æ˜¯è€ƒè™‘å¦‚ä½•æŠŠå›¾åƒæ•°æ® H * W * C,åºåˆ—åŒ–æˆä¸€ä¸ªä¸€ä¸ªè¯é‚£ç§ç»“æ„ï¼Œè‡ªç„¶å°±æƒ³åˆ°å°†å›¾ç‰‡cropæˆä¸€ä¸ªä¸€ä¸ªpatchï¼Œå‡è®¾æœ‰Nä¸ªpatch,ç»´åº¦ä¸º p * p * C,reshapeåŠ concateä¸€ä¸‹å°±å˜æˆä¸ªN * p^2C,ä¹Ÿå°±ç±»ä¼¼è¯å‘é‡ã€‚</p><hr><h4 id="æ¨¡å‹ç»“æ„"><a href="#æ¨¡å‹ç»“æ„" class="headerlink" title="æ¨¡å‹ç»“æ„"></a>æ¨¡å‹ç»“æ„</h4><p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><img src="/91bcdcae/1.webp"><h5 id="å›¾åƒè½¬åºåˆ—"><a href="#å›¾åƒè½¬åºåˆ—" class="headerlink" title="å›¾åƒè½¬åºåˆ—"></a>å›¾åƒè½¬åºåˆ—</h5><p>å°†å›¾ç‰‡ H * W * C,cropæˆï¼®ä¸ªpatch,ç„¶ååœ¨è½¬æ¢æˆN * (p^2C),åŒæ—¶ä¸ºäº†é¿å…æ¨¡å‹ç»“æ„å—åˆ°patch sizeçš„å½±å“ï¼Œé‡‡ç”¨Linear projectå°†ä¸åŒflatten patchsè½¬æ¢æˆDç»´å‘é‡ã€‚è¿™æ ·çš„è¯è¾“å…¥å›¾ç‰‡æ•°æ®å°±æˆäº†N*DäºŒç»´çŸ©é˜µå°±å’Œè¯å‘é‡çŸ©é˜µå¯¹åº”ä¸Šäº†ã€‚</p><img src="/91bcdcae/2.webp"><h5 id="Position-embeddings"><a href="#Position-embeddings" class="headerlink" title="Position embeddings"></a>Position embeddings</h5><p>ä½œè€…ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„embeddingå‘é‡å»å°†å›¾åƒä½ç½®ä¿¡æ¯åŠ å…¥åˆ°åºåˆ—ä¸­ã€‚</p><img src="/91bcdcae/3.webp"><h5 id="learnable-embedding"><a href="#learnable-embedding" class="headerlink" title="learnable embedding"></a>learnable embedding</h5><p>ä¸Šå›¾ä¸­ï¼Œå¸¦*å·çš„ç²‰è‰²æ¡†æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„embeddingï¼Œè®°ä½Xclass,ç»è¿‡encoderåçš„ç»“æœä½œä¸ºæ•´å¼ å›¾åƒçš„è¡¨ç¤ºã€‚ä¹‹æ‰€ä»¥ä¸ç”¨å…¶ä¸­ä¸€ä¸ªpatchçš„embeddingæ˜¯å› ä¸ºï¼Œè¿™ç§embeddingä¸å¯é¿å…å¸¦æœ‰pathçš„ä¿¡æ¯ï¼Œè€Œæ–°å¢çš„è¿™ä¸ªæ²¡æœ‰è¯­ä¹‰ä¿¡æ¯ï¼Œèƒ½æ›´ä½³åæ˜ æ•´å¼ å›¾ç‰‡ã€‚</p><img src="/91bcdcae/4.webp"><h5 id="è¾“å…¥transformer-encoder"><a href="#è¾“å…¥transformer-encoder" class="headerlink" title="è¾“å…¥transformer encoder"></a>è¾“å…¥transformer encoder</h5><p>æ•´ä¸ªå…¬å¼å¦‚ä¸‹ï¼š</p><img src="/91bcdcae/5.webp"><hr><h4 id="å®éªŒç»“æœ"><a href="#å®éªŒç»“æœ" class="headerlink" title="å®éªŒç»“æœ"></a>å®éªŒç»“æœ</h4><p>åœ¨ä¸­ç­‰æ•°æ®é›†ï¼ˆä¾‹å¦‚ImageNetï¼‰ï¼Œæ•ˆæœä¸å¦‚resnetï¼Œä½†æ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œè¡¨ç°æ›´ä½³ã€‚</p><img src="/91bcdcae/6.webp"> <img src="/91bcdcae/7.webp"><hr><p>ViTï¼šè¿‡å»ä¸€å¹´ï¼ŒCV æœ€æœ‰å½±å“åŠ›çš„å·¥ä½œ</p><ul><li>æ¨ç¿»äº† 2012 Alexnet æå‡ºçš„ CNN åœ¨ CV çš„ç»Ÿæ²»åœ°ä½</li><li>æœ‰è¶³å¤Ÿå¤šçš„é¢„è®­ç»ƒæ•°æ®ï¼ŒNLP çš„ Transformer æ¬è¿åˆ° CVï¼Œæ•ˆæœå¾ˆå¥½</li><li>æ‰“ç ´ CV å’Œ NLP çš„å£å’ï¼Œç»™ CVã€å¤šæ¨¡æ€ æŒ–å‘</li></ul><p>ViTæ•ˆæœæœ‰å¤šå¥½ï¼Ÿ</p><ul><li>CV ä»»åŠ¡åˆ·æ¦œ</li><li>paperwithcodeç½‘ç«™ éœ¸æ¦œ ImageNet ï¼ˆåŸºäº ViTï¼‰å’Œ COCO ,ç›®æ ‡æ£€æµ‹ï¼ˆSwin Transformer ICCV 21 best paperï¼šå¤šå°ºåº¦çš„ ViT ï¼‰çš„æ¨¡å‹</li></ul><p>å››ç§æƒ…å†µ ViT éƒ½èƒ½å¤„ç†ï¼š<br>é®æŒ¡ã€æ•°æ®åˆ†å¸ƒçš„åç§»ï¼ˆçº¹ç†çš„å»é™¤ï¼‰ã€é¸Ÿå¤´éƒ¨+å¯¹æŠ—çš„patchã€å›¾ç‰‡æ‰“æ•£é‡æ–°æ’åˆ—ç»„åˆ</p><hr><h3 id="æ ‡é¢˜"><a href="#æ ‡é¢˜" class="headerlink" title="æ ‡é¢˜"></a>æ ‡é¢˜</h3><p><strong>An image is worth 16 * 16 words</strong></p><p>æ¯ä¸€ä¸ªæ–¹æ ¼éƒ½æ˜¯ 16 * 16 å¤§å°ï¼Œå›¾ç‰‡æœ‰å¾ˆå¤š 16 * 16 æ–¹æ ¼ patches â€“&gt; an image is worth 16 * 16 words</p><p>Transformers for image recognition at scale</p><p>transformer å»åšå¤§è§„æ¨¡çš„å›¾åƒè¯†åˆ«</p><p>ä½œè€…æ¥è‡ª Google research å’Œ Google brain team</p><hr><h3 id="æ‘˜è¦"><a href="#æ‘˜è¦" class="headerlink" title="æ‘˜è¦"></a>æ‘˜è¦</h3><p>Transformer åœ¨ NLP æ˜¯åŸºæœ¬æ“ä½œï¼Œi.e., BERT, GPT3, T5, ä½† transformer åœ¨ CV çš„åº”ç”¨æœ‰é™ã€‚</p><p>CV é‡Œçš„ attention æ˜¯æ€ä¹ˆç”¨çš„å‘¢ï¼Ÿ<br>attention + CNN, or attention æ›¿æ¢ CNN components ä½†ä¾ç„¶ä¿æŒ CNN æ•´ä½“ç»“æ„ã€‚</p><p>å¦‚ä½•ç†è§£ CNN æ•´ä½“ç»“æ„ä¸å˜ï¼Ÿ<br>ResNet 50 æœ‰ 4 ä¸ª stages (res2 res3 res4 res5), stage ä¸å˜ï¼Œattention å–ä»£ æ¯ä¸€ä¸ª stage æ¯ä¸€ä¸ª block é‡Œçš„è¿™ä¸ªæ“ä½œã€‚</p><p>æœ¬æ–‡æ€ä¹ˆçœ‹ CV é‡Œçš„ attention?<br>attention ä¸ç”¨å’Œ CNN ç»‘åœ¨ä¸€èµ·ï¼Œå’Œ æˆ‘ transformer ç»„é˜Ÿï¼Œåœ¨ CV é¢†åŸŸ å¤§æ€å››æ–¹ã€‚<br>esp, å¤§è§„æ¨¡æ•°æ®é›†åšé¢„è®­ç»ƒï¼Œmid-sized or small æ•°æ®é›†åšå¾®è°ƒï¼ŒViT SOTA</p><p>ViT fewer computational resources to train, really?<br>å°‘çš„è®­ç»ƒèµ„æº &#x3D;&#x3D; TPUv 3 + 2500 å¤©ã€‚<br>â€œfewerâ€ ç›¸å¯¹æ¥è¯´</p><hr><h3 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h3><p>self-attention æ¶æ„ï¼Œ esp Transformersï¼Œæ˜¯ NLP å¿…é€‰æ¨¡å‹ã€‚ä¸»æµæ–¹å¼æ˜¯ BERT æå‡ºçš„ï¼Œå¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒï¼Œåœ¨ ç‰¹å®šé¢†åŸŸçš„å°æ•°æ®é›† åšå¾®è°ƒã€‚ Transformer çš„ è®¡ç®—é«˜æ•ˆå’Œå¯æ‰©å±•æ€§ï¼Œ1000äº¿å‚æ•°éƒ½è¿˜æ²¡æœ‰ æ€§èƒ½é¥±å’Œ çš„ç°è±¡ã€‚<br>i.e., MT-N;P 5300äº¿å‚æ•°ï¼ŒSOTAï¼Œä¹Ÿæ²¡æœ‰æ€§èƒ½é¥±å’Œçš„ç°è±¡ã€‚</p><p>Transformer åº”ç”¨åœ¨ CV æœ‰<strong>éš¾ç‚¹</strong>å—ï¼Ÿ<br>è®¡ç®—åƒç´ çš„ self-attentionï¼Œåºåˆ—é•¿ï¼Œç»´åº¦çˆ†ç‚¸<br>Trnasformer çš„è®¡ç®—å¤æ‚åº¦æ˜¯ åºåˆ—é•¿åº¦ n çš„ å¹³æ–¹ Oï¼ˆn^2ï¼‰<br>224 åˆ†è¾¨ç‡çš„å›¾ç‰‡ï¼Œæœ‰ 50176 ä¸ªåƒç´ ç‚¹ï¼Œï¼ˆ2d å›¾ç‰‡ flattenï¼‰åºåˆ—é•¿åº¦æ˜¯ BERT çš„è¿‘ 100 å€ã€‚</p><img src="/91bcdcae/8.webp"><p>å‘¼åº”æ‘˜è¦+æ–‡çŒ®ï¼š<strong>CNN åœ¨ CV é¢†åŸŸ ç«ï¼Œ Transformer, self-attention åœ¨ NLP é¢†åŸŸ ç«ã€‚CV å¦‚ä½•ç”¨ attention å‘¢ï¼Ÿ</strong><br>CNN ç»“æ„ + self-attention or attention æ›¿ä»£å·ç§¯</p><p>CVPR Wang et al. 2018, Local Network, ç½‘ç»œä¸­çš„ç‰¹å¾å›¾ è¾“å…¥ Transformer<br>ResNet 50 æœ€åä¸€ä¸ª stage, res4 çš„ feature map 14 * 14ï¼Œ 196</p><p>é™ä½åºåˆ—é•¿åº¦çš„æ–¹å¼ï¼šç”¨ç‰¹å¾å›¾åš transformer è¾“å…¥ï¼ˆWang et al 2018ï¼‰, replacing the convolutions entirely (Ramachandran et al., 2019 stand-alone attention å­¤ç«‹è‡ªæ³¨æ„åŠ›; Wang et al., 2020 axial attention è½´æ³¨æ„åŠ›)</p><p>stand-alone attention å­¤ç«‹è‡ªæ³¨æ„åŠ›<br>ç”¨ local window å±€éƒ¨å°çª—å£ æ§åˆ¶ transformer çš„è®¡ç®—å¤æ‚åº¦ï¼Œæœ‰ç‚¹åƒ å·ç§¯ï¼Œ å·ç§¯ä¹Ÿæœ‰ localityï¼Œå±€éƒ¨çª—å£å·ç§¯ã€‚</p><p>axial attention è½´æ³¨æ„åŠ› â€“&gt; 2 ä¸ª 1d é¡ºåºæ“ä½œï¼Œé™ä½è®¡ç®—å¤æ‚åº¦<br>å›¾ç‰‡çš„åºåˆ—é•¿åº¦ n &#x3D; H * W<br>2d çŸ©é˜µ æ‹†åˆ†ä¸º 2ä¸ª1d å‘é‡ï¼Œå…ˆåœ¨ H é«˜åº¦ dimension åšä¸€æ¬¡ self-attentionï¼Œå† W å®½åº¦ dimension åšä¸€æ¬¡ self-attention</p><p><strong>replacing the convolutions entirely å¥½ä¸å¥½å‘¢ï¼Ÿ</strong><br>ç†è®ºé«˜æ•ˆï¼Œä½†ç¡¬ä»¶æ— æ³•åŠ é€Ÿ â€“&gt; æ­¤ç±»æ¨¡å‹éƒ½è¿˜æ²¡æœ‰å¤ªå¤§ã€‚<br>æœ¬æ®µï¼ˆç¬¬äºŒæ®µï¼‰æ€»ç»“ï¼šåœ¨å¤§è§„æ¨¡çš„å›¾åƒè¯†åˆ«ä¸Šï¼ŒResNet è¿˜æ˜¯æ•ˆæœæœ€å¥½çš„ã€‚</p><p><strong>æœ¬æ–‡ ViT çš„å·¥ä½œæ˜¯ä»€ä¹ˆï¼Ÿ</strong><br><strong>ç°çŠ¶ï¼š</strong>attention å·²ç»åœ¨ CV é¢†åŸŸæœ‰åº”ç”¨ï¼Œç”šè‡³ä¹Ÿæœ‰ attention æ›¿ä»£å·ç§¯çš„æ“ä½œ<br><strong>è®²æ•…äº‹çš„è§’åº¦ï¼š</strong>Inspired by the Transformer scaling å¯æ‰©å±•æ€§ success in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.<br>æ ‡å‡† Transformer ç›´æ¥åº”ç”¨äºå›¾ç‰‡ï¼Œåšæœ€å°‘çš„ä¿®æ”¹ï¼Œä¸åšä»»ä½•é’ˆå¯¹è§†è§‰ä»»åŠ¡çš„ç‰¹å®šçš„æ”¹å˜ã€‚</p><p>The fewest possible modifications æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ<br>æŠŠå›¾ç‰‡åˆ’åˆ†æˆå¾ˆå¤š patchesï¼Œæ¯ä¸ª patch å…ƒç´  æ˜¯ 16 * 16ï¼Œåºåˆ—é•¿åº¦ 14 * 14 &#x3D; 196ä¸ªå…ƒç´ </p><p>æ¯ä¸€ä¸ª patch ç»è¿‡ä¸€ä¸ª FC layer(fully connected layer)å¾—åˆ°ä¸€ä¸ª linear embeddingï¼Œpatches çš„ linear embeddings æ˜¯ Transformer çš„è¾“å…¥ã€‚</p><p>ä¸€ä¸ª 224 * 224 å›¾ç‰‡ å˜æˆä¸€ä¸ª 196 ä¸ªçš„ 16 * 16 å›¾ç‰‡å—ï¼ˆwords in NLPï¼‰ã€‚</p><p><strong>ä¸ºä»€ä¹ˆ transformer çš„è®­ç»ƒæ˜¯ supervised fashionï¼Ÿ</strong><br>NLP çš„ Transformer æ— ç›‘ç£è®­ç»ƒ by language model LM or mask language model MLMï¼›CV ä»»åŠ¡çš„ benchmark ä½¿ç”¨æœ‰ç›‘ç£è®­ç»ƒã€‚</p><p>ViT æŠŠ CV ä»»åŠ¡å½“æˆ NLP ä»»åŠ¡ï¼Œæ¨¡å‹ä½¿ç”¨çš„æ˜¯ BERT, Transformer encoder ç®€æ´æ¡†æ¶ã€‚Transformer åœ¨è§†è§‰ä¹Ÿæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚</p><p><strong>Transformer in CVï¼Œä¹‹å‰æœ‰äººåšå—ï¼Ÿ</strong><br>ICLR 2020 ä»è¾“å…¥å›¾ç‰‡é‡ŒæŠ½å– 2 * 2 patchesã€‚ 2 * 2 size enoughï¼šCIFAR-10 32 * 32 å›¾ç‰‡ï¼Œ16 * 16 ä¼šè¿‡å¤§ã€‚ æŠ½å¥½ patch ä¹‹åï¼Œåœ¨ patches ä¸Š åš self-attentionã€‚ â€“&gt; æŠ€æœ¯ä¸Šçš„ Vision Transformer</p><p><strong>ViT å’Œ ICLR 2 * 2 patches çš„åŒºåˆ«ï¼Ÿ</strong></p><ul><li>ViTè¯æ˜äº† å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒ ï¼ˆNLP å¸¸ç”¨ï¼‰ä¹‹åçš„ Transformerï¼Œä¸éœ€è¦åš é’ˆå¯¹è§†è§‰ä»»åŠ¡çš„ ä¿®æ”¹ï¼Œæ¯”æœ€å¥½çš„ CNNs æ•ˆæœå·®ä¸å¤š or ç”šè‡³æ›´å¥½ã€‚</li><li>2 * 2 patches applicable only to small-resolution images, ViT handles medium-resolution images as well.</li><li>ViT å‘Šè¯‰å¤§å®¶ï¼ŒTransformer åœ¨ vision é¢†åŸŸèƒ½æ‹“å±•åˆ°æœ‰å¤šå¥½ã€‚large æ•°æ®é›† + large æ¨¡å‹ï¼Œtransformer èƒ½å¦å–ä»£ CNN åœ°ä½ï¼Ÿ</li></ul><p><strong>å¼•è¨€çš„æœ€åï¼š</strong><br>æœ€æƒ³è¯´çš„ç»“è®º or æœ€æƒ³å±•ç¤ºçš„ç»“æœï¼šå–ç‚¹ï¼Œä¸ç”¨çœ‹å®Œæ•´ç¯‡è®ºæ–‡ï¼Œå°±çŸ¥é“æ­¤ç¯‡è®ºæ–‡çš„è´¡çŒ®ã€‚</p><p><strong>ViT ä»»ä½•æƒ…å†µéƒ½å¾ˆå¼ºå—ï¼Ÿ</strong><br>No<br>mid-sized datasets ImageNet without strong regularizationï¼ŒViT æ¯” ResNet of comparable size å¼±å‡ ä¸ªç‚¹ã€‚</p><p><strong>Why å¼±ï¼Ÿ expected</strong><br>Transformer æ¯” CNN å°‘ inductive biases å½’çº³åç½®<br>inductive biases å½’çº³åç½®ï¼šå…ˆéªŒçŸ¥è¯† or æå‰çš„å‡è®¾<br>CNN çš„ inductive biases æ˜¯ locality å’Œ å¹³ç§»ç­‰å˜æ€§ translation equaivarianceï¼ˆå¹³ç§»ä¸å˜æ€§ spatial invarianceï¼‰ã€‚</p><p>locality: CNNç”¨æ»‘åŠ¨çª—å£åœ¨å›¾ç‰‡ä¸Šåšå·ç§¯ã€‚å‡è®¾æ˜¯å›¾ç‰‡ç›¸é‚»çš„åŒºåŸŸæœ‰ç›¸ä¼¼çš„ç‰¹å¾ã€‚i.e., æ¡Œæ¤…åœ¨ä¸€èµ·çš„æ¦‚ç‡å¤§ï¼Œè·ç¦»è¿‘çš„ç‰©å“ ç›¸å…³æ€§è¶Šå¼ºã€‚</p><p>translation equaivarianceï¼šf(g(x)) &#x3D; g(f(x))<br>f å’Œ g å‡½æ•°çš„é¡ºåºä¸å½±å“ç»“æœã€‚<br>fï¼šå·ç§¯ gï¼šå¹³ç§»; æ— è®ºå…ˆåšå¹³ç§» g è¿˜æ˜¯å…ˆåšå·ç§¯ f , æœ€åç»“æœä¸€æ ·ã€‚<br>CNN çš„å·ç§¯æ ¸ åƒä¸€ä¸ª template æ¨¡æ¿ï¼ŒåŒæ ·çš„ç‰©ä½“æ— è®ºç§»åŠ¨åˆ°å“ªé‡Œï¼Œé‡åˆ°äº†ç›¸åŒçš„å·ç§¯æ ¸ï¼Œå®ƒçš„è¾“å‡ºä¸€è‡´ã€‚</p><p>CNN æœ‰ locality å’Œ translation equivariance å½’çº³åç½®ï¼Œâ€“&gt; CNN æœ‰ å¾ˆå¤šå…ˆéªŒä¿¡æ¯ â€“&gt; éœ€è¦è¾ƒå°‘çš„æ•°æ®å»å­¦å¥½ä¸€ä¸ªæ¨¡å‹ã€‚</p><p>Transformer æ²¡æœ‰è¿™äº›å…ˆéªŒä¿¡æ¯ï¼Œåªèƒ½ ä»å›¾ç‰‡æ•°æ®é‡Œï¼Œè‡ªå·±å­¦ä¹ å¯¹ è§†è§‰ä¸–ç•Œ çš„æ„ŸçŸ¥ã€‚</p><p><strong>æ€ä¹ˆéªŒè¯ Transformer æ—  inductive bias çš„å‡è®¾ï¼Ÿ</strong><br>åœ¨ 1400ä¸‡(ImageNet-21K) - 3000 ä¸‡(JFT-300)å¾—åˆ°å›¾ç‰‡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒ trumps inductive bias, ViT +è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼ŒCV SOTAã€‚</p><p>VTAB èåˆäº† 19 ä¸ªæ•°æ®é›†ï¼Œæ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒViTçš„ robustness ä¹Ÿå¾ˆå¥½ã€‚</p><p><strong>å¼•è¨€æ€»ç»“ï¼š</strong><br><strong>ç¬¬ä¸€æ®µï¼š</strong>Transformer åœ¨ NLP æ‰©å±•çš„å¾ˆå¥½ï¼Œæ²¡æœ‰å› ä¸ºå¤§æ¨¡å‹å’Œå¤§æ•°æ®é›†è€Œé¥±å’Œï¼Œperformance ä¸€ç›´æœ‰æå‡ï¼ŒTransformer åœ¨ CV é‡Œèƒ½ä¸èƒ½ä¹Ÿæœ‰å¤§å¹…åº¦çš„æå‡å‘¢ï¼Ÿ<br><strong>ç¬¬äºŒæ®µï¼š</strong>å‰äººå·¥ä½œã€‚è¿™ä¹ˆå¥½çš„ idea æœ‰å“ªäº›äººåšè¿‡å‘¢ï¼Ÿè¦è®²æ¸…æ¥šè‡ªå·±çš„å·¥ä½œå’Œ related works çš„åŒºåˆ«<br>ä¹‹å‰çš„å·¥ä½œæ˜¯ CNN + attention æˆ–è€… attention æ›¿ä»£ convolutionsï¼Œæ²¡æœ‰å·¥ä½œå°† transformer ç”¨åˆ° CV é¢†åŸŸï¼Œæ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„æ‰©å±•æ•ˆæœã€‚<br><strong>ç¬¬ä¸‰æ®µï¼š</strong>Vision Transformer æ˜¯ standard Transformer with the fewest possible modifications<br>å¯¹å›¾ç‰‡çš„æœ€å°‘ä¿®æ”¹æ˜¯ä»€ä¹ˆï¼Ÿ<br>å›¾ç‰‡å˜æˆ 16 * 16 çš„åƒç´ å— patchesï¼Œç»è¿‡ ä¸€ä¸ª fc layer å¾—åˆ°çš„ linear embeddings è¾“å…¥ transformer<br>ViT èåˆäº† CV å’Œ NLP é¢†åŸŸã€‚<br><strong>ç¬¬å››+äº”æ®µï¼š</strong>show ç»“æœ<br>è¶³å¤Ÿå¤šçš„æ•°æ®é›†ï¼ŒViT èƒ½ SOTA</p><hr><h3 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h3><p>explored the direct application of Transformers to image recognition. ç›´æ¥ ç”¨ NLP çš„ Transformer æ¥å¤„ç†å›¾ç‰‡ã€‚</p><p><strong>å’Œå…¶å®ƒ self-attention in CV çš„å·¥ä½œä¸åŒï¼š</strong>é™¤äº†å°†å›¾ç‰‡è½¬æˆ 16 * 16 patches + ä½ç½®ç¼–ç  ä¹‹å¤–ï¼Œæ²¡æœ‰é¢å¤–å¼•å…¥ å›¾åƒç‰¹æœ‰çš„ inductive bias</p><p><strong>æ²¡æœ‰å›¾ç‰‡çš„ inductive bias çš„å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ</strong><br>ä¸éœ€è¦å¯¹ vision é¢†åŸŸçš„äº†è§£ï¼Œä¸éœ€è¦ domain knowledgeï¼Œç›´æ¥æŠŠ å›¾ç‰‡ç†è§£æˆ a sequence of patches, i.e., ä¸€ä¸ªå¥å­é‡Œçš„å¾ˆå¤šå•è¯ã€‚<br>An image is worth 16 * 16 words.</p><p>ç›´æ¥ç”¨ NLP é‡Œçš„ Transformer encoderï¼Œ simple yet scalableï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ•ˆæœéå¸¸å¥½ã€‚</p><p><strong>ViT æ•ˆæœæœ‰å¤šå¥½ï¼Ÿ</strong><br>image classification SOTA, relatively cheap to pre-train</p><p><strong>ViT æ²¡æœ‰è§£å†³çš„é—®é¢˜ï¼Ÿ</strong><br>æ–‡ç« æŒ–å‘ï¼šæ–°é—®é¢˜ or æ–°æ¨¡å‹<br>ViT æŒ–å‘ï¼šæ–°æ¨¡å‹ ViT<br>**future directions: **æ–°é—®é¢˜ â€”â€” CV é™¤äº† image classfication å…¶ä»–çš„ä»»åŠ¡ï¼Œè¡Œä¸è¡Œå‘¢ï¼Ÿåˆ†å‰²ã€æ£€æµ‹</p><p>DETR (Carion et al. 2020) ç›®æ ‡æ£€æµ‹çš„åŠ›ä½œï¼Œæ”¹å˜äº†ç›®æ ‡æ£€æµ‹ å‡ºæ¡†çš„æ–¹å¼ã€‚ViT åšå…¶å®ƒ CV ä»»åŠ¡åº”è¯¥æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚</p><p>2020å¹´ 12 æœˆ(ViT 1.5æœˆä¹‹å)<br>ViT-FRCNN æ£€æµ‹ detection<br>SETR åˆ†å‰² segmentation ï¼ˆCVPR è®ºæ–‡ 11.15å®Œæˆå†™ä½œæŠ•ç¨¿ï¼‰<br>ï¼ˆ3ä¸ªæœˆåï¼‰Swin Transformer èåˆ Transformer å’Œå¤šå°ºåº¦è®¾è®¡</p><p>Transformer æ˜¯ CV é¢†åŸŸçš„ä¸€ä¸ªé€šç”¨çš„éª¨å¹²ç½‘ç»œ backbone<br>å¦å¤–ä¸€ä¸ªæœªæ¥å·¥ä½œæ–¹å‘ï¼Œè‡ªç›‘ç£çš„é¢„è®­ç»ƒæ–¹å¼ã€‚<br>NLP å¤§çš„ transformer æ¨¡å‹ä½¿ç”¨ è‡ªç›‘ç£ é¢„è®­ç»ƒï¼ŒViTæœ‰ initial experiments è¯æ˜ è‡ªç›‘ç£é¢„è®­ç»ƒä¹Ÿå¯ä»¥ï¼Œä½†å’Œæœ‰ç›‘ç£çš„è®­ç»ƒæœ‰å·®è· still large gapã€‚</p><p>æŠŠ ViT å˜å¾—å¾ˆå¤§ï¼Œwould likely lead to improved performanceã€‚scaling ViT, ViT-G, ImageNet 90 +%</p><p>ViT æŒ–å‘ï¼š</p><ul><li>è§†è§‰é¢†åŸŸ CV</li><li>å¤šæ¨¡æ€ï¼Œä¸€ä¸ª transformer å¤„ç† CV å’Œ NLP</li></ul><hr><h3 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h3><p>Transformer åœ¨ NLP é¢†åŸŸçš„åº”ç”¨ï¼šBERT, GPT<br>Transformer å…ˆåœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šåšé¢„è®­ç»ƒï¼Œå†æ ¹æ®å…·ä½“çš„ä»»åŠ¡æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚<br>BERT: denosing maskæŒ–è¯ã€å®Œå½¢å¡«ç©ºï¼ŒæŠŠmaskedçš„è¯é¢„æµ‹å‡ºæ¥<br>GPT: language modelling, é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ next word prediction<br>å®Œå½¢å¡«ç©º or é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œäººä¸ºè®¾å®šã€‚è¯­æ–™å¥å­æ˜¯å®Œæ•´çš„ï¼Œå»æ‰æŸäº›è¯ï¼ˆå®Œå½¢å¡«ç©ºï¼‰ or æœ€åè¯ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰ â€“&gt; è‡ªç›‘ç£çš„è®­ç»ƒæ–¹å¼ã€‚</p><p>self-attention åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨</p><p>self-attention to each pixelï¼šâŒ<br>224 * 224 image: O(n^2 &#x3D; 50176)<br>1k, 4k image: ç»´åº¦çˆ†ç‚¸</p><p>self-attention to each image with approximationsï¼š</p><ul><li>ä¸ç”¨æ•´å¼ å›¾ï¼Œåªç”¨ local neighborhoodsï¼Œé™ä½åºåˆ—é•¿åº¦</li></ul><p>sparse transformer</p><ul><li>å…¨å±€æ³¨æ„åŠ›çš„è¿‘ä¼¼</li><li>åªå¯¹ ç¨€ç–çš„ç‚¹ åšæ³¨æ„åŠ›</li></ul><p>scale attention by applying attention in blocks of varying size</p><ul><li>æŠŠè‡ªæ³¨æ„åŠ›ç”¨åˆ°ä¸åŒå¤§å°çš„ blocks</li><li>in the extreme case only along individual axes æç«¯æƒ…å†µï¼Œåªå…³å¿ƒè½´ï¼Œ axial self-attentionï¼Œæ¨ªè½´ + çºµè½´</li></ul><p>å°ç»“ï¼šä»¥ä¸Š self-attention + CV æ•ˆæœä¸é”™ï¼Œä½†å·¥ç¨‹å®ç°åŠ é€Ÿå¾ˆéš¾ã€‚å¯åœ¨ cpu gpuè·‘ï¼Œä½†å¤§è§„æ¨¡è®­ç»ƒä¸è¡Œã€‚</p><p><strong>å’Œ ViT æœ€ç›¸ä¼¼çš„å·¥ä½œï¼š</strong><br>ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 å›¾ç‰‡<br><strong>ViT èƒœåœ¨å“ªé‡Œï¼š</strong>æ›´å¤§çš„ patches 16 *16 + æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†</p><p>CV ä¸­ æ£€æµ‹ã€åˆ†ç±»ã€è§†é¢‘å¤„ç†ã€å¤šæ¨¡æ€ self-attention with CNNs</p><p><strong>å¦ä¸€ä¸ªç›¸ä¼¼å·¥ä½œï¼šimage GPT</strong><br>GPT æ˜¯ NLP çš„ç”Ÿæˆæ¨¡å‹ï¼Œimage GPT æ— ç›‘ç£é¢„è®­ç»ƒï¼Œç”Ÿæˆæ¨¡å‹ã€‚</p><p>image GPT ä¹Ÿç”¨äº† transformer å›¾ç‰‡ï¼ˆé™ä½åˆ†è¾¨ç‡å’Œ color spaceï¼‰ã€‚ç”¨è®­ç»ƒå¥½çš„ image GPT or ç›´æ¥æŠŠ image GPT å½“æˆç‰¹å¾æå–å™¨ï¼Œ ImageNet å‡†ç¡®ç‡ 72%ï¼›ViT ImageNet å‡†ç¡®ç‡ 88.5%</p><p><strong>psï¼šæœ€è¿‘çˆ†ç«çš„ MAE</strong><br>åœ¨ BEiT æˆ– MAE è®ºæ–‡ä¹‹å‰ï¼Œç”Ÿæˆå¼ç½‘ç»œ åœ¨ CV æ¯” åˆ¤åˆ«å¼ç½‘ç»œ å¼±å¾ˆå¤šã€‚<br>MAE ç”Ÿæˆå¼æ¨¡å‹ åœ¨ ImageNet-1k åšè®­ç»ƒï¼Œæ¯”åˆ¤åˆ«å¼æ¨¡å‹å¥½ã€‚åˆ†ç±» âœ”ï¼Œç›®æ ‡æ£€æµ‹ âœ” ï¼ˆtransfer learningï¼‰</p><p>è¿˜æœ‰å…¶å®ƒå’Œ ViT ç›¸ä¼¼çš„å·¥ä½œå—ï¼Ÿ</p><ul><li>ç”¨æ¯” ImageNet è¿˜å¤§çš„æ•°æ®é›†åšé¢„è®­ç»ƒï¼Œå¤§åŠ›å‡ºå¥‡è¿¹</li><li>Sun et al 2017 JFT-300M æ•°æ®é›†ï¼ŒCNN çš„æ•ˆæœéšæ•°æ®é›†å¢åŠ è€Œæå‡</li><li>Djolonga et al 2020 ç ”ç©¶å¤§æ•°æ®é›†é¢„è®­ç»ƒè¿ç§»åˆ°å°æ•°æ®é›†çš„æ•ˆæœ</li><li>åœ¨ ImageNet-21K æˆ– JFT-300M æ•°æ®é›†åšé¢„è®­ç»ƒï¼Œè¿ç§»åˆ° ImageNet æˆ– CIFAR-100 æ•ˆæœæ€ä¹ˆæ ·</li></ul><p>æœ¬æ–‡ ViT å’Œè¿™äº›ç›¸ä¼¼è®ºæ–‡çš„å…³ç³»ï¼Ÿ<br>ViT å…³æ³¨ ImageNet-21K æˆ– JFT-300M æ•°æ®é›†ï¼Œ ä¸è®­ç»ƒ ResNetï¼Œè®­ç»ƒ Transformer</p><p>Related work å†™ä½œæ€»ç»“ï¼š<br>æ–¹æ–¹é¢é¢ç›¸å…³çš„éƒ½å†™åˆ°äº†ï¼Œä¹Ÿåˆ—ä¸¾äº†éå¸¸ç›¸ä¼¼çš„å·¥ä½œ ICLR 2020 2*2 patches, image GPT ç”Ÿæˆæ¨¡å‹ï¼Œå¤§æ•°æ®é›† BIT ç›¸å…³çš„æ–‡ç« </p><p>Related work ç›®çš„ï¼š<br>è®©è¯»è€…çŸ¥é“åœ¨ä½ çš„å·¥ä½œä¹‹å‰ï¼Œåˆ«äººåšäº†å“ªäº›å·¥ä½œï¼Œä½ è·Ÿä»–ä»¬çš„åŒºåˆ«åœ¨å“ªé‡Œ</p><p>related work ç« èŠ‚çš„è¯¦ç»†ä¸ä¼šé™ä½è®ºæ–‡çš„åˆ›æ–°æ€§ï¼Œåè€ŒåŠ åˆ†ï¼Œè®©æ•´ä¸ªæ–‡ç« å˜å¾—æ›´ç®€å•æ˜“æ‡‚ã€‚</p><hr><h3 id="ViTæ¨¡å‹"><a href="#ViTæ¨¡å‹" class="headerlink" title="ViTæ¨¡å‹"></a>ViTæ¨¡å‹</h3><p>ViT å°½å¯èƒ½ä½¿ç”¨ original Transformerï¼Œäº«å— Transformer efficient implementationsã€‚<br>NLP ä¸­ Transformer å¾ˆç«ï¼Œæœ‰å¾ˆå¤š Transformer çš„é«˜æ•ˆå®ç°</p><h4 id="Vision-Transformer-æ¨¡å‹å›¾"><a href="#Vision-Transformer-æ¨¡å‹å›¾" class="headerlink" title="Vision Transformer æ¨¡å‹å›¾"></a>Vision Transformer æ¨¡å‹å›¾</h4><p>Model overview<br>å¥½å›¾ï¼šä»¥å›¾è¯»è®ºæ–‡ï¼Œè®²è§£ ViT ç›´æ¥å¤åˆ¶</p><img src="/91bcdcae/9.webp"><p><strong>Input:</strong> 1 å¼ å›¾<br><strong>Process:</strong> ä¹å®«æ ¼ 9 patches â€“&gt; Flattened Patches (3 * 3 â€“&gt; 1 * 9 æ‹å¹³) â€“&gt; Linear Projections â€”&gt; Patch embedding<br><strong>Why need position embedding?</strong><br>self-attention æ‰€æœ‰å…ƒç´ ä¸¤ä¸¤ç®—è‡ªæ³¨æ„åŠ›ï¼Œå’Œé¡ºåºä½ç½®æ— å…³ã€‚ä½†å›¾ç‰‡çš„ patches æ˜¯æœ‰é¡ºåºçš„ï¼Œ+ position embedding</p><p>Patch embedding + position embedding &#x3D;&#x3D; token åŒ…å« å›¾ç‰‡ patch ä¿¡æ¯ å’Œ patch åœ¨åŸå›¾ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚</p><p><strong>ViT å¯¹ å›¾ç‰‡çš„æ“ä½œï¼š</strong> åˆ’åˆ† patchesï¼Œflatten patches çš„çº¿æ€§æŠ•å½± + patches çš„ä½ç½®ä¿¡æ¯ï¼Œå¾—åˆ°è¾“å…¥ transformer çš„ tokens</p><p>å¾—åˆ° tokens ä¹‹åï¼Œå¯¹ visual tokens è¿›è¡Œ NLP æ“ä½œï¼š<br>tokens ä¼ å…¥ Transformer encoderï¼Œå¾—åˆ°å¾ˆå¤šè¾“å‡ºã€‚</p><p><strong>Q: æ¯ä¸€ä¸ª token éƒ½æœ‰è¾“å‡ºï¼Œç”¨å“ªä¸ªè¾“å‡ºåˆ†ç±»å‘¢ï¼Ÿ</strong><br>å€Ÿé‰´ BERTï¼Œ extra learnable {class} embedding â€“&gt;, a special classification token, * in figure 1.<br>ä¹Ÿæœ‰ position embedding, 0(æ°¸è¿œæ˜¯0)</p><p><strong>Q: Why worksï¼Ÿself-attention O(n^2)</strong><br>self-attention in transformer encoderï¼Œæ‰€æœ‰çš„ tokens åœ¨åšä¸¤ä¸¤çš„äº¤äº’ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¹Ÿä¼šå’Œæ‰€æœ‰çš„ å›¾ç‰‡ patches çš„ token äº¤äº’ï¼Œä»è€Œä»å›¾ç‰‡ patches + position çš„ embedding å­¦åˆ°æœ‰ç”¨ä¿¡æ¯ï¼Œæœ€åç”¨åšåˆ†ç±»åˆ¤æ–­ã€‚</p><p><strong>Q: ä»æ€ä¹ˆå¾—åˆ°æœ€åçš„åˆ†ç±»ï¼Ÿ é€šç”¨MLP Head</strong><br>è¾“å…¥ ä¸€ä¸ªé€šç”¨çš„ MLP Headï¼Œå¾—åˆ° Classï¼Œcross-entropy æŸå¤±å‡½æ•°è®­ç»ƒæ¨¡å‹ã€‚</p><p><strong>Q: ViT ç”¨äº†æ ‡å‡†çš„ transformer ç»“æ„ï¼ŒViTçš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ</strong><br>å›¾ç‰‡ patches åŒ– + position embedding è½¬åŒ–ä¸º tokens</p><p><strong>ViT å‰å‘è¿‡ç¨‹</strong><br><strong>Vision é—®é¢˜ å˜æˆ NLP é—®é¢˜</strong></p><p>å›¾ç‰‡ Xï¼š 224 * 224 * 3 (RGB, 3 channels)<br>patches æ•° Nï¼š 224 ^ 2 &#x2F; 16 ^ 2 &#x3D; 14 ^ 2 &#x3D; 196<br>æ¯ä¸€ä¸ª patch çš„ç»´åº¦ï¼š16 * 16 * 3 (RGB, 3 channels) &#x3D; 768<br>Linear Projection å…¨è¿æ¥å±‚ E: 768( ä¸å˜ï¼Œpatch è®¡ç®—è€Œæ¥ ) * D(embedding_dim) 768 æˆ– æ›´å¤§<br>å›¾ç‰‡ X * E &#x3D; patches (196 patches ä¸ªæ•° * 768 æ¯ä¸ª patch çš„ç»´åº¦) * E ( 768 * D ) &#x3D; 196 * D (768)</p><img src="/91bcdcae/10.webp"><p>**Vision to NLP done! **<br>a 2d image â€“&gt; a sequence 1d tokens</p><p><strong>Q: è¿›å…¥ transformer encoder çš„åºåˆ—é•¿åº¦ï¼Ÿ</strong><br>196 * 768(å›¾ç‰‡å¯¹åº”çš„ tokens) æ‹¼æ¥ concatenate [CLS] token (1 * 768) &#x3D; 197 * 768</p><p><strong>Q: position embedding æ€ä¹ˆåŠ  patch embeddingï¼Ÿsum()</strong><br>å›¾1 çš„ 1-9 ä¸æ˜¯çœŸæ­£ä½¿ç”¨çš„ position embeddingï¼Œå®é™…çš„ position embedding è¡¨ï¼Œ1 - 5 è¡Œ ä»£è¡¨ å›¾1 çš„ 1 - 5 å€¼ã€‚<br>æ¯è¡Œå‘é‡çš„ç»´åº¦æ˜¯ 1 * 768<br><strong>ç›¸åŠ  sumï¼š</strong><br>patch embeddingï¼ˆ197 * 768ï¼‰ + position embedding ï¼ˆ(1 CLS + 196 patches) * 768ï¼‰&#x3D; ï¼ˆ197 * 768ï¼‰</p><img src="/91bcdcae/11.webp"><p>ViT base: 12 heads<br>MLPï¼šæ”¾å¤§ 4 å€ï¼Œå†ç¼©å°åˆ°åŸç»´åº¦å¤§å°<br>Transfomer encoder è¾“å…¥è¾“å‡ºç»´åº¦ä¸€è‡´ï¼Œå¯ä»¥ç›´æ¥å åŠ  L ä¸ª</p><img src="/91bcdcae/12.webp"><hr><h4 id="Vision-Transformeræ­£æ–‡"><a href="#Vision-Transformeræ­£æ–‡" class="headerlink" title="Vision Transformeræ­£æ–‡"></a>Vision Transformeræ­£æ–‡</h4><p>å…¬å¼çš„å…·ä½“å€¼è®¡ç®—ï¼Œå‚è€ƒä¸Šä¸€å°èŠ‚ã€‚<br>æœ‰äº†å…·ä½“å«ä¹‰çš„å…¬å¼å­—ç¬¦ï¼Œä¹Ÿä¸é‚£ä¹ˆå¯æ€•äº†å‘¢ o(<em>ï¿£â–½ï¿£</em>)ãƒ–</p><p>ViT ç”¨çš„æ˜¯ BERT 1d position embeddingï¼Œå›¾ç‰‡ 2d aware position embedding ç»“æœä¹Ÿå·®ä¸å¤šã€‚</p><p><strong>D.3 Head type and class token ä½œè€…çš„æ¶ˆèå®éªŒ</strong><br>ViT é™¤äº†æ ‡å‡†çš„ transformerï¼Œå…³é”®éƒ¨åˆ†æ˜¯ æ€ä¹ˆå¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç† å’Œ æ€ä¹ˆå¯¹å›¾ç‰‡æœ€åçš„è¾“å‡ºè¿›è¡Œåå¤„ç†ã€‚</p><p><strong>class tokenï¼šè¯æ˜ æ ‡å‡†çš„ transformer åšè§†è§‰ï¼Œæ²¡é—®é¢˜ï¼</strong><br>æ§åˆ¶å’Œ NLP çš„å·®å¼‚ï¼šä½¿ç”¨ BERT çš„ CLSï¼ŒCLS åœ¨ NLP ç†è§£ä¸º ä¸€ä¸ªå…¨å±€çš„å¯¹å¥å­ç†è§£çš„ç‰¹å¾ï¼›ViT çš„ CLS ç†è§£ä¸º ä¸€ä¸ªå›¾åƒçš„æ•´ä½“ç‰¹å¾ã€‚</p><p>CLS token + MLP (tanh acitvation) &#x3D;&#x3D; åˆ†ç±»</p><p>CV é€šå¸¸çš„ å…¨å±€ç‰¹å¾ï¼ši.e., Res50<br>feature map (14 * 14) â€“&gt; GAP globally average-pooling å…¨å±€å¹³å‡æ± åŒ– â€“&gt; a flatten vector å…¨å±€çš„å›¾ç‰‡ç‰¹å¾å‘é‡ â€“&gt; MLP åˆ†ç±»</p><p>ç±»ä¼¼çš„ï¼ŒTransformer çš„ è¾“å‡ºå…ƒç´  + GAP å¯ä»¥ç”¨åšå…¨å±€ä¿¡æ¯ + åˆ†ç±»å—ï¼Ÿ Ok</p><p><strong>CV çš„ CLS GAP å’Œ NLP çš„ CLS æ•ˆæœå·®å¼‚ä¸å¤§ã€‚</strong></p><p>CLS-Token å’Œ GAP çš„ é€‚ç”¨å‚æ•° ä¸ä¸€æ ·ã€‚</p><p><strong>ä½ç½®ç¼–ç ï¼š 1d 2d relative æ— æ‰€è°“</strong></p><p><strong>1dï¼š</strong>NLP 1, 2, 3, â€¦, 9 D<br><strong>2dï¼š</strong>D &#x2F; 2 * D &#x2F; 2<br>11 12 13<br>21 22 23<br>31 32 33</p><img src="/91bcdcae/13.webp"><p><strong>relative: offset</strong><br>ç»å¯¹è·ç¦»è½¬ç›¸å¯¹è·ç¦»ï¼Œ1 - 9 å’Œ -4, â€¦, 0, â€¦, 4</p><p><strong>ä¸ºå•¥éƒ½æ˜¯ 0.64 å·¦å³ï¼Œæ— æ‰€è°“ï¼Ÿ</strong><br>ViT ç›´æ¥ä½œç”¨äº 14 * 14 patchesï¼Œè€Œä¸æ˜¯ 224 * 224 åƒç´ ã€‚è¾ƒå°‘æ•°é‡çš„ patches ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œå®¹æ˜“å­¦åˆ°ã€‚</p><hr><h4 id="ViT-æ­£æ–‡-CLS-continued"><a href="#ViT-æ­£æ–‡-CLS-continued" class="headerlink" title="ViT æ­£æ–‡ CLS continued"></a>ViT æ­£æ–‡ CLS continued</h4><p>CLS å¯ç”¨ GAP global average pooling æ›¿æ¢<br>1d position embedding å¯ç”¨ 2d or relative æ›¿æ¢</p><p>ViT å¯¹é½ æ ‡å‡†çš„ transformerï¼Œé€‰ç”¨ NLP é‡Œå¸¸ç”¨çš„ CLS å’Œ 1d position embedding</p><p>Appendix: Transformer multi-head è§£é‡Šï¼Œi.e., å·ç§¯è§£é‡Š in CNN papers</p><p><strong>å…¬å¼æ€»ç»“ ViT çš„å‰å‘ä¼ æ’­è¿‡ç¨‹</strong></p><img src="/91bcdcae/14.webp"><p><strong>Inductive bias</strong></p><p>CNN çš„ inductive bias: locality å±€éƒ¨æ€§, translation equivalence å¹³ç§»ç­‰å˜æ€§ã€‚åœ¨ CNN æ¨¡å‹æ¯ä¸€å±‚éƒ½æœ‰æ‰€ä½“ç°ï¼Œ&#x3D;&#x3D;ã€‹æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä»å¤´åˆ°å°¾ï¼Œè´¯ç©¿æ•´ä¸ªæ¨¡å‹ã€‚</p><p><strong>ViT æ¯” CNN çš„ inductive bias å°‘, only MLP</strong><br>In ViT, only MLP layers are local and translationally equivariant, <strong>while the self-attention layers are global.</strong></p><p>ViT çš„ inductive bias in imagesï¼š<br>å›¾ç‰‡ åˆ‡æˆ patchesï¼›+ position embeddingï¼ˆéšæœºåˆå§‹åŒ–ï¼Œæ²¡æœ‰æºå¸¦ 2d ä½ç½®ä¿¡æ¯ï¼‰</p><p>ViT çš„ patches å—çš„ 2d ä½ç½®ä¿¡æ¯ + spatial relations å›¾åƒå—ä¹‹é—´çš„åœºæ™¯ä¿¡æ¯ï¼Œéƒ½éœ€è¦é‡æ–°å­¦ã€‚ &#x3D;&#x3D;ã€‹ <strong>ViT æ²¡æœ‰å¾ˆå¤š inductive bias</strong> &#x3D;&#x3D;ã€‹ä¸­å°å‹æ•°æ®é›†è®­ç»ƒ ViT æ•ˆæœä¸å¦‚ CNN</p><p><strong>Hybrid architecture</strong><br>Transformer: å…¨å±€å»ºæ¨¡èƒ½åŠ›å¼º<br>CNN: data-efficient ä¸ç”¨é‚£ä¹ˆå¤šè®­ç»ƒæ•°æ®</p><p>å‰ CNN + å Transformer â€“&gt; Hybrid archtecture<br><strong>ä¸åŒçš„å›¾ç‰‡é¢„å¤„ç†æ–¹å¼ï¼š</strong>ä¸åˆ’åˆ† patchesï¼Œé‡‡ç”¨ CNN (Res50 çš„ feature map 14 * 14 &#x3D; 196)ï¼Œè¿‡å…¨è¿æ¥å±‚ <strong>E</strong> Linear projections å¾—åˆ°å›¾ç‰‡çš„ embedding</p><img src="/91bcdcae/15.webp"><p><strong>ViT çš„å›¾ç‰‡é¢„å¤„ç†æ–¹å¼ï¼š</strong><br>æŠŠä¸€å¼ å›¾åˆ’åˆ†æˆ patchesï¼Œç›´æ¥è¿‡å…¨è¿æ¥å±‚ fc</p><hr><h4 id="Fine-tuning-and-higher-resolution"><a href="#Fine-tuning-and-higher-resolution" class="headerlink" title="Fine-tuning and higher resolution"></a>Fine-tuning and higher resolution</h4><p>å¾®è°ƒæ—¶ç”¨å¤§å›¾ç‰‡å°ºå¯¸ i.e., 256 * 256ï¼Œ 320 * 320 è€Œä¸æ˜¯ 224 * 224ï¼Œæ•ˆæœæ›´å¥½</p><p><strong>Q: é¢„è®­ç»ƒå¥½çš„ ViT å¯ä»¥åœ¨æ›´å¤§å°ºå¯¸çš„å›¾ç‰‡ä¸Šä¸ºæ¡ç ï¼Ÿ</strong><br>if patch size ä¸å˜ 16 * 16ï¼Œæ›´å¤§å°ºå¯¸çš„å›¾ç‰‡ â€“&gt; åºåˆ—é•¿åº¦çš„å¢åŠ  i.e., 14 * 14 â€“&gt; 20 * 20 in 320 * 320 image</p><p>Transformer ç†è®ºä¸Šï¼Œå¯ä»¥å¤„ç†ä»»æ„é•¿åº¦ã€‚<br><strong>Butï¼Œæå‰è®­ç»ƒå¥½çš„ position embedding å¯èƒ½å¤±æ•ˆ</strong></p><p>1 - 9 çš„ä¹å®«æ ¼ å›¾ç‰‡ patches ä½ç½®ç¼–ç  â€“&gt; patches å¢å¤šï¼Œ1 - 25 ä½ç½®ç¼–ç </p><p><strong>Q: patches æ•°å¢å¤šï¼Œå¦‚ä½•ä½¿ç”¨ å·²é¢„è®­ç»ƒå¥½çš„ ä½ç½®ç¼–ç å‘¢ï¼Ÿ</strong><br>2d æ’å€¼ï¼Œtorch çš„ interpolate å‡½æ•°å®ç°ï¼›ä½†ä¹Ÿä¸æ˜¯ä»»æ„é•¿åº¦å¢åŠ éƒ½èƒ½ä¿æŒæ•ˆæœã€‚<br>256 â€“&gt; 512 â€“&gt; 768 é•¿åº¦çš„å¢åŠ ï¼Œç›´æ¥ä½¿ç”¨å·®å€¼ï¼Œæœ€åæ•ˆæœæ‰ç‚¹ã€‚ï¼ˆé‡‡æ ·å®šç†ï¼‰</p><p>æ’å€¼ interpolate ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ŒViT å¾®è°ƒæ—¶çš„ä¸€ä¸ªå±€é™ã€‚</p><p>ViT ç”¨äº†å›¾ç‰‡ 2d ç»“æ„ çš„ inductive bias åœ°æ–¹ï¼šresolution adjustment å°ºå¯¸æ”¹å˜ å’Œ patch extraction æŠ½ patches</p><hr><h3 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h3><p>å¯¹æ¯” ResNet, ViT, Hybrid ViT (CNN ç‰¹å¾å›¾ï¼Œä¸æ˜¯å›¾ç‰‡ç›´æ¥ patch åŒ–) çš„ representation learning capabilities è¡¨å¾å­¦ä¹ èƒ½åŠ›ã€‚</p><p>ä¸ºäº†äº†è§£æ¯ä¸ªæ¨¡å‹é¢„è®­ç»ƒå¥½ åˆ°åº•éœ€è¦å¤šå°‘æ•°æ®ï¼Œåœ¨ä¸åŒå¤§å°çš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨å¾ˆå¤š benchmark tasks åšæµ‹è¯•ã€‚</p><p>è€ƒè™‘æ¨¡å‹é¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬æ—¶ï¼ŒViT performs very favourably è¡¨ç°å¾ˆå¥½ï¼Œ SOTA + fewer resource è®­ç»ƒæ—¶é—´æ›´å°‘</p><p>ViT çš„è‡ªç›‘ç£è®­ç»ƒï¼Œå¯è¡Œï¼Œæ•ˆæœä¹Ÿè¿˜ä¸é”™ï¼Œæœ‰æ½œåŠ›ï¼›ä¸€å¹´ä¹‹åï¼ŒMAE ç”¨è‡ªç›‘ç£è®­ç»ƒ ViT æ•ˆæœå¾ˆå¥½ã€‚</p><p><strong>4.1 Setup</strong><br><strong>datasets:</strong><br>ImageNet-1K: 1000 classes, 1.3M images<br>ImageNet-21K: 21000 classes, 14M images<br>JFG-300: 303M images Google ä¸å¼€æº</p><p>ä¸‹æ¸¸ä»»åŠ¡ï¼šåˆ†ç±» CFIAR etc.</p><p><strong>Model variants</strong></p><p>ViT Base, Large, Huge<br>Layers, Hidden size D, MLP size, Heads ç›¸åº”å¢åŠ </p><p>æ¨¡å‹å˜ä½“ &#x3D; (Base, Large, Hugh) + (patch size è¡¨ç¤º)<br>ViT-L&#x2F;16 ä½¿ç”¨ Large å‚æ•° å’Œ patch 16 * 16 è¾“å…¥</p><p>Q: Why patch size in name of model variants?<br>ViT æ¨¡å‹çš„ patch size å˜åŒ–æ—¶, i.e., 16 * 16 â€“&gt; 32 * 32 or 8 * 8, æ¨¡å‹çš„ä½ç½®ç¼–ç ä¼šå˜åŒ–</p><ul><li>transformer è¾“å…¥çš„åºåˆ—é•¿åº¦ ä¸ patch size æˆåæ¯”</li><li>patch size è¶Šå°ï¼Œä¸€å¼ å›¾ç‰‡çš„ patches æ•°è¶Šå¤šï¼Œè®­ç»ƒè¶Šè´µ because of åºåˆ—é•¿åº¦çš„å¢åŠ </li></ul><p><strong>ç»“æœ</strong></p><p>ViT-H&#x2F;4 ç§€è‚Œè‚‰ åˆ·æ¦œ</p><img src="/91bcdcae/16.webp"><p>å’Œ CNN çš„å·¥ä½œ BiT-L, Noisy Student åšå¯¹æ¯”<br><strong>BiT-L:</strong> CNNæ¯”è¾ƒå¤§çš„æ¨¡å‹ï¼ŒViTè®ºæ–‡ä½œè€…å›¢é˜Ÿè‡ªå·±çš„å·¥ä½œ<br><strong>Noisy Student:</strong> ImageNet å½“æ—¶è¡¨ç°æœ€å¥½çš„æ–¹æ³•ã€‚ç”¨ ä¼ªæ ‡ç­¾ pseudo-label å» self-training</p><p>ViT-H&#x2F;14 è®­ç»ƒæ¯” ViT-H&#x2F;16 è´µï¼Œæ•ˆæœå’Œ BiT-L å·®ä¸å¤šï¼Œä¼˜åŠ¿ä¸æ˜æ˜¾ã€‚æ€ä¹ˆçªå‡º ViT çš„å¥½å‘¢ï¼Ÿ</p><p>ViT è®­ç»ƒæ›´ä¾¿å®œã€‚TPUv3 å¤©æ•°ï¼šViT-H&#x2F;14 2.5K, BiT-L 9.9K, Noisy Student 12.3K</p><p>ViT ä¼˜ç‚¹ï¼šæ•ˆæœå¥½ + è®­ç»ƒå¿«</p><p><strong>ç»“æœåˆ†æ</strong></p><p>Vision Transformer åˆ°åº•éœ€è¦å¤šå°‘æ•°æ®æ‰èƒ½è®­ç»ƒå¥½ï¼Ÿ<br>å›¾3 ç°è‰²åŒºåŸŸ ResNet çš„æ•ˆæœï¼Œåœ†åœˆ ViT çš„æ•ˆæœ</p><p><strong>Take home message: å›¾ 3</strong></p><p>å¦‚æœæƒ³ç”¨ ViTï¼Œè‡³å°‘éœ€è¦ ImageNet-21K 14M å¤§å°çš„æ•°æ®é›†</p><ul><li>å°äºæ•´ä¸ªæ•°æ®é‡ï¼ŒCNN æ›´åˆé€‚ï¼Œæ›´å¥½çš„åˆ©ç”¨ inductive biasï¼ŒViT æ²¡æœ‰ç‰¹åˆ«å¤š inductive bias éœ€è¦æ›´å¤šæ•°æ®è®­ç»ƒã€‚</li></ul><p>æ•°æ®é›†è§„æ¨¡æ¯” ImageNet-21K æ›´å¤§æ—¶ï¼ŒVision Transformer æ•ˆæœæ›´å¥½ï¼Œå› ä¸ºå¯æ‰©å±•æ€§ scaling æ›´å¥½ã€‚</p><img src="/91bcdcae/17.webp"><p><strong>å›¾ 4 Linear few-shot evaluation</strong></p><p>å›¾ 3 ViT å’Œ ResNet æ¯”ï¼ŒåŠ äº†å¼ºçº¦æŸï¼šdropoutã€weight decayã€label smoothingï¼Œçº¦æŸäº† ViT çš„å‘æŒ¥</p><p>linear evalution: æŠŠ ViT é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ ç›´æ¥ä½œä¸º ç‰¹å¾æå–å™¨ï¼Œä¸ fine-tuneï¼Œ+ ä¸€ä¸ª logistic regression å¾—åˆ°åˆ†ç±»ç»“æœã€‚</p><p>Few-shotï¼š5-shotï¼Œåœ¨ ImageNet åš linear evaluation æ—¶ï¼Œæ¯ç±»å›¾ç‰‡éšæœºé€‰å– 5 ä¸ª samplesï¼Œevaluation å¾ˆå¿«ï¼Œåš æ¶ˆèå®éªŒã€‚</p><p>linear few-shot evaluation é‡‡ç”¨ JFT æ•°æ®é›† 10M, 30M, 100M, 300Mã€‚æ¥è‡ªåŒä¸€ä¸ªæ•°æ®é›†ï¼Œæ•°æ®æ²¡æœ‰ distribution gapï¼Œæ¨¡å‹çš„æ•ˆæœæ›´èƒ½ä½“ç° Vision Transformer æœ¬èº«ç‰¹è´¨ã€‚</p><p>ViT å›¾4 æ•ˆæœ å’Œ å›¾3 å·®ä¸å¤šã€‚<strong>å¦‚ä½•ç”¨ ViT åšå°æ ·æœ¬å­¦ä¹ ï¼Œæœªæ¥ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚</strong></p><p><strong>å›¾ 5 ç”¨ ViT æ¯” CNNs ä¾¿å®œ çš„å®éªŒæ”¯æŒ</strong><br>å¤§å®¶çš„å°è±¡ï¼šTransformer åˆå¤§åˆè´µï¼Œå¾ˆéš¾è®­ç»ƒ</p><img src="/91bcdcae/18.webp"><p>average-5ï¼šImageNet-real, Pets, Flower, CIFAR10, CIFAR100 å¹³å‡<br>ImageNet å•ç‹¬çš„å¯¹æ¯”</p><p>åŒç­‰è®¡ç®—å¤æ‚åº¦ï¼šViT æ¯” ResNet æ•ˆæœå¥½ï¼Œå°è¯äº† ViT è®­ç»ƒæ›´ä¾¿å®œ</p><p><strong>Q: Hybrid æ¨¡å‹ï¼ŒCNN æŠ½å–å‡ºæ¥çš„ç‰¹å¾ï¼Œèƒ½ä¸èƒ½å¸®åŠ© Transformer æ›´å¥½çš„å­¦ä¹ å‘¢ï¼Ÿ</strong></p><ul><li>å°æ¨¡å‹ï¼ŒHybrid æ¨¡å‹å¸æ”¶ CNN å’Œ Transformer çš„ä¼˜ç‚¹ï¼Œæ•ˆæœå¥½ã€‚ä¸éœ€è¦å¾ˆå¤šçš„æ•°æ®é¢„è®­ç»ƒï¼Œè¾¾åˆ° Transformer çš„æ•ˆæœ</li><li>å¤§æ¨¡å‹ï¼ŒHybrid æ¨¡å‹ å’Œ Transformer å·®ä¸å¤šï¼Œç”šè‡³ä¸å¦‚ Transformer æ¨¡å‹ã€‚<strong>Whyï¼Ÿ</strong><br><strong>å¦‚ä½• é¢„å¤„ç†å›¾åƒï¼Œå¦‚ä½•åš tokenization å¾ˆé‡è¦</strong>ï¼Œåç»­è®ºæ–‡æœ‰ç ”ç©¶</li></ul><p>æ•´ä½“è¶‹åŠ¿ï¼šæ¨¡å‹å¢åŠ ï¼Œé™¤äº† Hybrid æ¨¡å‹æœ‰ç‚¹é¥±å’Œï¼ˆé¥±å’Œï¼šå¢åŠ åˆ°ä¸€ä¸ªå¹³å°å€¼åï¼Œä¸å¢åŠ äº†ï¼‰ã€‚ResNet å’Œ Transformer éƒ½æ²¡æœ‰é¥±å’Œã€‚</p><p><strong>4.5 Inspecting Vision Transformer</strong></p><p>å¯è§†åŒ–åˆ†æ ViT å†…éƒ¨è¡¨å¾ internal representations: <strong>Patch embedding, position embedding</strong><br><strong>ViT ç¬¬ä¸€å±‚ Linear projection E å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ</strong></p><p>Figure 7 (left) embed RGB value å‰ 28 ä¸ªä¸»æˆåˆ†</p><img src="/91bcdcae/19.webp"><p>Vision Transformer å’Œ CNN å­¦åˆ°çš„å¾ˆåƒï¼Œç±»ä¼¼ gabor filter æœ‰é¢œè‰²ã€çº¹ç†ï¼Œ å¯ä»¥åš plausible basis functionsï¼Œå¯ä»¥æè¿°æ¯ä¸ªå›¾åƒå—çš„åº•å±‚ä¿¡æ¯ a low-dimensional representation of the fine structure within each patch.</p><p><strong>Position embedding èƒ½å­¦åˆ°ä¸€äº›è¡¨ç¤ºä½ç½®è·ç¦»çš„ä¿¡æ¯</strong></p><ul><li>patch è‡ªå·±æœ¬èº« ç›¸ä¼¼åº¦é«˜ é»„è‰² 1</li><li>å­¦åˆ°äº†è·ç¦»çš„æ¦‚å¿µ<ul><li>(4, 4) é»„è‰²ä¸­å¿ƒç‚¹ï¼Œè¶Šè¾¹ç¼˜ï¼Œç›¸ä¼¼åº¦è¶Šä½ï¼Œé¢œè‰²è¶Šè“</li></ul></li><li>å­¦åˆ°äº† è¡Œ å’Œ åˆ— çš„è·ç¦»è§„åˆ™<ul><li>åŒè¡ŒåŒåˆ—ï¼Œé¢œè‰²æ¡ çš„è¡¨ç¤º<br>è™½ç„¶æ˜¯ 1d çš„ position embeddingï¼Œä½†å·²ç»å­¦åˆ°äº† 2d çš„å›¾åƒä½ç½®æ¦‚å¿µï¼›æ‰€ä»¥æ¢æˆ 2d position æå‡ä¸å¤šã€‚</li></ul></li></ul><img src="/91bcdcae/20.webp"><p><strong>Self-attention æœ‰æ²¡æœ‰èµ·ä½œç”¨ï¼Ÿ</strong></p><p>ç”¨ Transformer çš„åŸå› ï¼šè‡ªæ³¨æ„åŠ› èƒ½æ¨¡æ‹Ÿé•¿è·ç¦»çš„å…³ç³»ã€‚</p><ul><li>NLP ä¸€ä¸ªå¾ˆé•¿çš„å¥å­é‡Œï¼Œå¼€å¤´çš„ä¸€ä¸ªè¯å’Œç»“å°¾çš„ä¸€ä¸ªè¯ å¯èƒ½äº’ç›¸æœ‰å…³è”ã€‚</li><li>CV é‡Œ å¾ˆè¿œçš„ä¸¤ä¸ªåƒç´ ç‚¹ä¹‹é—´ ä¹Ÿèƒ½åšè‡ªæ³¨æ„åŠ›ã€‚</li></ul><p><strong>ViT çš„ self-attention æ˜¯ä¸æ˜¯ å¾ˆè¿œçš„åƒç´ ç‚¹ä¹Ÿèƒ½æœ‰äº¤äº’ï¼Ÿ</strong><br>ViT-L&#x2F;16 æœ‰ 24 å±‚ï¼ˆæ¨ªåæ ‡å€¼ï¼‰ï¼Œäº”é¢œå…­è‰²çš„ç‚¹ï¼štransformer æ¯å±‚ multi-head çš„headsï¼ŒViT-L 16 heads, æ¯ä¸€åˆ—æœ‰ 16 ä¸ªç‚¹</p><img src="/91bcdcae/21.webp"><p>çºµè½´æ˜¯ mean attention distance<br>d_ab &#x3D; l_ab * A_ab &#x3D; ab ä¸¤ç‚¹ pixel ä¹‹é—´çš„è·ç¦»å·® * ab ä¸¤ç‚¹ä¹‹é—´çš„attention weights<br>d_ab çš„å¤§å°ï¼Œåæ˜ æ¨¡å‹èƒ½ä¸èƒ½æ³¨æ„åˆ°å¾ˆè¿œçš„ 2 ä¸ª pixels</p><ul><li>self-attention åˆšå¼€å§‹èƒ½æ³¨æ„åˆ° 10 - 110 pixels</li><li>self-attention åˆšå¼€å§‹å°±æ³¨æ„åˆ°å…¨å±€çš„ä¿¡æ¯ï¼›CNN åˆšå¼€å§‹ç¬¬ä¸€å±‚çš„æ„Ÿå—é‡ receptive filed å¾ˆå°ï¼Œåªèƒ½çœ‹åˆ°é™„è¿‘çš„ pixel</li></ul><img src="/91bcdcae/22.webp"><p>ç½‘ç»œåŠ æ·±ï¼Œæ¨¡å‹å­¦åˆ°çš„ç‰¹å¾è¶Šæ¥è¶Š high levelï¼Œè¶Šæ¥è¶Šæœ‰è¯­ä¹‰ä¿¡æ¯ï¼Œåƒç´ çš„è‡ªæ³¨æ„åŠ›è·ç¦» è¶Šæ¥è¶Šè¿œï¼Œä¸æ˜¯é é‚»è¿‘çš„åƒç´ ç‚¹åšåˆ¤æ–­ã€‚</p><p><strong>è¯æ˜ è‡ªæ³¨æ„åŠ› æœ‰å­¦åˆ° å¾ˆè¿œè·ç¦»çš„ pixel ä¿¡æ¯ï¼Œè¯æ˜ by å›¾6</strong></p><p>ViT æœ€åä¸€å±‚ output çš„ token çš„ self-attention æŠ˜å°„ï¼ˆé€†å‘æ˜ å°„ï¼‰å› åŸæ¥çš„è¾“å…¥å›¾ç‰‡ã€‚ViT çœŸçš„å­¦åˆ°äº†ä¸€äº›æ¦‚å¿µï¼šç‹—ã€é£æœº</p><img src="/91bcdcae/23.webp"><p>Globally å…¨å±€æ¥è¯´ï¼Œè¾“å‡ºçš„ token æ˜¯èåˆå…¨å±€çš„ç‰¹å¾ä¿¡æ¯ï¼ŒViT æ¨¡å‹å¯ä»¥å…³æ³¨åˆ° å’Œ classfication åˆ†ç±»ç›¸å…³çš„å›¾åƒåŒºåŸŸã€‚</p><p><strong>4.6 self-supervision</strong></p><p>å¦‚ä½•ç”¨ è‡ªç›‘ç£ çš„æ–¹å¼ è®­ç»ƒä¸€ä¸ª vision transformerï¼Ÿ</p><p>å¾ˆé‡è¦ï¼Œ22é¡µå…¨æ–‡ï¼Œåˆ«çš„ç»“æœéƒ½åœ¨ appendixï¼Œè‡ªç›‘ç£çš„ç»“æœåœ¨æ­£æ–‡ã€‚</p><p>å› ä¸º NLP çš„ transformer éƒ½æ˜¯ç”¨ large scale self-supervised pre-training <strong>å¤§è§„æ¨¡ã€è‡ªç›‘ç£</strong> çš„æ–¹å¼é¢„è®­ç»ƒçš„ã€‚</p><p><strong>NLP çš„ è‡ªç›‘ç£ï¼š</strong>BERT å®Œå½¢å¡«ç©º Mask language modelï¼ŒGPT ç”Ÿæˆï¼Œ<strong>é¢„æµ‹ä¸‹ä¸€ä¸ªè¯</strong> by language model</p><p>ViT å€Ÿé‰´ BERTï¼Œåˆ›å»ºä¸€ä¸ªä¸“å±äº vision çš„ç›®æ ‡å‡½æ•°ï¼Œmasked patch predictionã€‚ä¸€å¼ å›¾ç‰‡çš„æŸäº› patches éšæœºæŠ¹æ‰ï¼ŒViT é‡å»ºç¼ºå¤±çš„patches</p><p>Noteï¼šä» æ¨¡å‹ã€ç›®æ ‡å‡½æ•°ä¸Šï¼ŒCV å’Œ NLP çš„å¤§ä¸€ç»Ÿã€‚</p><p>ä½†æ˜¯ï¼ŒViT-B&#x2F;16 with masked patch prediction åœ¨ ImageNet <del>80% å‡†ç¡®ç‡ã€‚</del>80% æ¯” ä»å¤´è®­ç»ƒ ViT å¥½ 2%ï¼Œæ¯” supervised pre-training ä½ 4%ã€‚</p><p><strong>ViT å’Œ contrastive pre-training çš„ç»“åˆï¼š future work</strong> i.e., MOCOv3, DINO</p><p><strong>contrastive learning:</strong> 2020 å¹´ CV æœ€ç«çš„ topicï¼Œæ˜¯æ‰€æœ‰ è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¡¨ç°æœ€å¥½çš„ã€‚</p><hr><h3 id="è¯„è®º"><a href="#è¯„è®º" class="headerlink" title="è¯„è®º"></a>è¯„è®º</h3><p>å†™ä½œï¼šç®€æ´æ˜äº†ã€æœ‰è½»æœ‰é‡ï¼ˆé‡è¦ç»“æœæ”¾æ­£æ–‡ï¼‰ï¼Œå›¾è¡¨æ¸…æ™°ã€‚</p><p>å†…å®¹ï¼šVision Transformer æŒ–äº†ä¸€ä¸ªå¤§å‘ï¼šå„ä¸ªè§’åº¦çš„åˆ†æï¼Œæå‡ or æ¨å¹¿</p><p>task ä»»åŠ¡è§’åº¦ï¼šViT åªåšäº†åˆ†ç±»ï¼Œæ£€æµ‹ã€åˆ†å‰²ã€å…¶å®ƒé¢†åŸŸçš„ä»»åŠ¡ future work</p><p>ViT ç»“æ„çš„è§’åº¦ï¼š</p><ul><li>æ”¹åˆšå¼€å§‹çš„ tokenization</li><li>æ”¹ transformer block, i.e., self-attention æ¢æˆ MLP works<ul><li>MetaFormer è®¤ä¸º transformer work çš„åŸå› æ˜¯ transformer çš„æ¶æ„ï¼Œä¸æ˜¯ transformer æŸäº›ç‰¹æ®Šçš„ç®—å­</li><li>MetaFormerï¼Œself-attention æ”¹æˆ ï¼ˆä¸èƒ½å­¦ä¹ çš„ï¼‰pooling æ± åŒ–æ“ä½œï¼›ç”šè‡³æ”¹æˆ Identityï¼Œä¸ç”¨æ³¨æ„åŠ›</li></ul></li><li>æ”¹ ç›®æ ‡å‡½æ•°ï¼šæœ‰ç›‘ç£ã€or ä¸åŒçš„è‡ªç›‘ç£è®­ç»ƒæ–¹å¼</li></ul><p>ViT çš„å¤§å‘ï¼š</p><ul><li>æ‰“é€šäº† CV å’Œ LP ä¹‹é—´çš„é¸¿æ²Ÿ</li><li>æŒ–äº†ä¸€ä¸ªæ›´å¤§çš„<strong>å¤šæ¨¡æ€</strong>çš„å‘<ul><li>è§†é¢‘ã€éŸ³é¢‘ã€åŸºäº touch çš„ä¿¡å·</li><li>å„ç§ modality çš„ä¿¡å·éƒ½å¯ä»¥æ‹¿æ¥ç”¨</li></ul></li></ul><p><strong>CNN, self-attention, MLP é¹¿æ­»è°æ‰‹ï¼Ÿ</strong><br>çŠ¹æœªå¯çŸ¥ï¼ŒæœŸå¾…ä¸‹ä¸€ä¸ªæ”¹è¿›çš„ vision transformer</p><ul><li>ä¸€ä¸ªç®€æ´ã€é«˜æ•ˆã€é€šç”¨çš„è§†è§‰éª¨å¹²ç½‘ç»œ CV backboneï¼Œç”šè‡³å®Œå…¨ä¸ç”¨ä»»ä½•æ ‡æ³¨ä¿¡æ¯</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/a4d89ff3.html" rel="prev" title="æœ€æ–°CNNåè¶…Transfromerä¹‹ä½œï¼šConvNeXt"><i class="fa fa-chevron-left"></i> æœ€æ–°CNNåè¶…Transfromerä¹‹ä½œï¼šConvNeXt</a></div><div class="post-nav-item"><a href="/78187af6.html" rel="next" title="CS231n: Convolutional Neural Networks for Visual Recognition [2019ä¸­æ–‡] - Lecture 1 Introduction">CS231n: Convolutional Neural Networks for Visual Recognition [2019ä¸­æ–‡] - Lecture 1 Introduction <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">æ–‡ç« ç›®å½•</li><li class="sidebar-nav-overview">ç«™ç‚¹æ¦‚è§ˆ</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">åŸæ–‡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9F%E8%A7%88"><span class="nav-number">2.</span> <span class="nav-text">é€Ÿè§ˆ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3"><span class="nav-number">2.1.</span> <span class="nav-text">æ€æƒ³</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">æ¨¡å‹ç»“æ„</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%BD%AC%E5%BA%8F%E5%88%97"><span class="nav-number">2.2.1.</span> <span class="nav-text">å›¾åƒè½¬åºåˆ—</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Position-embeddings"><span class="nav-number">2.2.2.</span> <span class="nav-text">Position embeddings</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learnable-embedding"><span class="nav-number">2.2.3.</span> <span class="nav-text">learnable embedding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5transformer-encoder"><span class="nav-number">2.2.4.</span> <span class="nav-text">è¾“å…¥transformer encoder</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">2.3.</span> <span class="nav-text">å®éªŒç»“æœ</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">æ ‡é¢˜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">4.</span> <span class="nav-text">æ‘˜è¦</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">5.</span> <span class="nav-text">å¼•è¨€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">6.</span> <span class="nav-text">ç»“è®º</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">7.</span> <span class="nav-text">ç›¸å…³å·¥ä½œ</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ViT%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.</span> <span class="nav-text">ViTæ¨¡å‹</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vision-Transformer-%E6%A8%A1%E5%9E%8B%E5%9B%BE"><span class="nav-number">8.1.</span> <span class="nav-text">Vision Transformer æ¨¡å‹å›¾</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vision-Transformer%E6%AD%A3%E6%96%87"><span class="nav-number">8.2.</span> <span class="nav-text">Vision Transformeræ­£æ–‡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT-%E6%AD%A3%E6%96%87-CLS-continued"><span class="nav-number">8.3.</span> <span class="nav-text">ViT æ­£æ–‡ CLS continued</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-tuning-and-higher-resolution"><span class="nav-number">8.4.</span> <span class="nav-text">Fine-tuning and higher resolution</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">9.</span> <span class="nav-text">å®éªŒ</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E8%AE%BA"><span class="nav-number">10.</span> <span class="nav-text">è¯„è®º</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">é€¢å‚è‘µçš„ä¸ªäººåšå®¢</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1011</span> <span class="site-state-item-name">æ—¥å¿—</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">åˆ†ç±»</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub ğŸ‘¨â€ğŸ’» â†’ https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub ğŸ‘¨â€ğŸ’»</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub ğŸ‘©â€ğŸ’» â†’ https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub ğŸ‘©â€ğŸ’»</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail ğŸ“§ â†’ mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail ğŸ“§</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail ğŸ« â†’ mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail ğŸ«</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili ğŸ“º â†’ https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili ğŸ“º</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili ğŸ® â†’ https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili ğŸ®</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube ğŸ“º â†’ https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube ğŸ“º</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">ç«™ç‚¹æ€»å­—æ•°ï¼š</span> <span title="ç«™ç‚¹æ€»å­—æ•°">3.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">ç«™ç‚¹é˜…è¯»æ—¶é•¿ &asymp;</span> <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">148:14</span></div><div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> å¼ºåŠ›é©±åŠ¨</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"è¿™é‡Œå¯ä»¥å‘é€è¯„è®º~ï¼ˆä¸Šé¢å¯ä»¥è¾“å…¥æ˜µç§°ã€é‚®ç®±ï¼‰",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>