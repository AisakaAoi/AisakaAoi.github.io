<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇学习报告基于2020 KDD（Knowledge Discovery and Data Mining， CCF A类会议）的论文：《LayoutLM: Pre-training of Text and Layout for Document Image Understanding》，该论文由微软自然语言计算组提出。"><meta property="og:type" content="article"><meta property="og:title" content="学习报告：融合文本、布局和图像信息的文档理解预训练模型"><meta property="og:url" content="https://aisakaaoi.github.io/b820e5ed.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="本篇学习报告基于2020 KDD（Knowledge Discovery and Data Mining， CCF A类会议）的论文：《LayoutLM: Pre-training of Text and Layout for Document Image Understanding》，该论文由微软自然语言计算组提出。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/b820e5ed/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/b820e5ed/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/b820e5ed/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/b820e5ed/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/b820e5ed/5.webp"><meta property="article:published_time" content="2021-07-20T03:02:03.000Z"><meta property="article:modified_time" content="2023-11-23T03:12:06.809Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/b820e5ed/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/b820e5ed.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>学习报告：融合文本、布局和图像信息的文档理解预训练模型 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">50</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">864</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/b820e5ed.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">学习报告：融合文本、布局和图像信息的文档理解预训练模型</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-07-20 11:02:03" itemprop="dateCreated datePublished" datetime="2021-07-20T11:02:03+08:00">2021-07-20</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">⭐脑机接口与混合智能研究团队（BCI团队）</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%B8%8E%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E5%9B%A2%E9%98%9F%EF%BC%88BCI%E5%9B%A2%E9%98%9F%EF%BC%89/%F0%9F%92%AB%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">💫学习报告</span></a> </span></span><span id="/b820e5ed.html" class="post-meta-item leancloud_visitors" data-flag-title="学习报告：融合文本、布局和图像信息的文档理解预训练模型" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/b820e5ed.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/b820e5ed.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>7.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>18 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇学习报告基于2020 KDD（Knowledge Discovery and Data Mining， CCF A类会议）的论文：《LayoutLM: Pre-training of Text and Layout for Document Image Understanding》，该论文由微软自然语言计算组提出。</p><span id="more"></span><hr><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p><strong>智能文档（Document AI &#x2F; Document Intelligence）</strong>是一个近年来相对较新的研究主题，涉及自动阅读、理解和分析业务文档的技术。 在商业场景中，文件对公司的效率和生产力至关重要，它们是提供与公司内部和外部交易相关的详细信息的文件。这些文档可能是自动生成的，以电子文件的形式出现，或者来自书面或打印在纸上的扫描形式。商业文档的类型多种多样，常见示例包括采购订单、财务报告、商业电子邮件、销售协议、供应商合同、信件、发票、收据、简历等等（部分样例如图1）。商业文档的共同点在于它们都以自然语言的形式呈现，并且可以通过纯文本、多列布局和各种表格&#x2F;表格&#x2F;图形等多种方式进行组织。<strong>然而，由于布局和格式的多样性、文档图像质量的参差不齐以及模板结构的复杂性，理解商业文档直到现在仍然是一项非常具有挑战性的任务。</strong></p><img src="/b820e5ed/1.webp"><div align="center">图1 商业文档部分示例</div><p>目前，许多公司仍然是通过手动设计特定规则的方式从文档中提取数据，这种方法既耗时又昂贵。并且随着文档格式的更新，基于特定规则建模的方法效果会大大下降甚至不起作用。 为了解决这些问题，智能文档模型和算法旨在从商业文档中自动分类、提取和结构化信息，加速自动化文档处理工作流程。从计算机视觉或自然语言处理的角度来看，当前主流的智能文档方法要么基于单一模态深度学习网络模型，或者进行多模态的简单组合。尽管已经有一些模型在智能文档领域取得了重大进展，但这些方法中的大多数都面临两个局限性：<strong>（1）它们依赖于少数人工标记的训练样本，而没有充分探索使用大规模未标记训练的可能性。 (2) 他们通常利用预训练的 CV 模型或 NLP 模型，但不考虑文本和布局信息的联合训练。</strong>因此，如何在智能文档领域探究文本和布局的自监督预训练具有极高的研究和应用价值。</p><p>针对智能文档现阶段存在的一些问题，微软团队提出了 LayoutLM [1]，这是一种简单而有效的文本和布局预训练方法，用于文档图像理解任务。 受 BERT 模型 [2] 融合文本嵌入（text embeddings）和位置嵌入（position embeddings）作为输入表示的启发，LayoutLM 新增了两种类型的输入嵌入：<strong>（1）二维位置嵌入（2D position embedding）</strong>，表示 token（此任务中是指分词）在文档中的相对位置； (2) token 在文档中的<strong>图像嵌入（image embedding）</strong>。 LayoutLM 的架构如图2所示。 添加这两个输入嵌入是因为二维位置嵌入可以捕获文档中 token 之间的关系，同时图像嵌入可以捕获一些外观特征，例如字体方向、类型和颜色。 此外，针对LayoutLM 特点，该论文采用了多任务学习目标，包括遮罩式视觉语言模型 (Masked Visual-Language Model，MVLM) 损失和多标签文档分类 (Multi-label Document Classification，MDC) 损失，进一步加强了文本和布局的联合预训练。</p><img src="/b820e5ed/2.webp"><div align="center">图2 LayoutLM 结构示意图</div><p>该论文的贡献总结如下：（1）首次在单一框架中对来自扫描文档图像的文本和布局信息进行预训练，同时融合了图像特征最后达到了 state-of-the-art 的结果；（2）LayoutLM 使用遮罩式视觉语言模型和多标签文档分类作为训练目标，在文档图像理解任务中明显优于几个 SOTA 预训练模型；代码和预训练模型可在 <a target="_blank" rel="noopener" href="https://aka.ms/layoutlm">https://aka.ms/layoutlm</a> 上公开获取，用于更多下游任务。</p><hr><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="LayoutLM中的特征信息"><a href="#LayoutLM中的特征信息" class="headerlink" title="LayoutLM中的特征信息"></a>LayoutLM中的特征信息</h4><p>尽管类 BERT 模型在多项具有挑战性的 NLP 任务中达到了最优的结果，但它们通常仅利用输入中的文本信息。 当涉及到蕴含丰富视觉信息的文档数据时，可以将更多信息编码到预训练模型中。因此，该论文利用文档布局中视觉上丰富的信息并将它们与输入文本对齐。进而显著优化富文档中的语言表示：</p><p><strong>文档布局信息（Document Layout Information）。</strong>很明显，文档中单词的相对位置对语义表示有很大贡献。 以表单理解为例，给定表单中的一个关键词（例如“Passport ID:”），其对应的值更有可能在其右侧或下方，而不是左侧或上方。因此，可以将这些相对位置信息嵌入为二维位置表示。 基于 Transformer 中的 self-attention 机制，将二维位置特征嵌入到语言表示中可以更好地将布局信息与语义表示对齐。</p><p><strong>视觉信息（Visual Information）。</strong>与文本信息相比，视觉信息是文档表示中另一个重要的特征。 通常，文档包含一些视觉信号以显示文档段的重要性和优先级。视觉信息可以由图像特征表示并有效地用于文档表示。 对于文档级视觉特征，整幅图像可以指示文档布局，这是文档图像分类的基本特征。 对于词级视觉特征，粗体、下划线和斜体等样式也是序列标注任务的重要提示。 因此，可以认为将图像特征与传统文本表示相结合能为文档带来更丰富的语义表示。</p><hr><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>为了利用现有的预训练模型并适应文档图像理解任务，文中使用 BERT 架构作为主干并添加两个新的输入嵌入：<strong>二维位置嵌入和图像嵌入。</strong></p><p><strong>二维位置嵌入。</strong>根据 OCR 获得的文本 Bounding Box，可以能获取文本在文档中的具体位置。将对应坐标转化为虚拟坐标之后，计算该坐标对应在 x、y、w、h 四个嵌入子层的表示，最终的二维位置嵌入为四个子层的嵌入之和。</p><p><strong>图像嵌入。</strong>文中将每个文本相应的 Bounding Box 当作 Faster R-CNN 中的候选框（Proposal），从而提取对应的局部特征。特殊地，由于 [CLS] 符号用于表示整个输入文本的语义，同样使用整张文档图像作为该位置的图像嵌入，从而保持模态对齐。</p><hr><h4 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h4><p><strong>任务1：遮罩式视觉语言模型。</strong>大量实验已经证明 MVLM 能够在预训练阶段有效地进行自监督学习。该论文在此基础上进行了修改：在遮盖（Mask）当前词之后，保留对应的二维位置嵌入暗示，让模型预测对应的词。在这种方法下，模型根据已有的上下文和对应的视觉暗示预测被遮罩的词，从而让模型更好地学习文本位置和文本语义的模态对齐关系。</p><p><strong>任务2：多标签文档分类。</strong>MVLM 能够有效的表示词级别的信息，但是对于文档级的表示，需要文档级的预训练任务来引入更高层的语义信息。在预训练阶段该论文使用的 IIT-CDIP 数据集[3]为每个文档提供了多标签的文档类型标注，同时引入多标签文档分类任务。该任务使得模型可以利用这些监督信号去聚合相应的文档类别，并捕捉文档类型信息，从而获得更有效的高层语义表示。</p><hr><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="预训练数据集"><a href="#预训练数据集" class="headerlink" title="预训练数据集"></a>预训练数据集</h4><p>预训练模型的性能很大程度上取决于数据集的规模和质量。 因此，该论文采用一个大规模的扫描文档图像数据集来预训练 LayoutLM 模型。模型在 IIT-CDIP 测试集 1.0 上进行了预训练，其中包含超过 600 万个文档，以及超过 1100 万个扫描文档图像。 此外，每个文档都有其对应的文本和元数据存储在 XML 文件中。文本是通过将 OCR 应用于文档图像而产生的内容。元数据描述了文档的属性，例如唯一标识和文档标签。 尽管元数据包含错误和不一致的标签，但这个大规模数据集中的扫描文档图像非常适合对模型进行预训练。</p><hr><h4 id="微调数据集"><a href="#微调数据集" class="headerlink" title="微调数据集"></a>微调数据集</h4><p>该论文选择三个基准数据集作为下游任务来评估预训练的 LayoutLM 模型的性能。 第一个是用于空间布局分析和表单理解的 FUNSD 数据集 [4]。第二个是用于扫描小票&#x2F;收据信息提取的 SROIE 数据集 [5]。 第三个是用于文档图像分类的 RVL-CDIP 数据集 [6]，它由 16 个类别的 400,000 张灰度图像组成。</p><hr><h4 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h4><p>为了利用每个文档的布局信息，需要获取每个token的位置。 但是，预训练数据集（IIT-CDIP 测试集）仅包含纯文本，而缺少相应的边界框。 在这种情况下，需要重新处理扫描的文档图像以获得必要的布局信息。与 IIT-CDIP 测试集中的原始预处理一样，该论文采用的是开源 OCR 引擎 Tesseract6 [7]应用于文档图像来类似地处理数据集，但 不同之处在于可以同时获得识别的token及其在文档图像中的对应位置。OCR 结果通过标准的 hOCR 格式进行存储。</p><hr><h4 id="下游任务微调"><a href="#下游任务微调" class="headerlink" title="下游任务微调"></a>下游任务微调</h4><p>通过千万文档量级的预训练并在下游任务微调，文章在测试的三个不同类型的下游任务中都取得了目前的最佳成绩：在 FUNSD 数据集上将表单理解的 F1 值从70.72提高至79.2；将 ICDAR 2019 票据理解 SROIE 比赛中的第一名成绩94.02提高至95.24；在 RVL-CDIP 文档图像分类数据集上将目前的最好结果93.07提高至94.42。</p><p><strong>表单理解（Form Understanding）。</strong>在表单理解任务上，文章使用 FUNSD 作为测试数据集，该数据集中的199个标注文档包含31,485个词和9,707个语义实体。在该数据集上，需要对数据集中的表单进行键值对（key-value）抽取。通过引入位置信息的预训练，提出的模型在该任务上取得了显著的提升。实验结果表1所示。</p><img src="/b820e5ed/3.webp"><div align="center">表1：LayoutLM 在 FUNSD 数据集上的实验结果</div><p><strong>票据理解（Receipt Understanding）。</strong>在票据理解任务中，文章选择 SROIE 测评比赛作为测试。SROIE 票据理解包含1000张已标注的票据，每张票据标注了店铺名、店铺地址、总价、消费时间四个语义实体。通过在该数据集上微调，模型在 SROIE 测评中，F1 值高出第一名（2019）1.2个百分点，达到95.24%。</p><img src="/b820e5ed/4.webp"><div align="center">表2：LayoutLM 在 SROIE 测评上的实验结果</div><p><strong>文档图像分类（Document Image Classification）。</strong>对于文档图像分类任务，文章选择 RVL-CDIP 数据集进行测试。RVL-CDIP 数据集包含16类总记40万个文档，每一类都包含25,000个文档数据。模型在该数据集上微调之后将分类准确率提高了1.35个百分点，达到了94.42%。</p><img src="/b820e5ed/5.webp"><div align="center">表3：LayoutLM 在 RVL-CDIP 数据集上的实验结果</div><hr><h3 id="总结和展望"><a href="#总结和展望" class="headerlink" title="总结和展望"></a>总结和展望</h3><p>该论文提出了 LayoutLM，这是一种简单而有效的预训练技术，在单个框架中包含文本和布局信息。 LayoutLM 以 Transformer 架构为骨干，利用多模态输入，包括 token 嵌入、布局嵌入和图像嵌入。同时，该模型可以基于大规模未标记的扫描文档图像以自我监督的方式轻松训练。文章在三个任务上评估 LayoutLM 模型：表单理解、收据理解和扫描文档图像分类。 实验表明，LayoutLM 在这些任务中大大优于几个 SOTA 预训练模型。</p><p>对于未来的研究，文章团队将研究具有更多数据和更多计算资源的预训练模型，还将使用带有文本和布局的 LARGE 架构训练 LayoutLM，并在预训练步骤中涉及图像嵌入。 此外，文章团队还会探索新的网络架构和其他自我监督的训练目标，以进一步释放 LayoutLM 的力量。</p><hr><h3 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a>个人思考</h3><ol><li>微软团队的这篇文章主要围绕商业文档的理解开展了相关的研究工作。在实际的需求中，文档的类型更加多种多样，像试卷、扫描证件等也可被认为是一种特殊的文档，因此现实生产环境对于这方面业务的需求特别大。然而，正如该论文提及该领域存在的各种各样难题的存在，目前并没有特别好的方法能够高效地从文档中提取到用户所需要的数据。所以在近几年，国内外的头部公司都在陆续地开展相关的研究以争夺这块大蛋糕。</li><li>从方法的角度来看，该论文的思想其实是非常简洁的，归根到底就是将文本、其在图像中的相对位置以及对应图像做一个硬对齐，再通过“大力出奇迹”的方式实现了SOTA的效果。值得学习的是，论文并未直接暴力的构造网络然后从头训练，而是借鉴了BERT的思路，用自监督的方式在BERT的基础上训练了通用的模型，省了标注和各任务单独训练的时间和资源成本。</li><li>但真实的运行效果并非像论文中说的那么理想。一方面，该论文没有做消融实验，并无法说明其中的二维位置嵌入以及图像嵌入是哪个对最终效果贡献更大。而在文档分类复现过程中，在不引入图像嵌入的基础，二维位置嵌入帮了倒忙，即对于该任务实质上就是文本跟图像的一个融合，训练的模型其实并未很好地理解文档中各个token位置的概念；另一方面，这种强硬的对齐方式，需要做大量的预处理工作，甚至会直接影响预训练以及下游任务的效果，且对于算力的要求极高。</li><li>这篇文章开启了在文档理解含先验知识（位置等）的多模态文本和图像信息的融合，在其后续改进版本中LayoutLM 2.0 [8]、LayoutXLM [9]也证实了这种方式的有效性，这种改进思路在不同领域也值得学习。</li></ol><hr><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.scholat.com/teamwork/showPostMessage.html?id=10218">https://www.scholat.com/teamwork/showPostMessage.html?id=10218</a><br>[1] Xu Y, Li M, Cui L, et al. Layoutlm: Pre-training of text and layout for document image understanding[C]&#x2F;&#x2F;Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 1192-1200.<br>[2] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.<br>[3] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. 2006. Building a Test Collection for Complex Document Information Processing. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Seattle, Washington, USA) (SIGIR ’06). ACM, New York, NY, USA, 665–666.<br>[4] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents. 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW) 2 (2019), 1–6.<br>[5] <a target="_blank" rel="noopener" href="https://rrc.cvc.uab.es/?ch=13">https://rrc.cvc.uab.es/?ch=13</a><br>[6] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. 2015. Evaluation of deep convolutional nets for document image classification and retrieval. 2015 13th International Conference on Document Analysis and Recognition (ICDAR) (2015), 991–995.<br>[7] <a target="_blank" rel="noopener" href="https://github.com/tesseract-ocr/tesseract">https://github.com/tesseract-ocr/tesseract</a><br>[8] Xu Y, Xu Y, Lv T, et al. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding[J]. arXiv preprint arXiv:2012.14740, 2020.<br>[9] Xu Y, Lv T, Cui L, et al. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding[J]. arXiv preprint arXiv:2104.08836, 2021.<br>[10] <a target="_blank" rel="noopener" href="https://www.msra.cn/zh-cn/news/features/layoutlm">https://www.msra.cn/zh-cn/news/features/layoutlm</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/13b8c915.html" rel="prev" title="Android-四大组件-Activity生命周期"><i class="fa fa-chevron-left"></i> Android-四大组件-Activity生命周期</a></div><div class="post-nav-item"><a href="/6f4e5259.html" rel="next" title="跟李沐学AI-动手学深度学习 PyTorch版-52 文本预处理">跟李沐学AI-动手学深度学习 PyTorch版-52 文本预处理 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">背景介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LayoutLM%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF"><span class="nav-number">2.1.</span> <span class="nav-text">LayoutLM中的特征信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.3.</span> <span class="nav-text">预训练任务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.1.</span> <span class="nav-text">预训练数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.2.</span> <span class="nav-text">微调数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.3.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83"><span class="nav-number">3.4.</span> <span class="nav-text">下游任务微调</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E5%92%8C%E5%B1%95%E6%9C%9B"><span class="nav-number">4.</span> <span class="nav-text">总结和展望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">个人思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">参考链接</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">864</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">50</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">2.7m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">112:21</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>