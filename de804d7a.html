<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="原文"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读-AlexNet论文精读-《ImageNet Classification with Deep Convolutional Neural Networks》"><meta property="og:url" content="https://aisakaaoi.top/de804d7a.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="原文"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/28.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/29.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/30.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/31.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/1.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/2.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/3.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/4.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/5.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/6.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/7.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/8.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/9.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/10.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/11.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/12.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/13.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/14.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/15.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/16.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/17.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/18.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/19.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/20.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/21.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/22.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/23.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/24.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/25.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/26.webp"><meta property="og:image" content="https://aisakaaoi.top/de804d7a/27.webp"><meta property="article:published_time" content="2021-11-07T11:49:06.000Z"><meta property="article:modified_time" content="2023-06-18T18:51:16.501Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.top/de804d7a/28.webp"><link rel="canonical" href="https://aisakaaoi.top/de804d7a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>论文阅读-AlexNet论文精读-《ImageNet Classification with Deep Convolutional Neural Networks》 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">60</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">805</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.top/de804d7a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文阅读-AlexNet论文精读-《ImageNet Classification with Deep Convolutional Neural Networks》</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-07 19:49:06" itemprop="dateCreated datePublished" datetime="2021-11-07T19:49:06+08:00">2021-11-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/de804d7a.html" class="post-meta-item leancloud_visitors" data-flag-title="论文阅读-AlexNet论文精读-《ImageNet Classification with Deep Convolutional Neural Networks》" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/de804d7a.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/de804d7a.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>19k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>48 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><div class="pdfobject-container" data-target="./file/paper/2012-ImageNet-Classification-with-Deep-Convolutional.pdf" data-height="500px"></div><hr><span id="more"></span><h3 id="深度学习奠基作之一：AlexNet"><a href="#深度学习奠基作之一：AlexNet" class="headerlink" title="深度学习奠基作之一：AlexNet"></a>深度学习奠基作之一：AlexNet</h3><h4 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h4><p><strong>标题：</strong>ImageNet Classification with Deep Convolutional Neural Networks<br><strong>时间：</strong>2012<br><strong>出版源：</strong>Neural Information Processing Systems (NIPS)<br><strong>论文领域：</strong>深度学习，计算机视觉<br><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3065386">https://dl.acm.org/doi/10.1145/3065386</a><br><strong>引用格式：</strong>Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]&#x2F;&#x2F;Advances in neural information processing systems. 2012: 1097-1105.</p><h4 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h4><ul><li><strong>数据集：</strong>出现imageNet这样的大数据集，可以用来训练更复杂的模型 。</li><li><strong>CNN：</strong>而CNNs它们的能力可以通过改变深度和广度来控制，它们还对图像的本质（即统计的平稳性和像素依赖性的局部性）做出了强有力且基本正确的假设。与具有类似大小层的标准前馈神经网络相比，CNNs具有更少的连接和参数，因此更容易训练，而其理论上最好的性能可能只会稍微差一些。</li><li><strong>GPU：</strong>GPU和卷积操作结合，使得训练大型CNN网络成为可能</li></ul><h4 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h4><h5 id="贡献点"><a href="#贡献点" class="headerlink" title="贡献点"></a>贡献点</h5><ul><li>对ImageNet的子集进行了迄今为止最大的卷积神经网络训练，并取得了最好效果</li><li>高度优化GPU实现2D卷积方法，后面提到网络在2块GPU并行</li><li>网络包含很多新的不寻常特征</li><li>防止过拟合</li></ul><h5 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h5><p>使用ReLUs的四层卷积神经网络在CIFAR-10上达到25%的训练错误率，比使用tanh神经元的同等网络快六倍。每个网络的学习率都是独立选择的，以使训练尽可能快。没有任何形式的正规化。这里演示的效果的大小随网络结构的不同而不同，但是使用ReLUs的网络始终比使用饱和神经元的网络学习速度快几倍。</p><h5 id="Local-Response-Normalization-局部响应归一化（没啥用）"><a href="#Local-Response-Normalization-局部响应归一化（没啥用）" class="headerlink" title="Local Response Normalization 局部响应归一化（没啥用）"></a>Local Response Normalization 局部响应归一化（没啥用）</h5><img src="/de804d7a/28.webp"><p>此方案有助于泛化，CNN在未归一化的情况下，测试错误率为13%;在归一化的情况下，测试错误率为11%。</p><p>大致意思就是当今kernel的map的(x, y)像素位置在周围邻居相同kernel的map的(x, y)像素。然后把这些邻居pixel的值平方再加和。乘以一个系数 α 再加上一个常数k, 然后 β 次幂就是分母，分子就是kernel对应的map的(x, y)位置的pixel值。关键是参数 α, β, k如何确定, 论文中说在验证集中确定, 最终确定的结果为: k &#x3D; 2, n &#x3D; 5, α &#x3D; 0.0001, β &#x3D; 0.75</p><h5 id="Overlapping-Pooling-重叠池化"><a href="#Overlapping-Pooling-重叠池化" class="headerlink" title="Overlapping Pooling 重叠池化"></a>Overlapping Pooling 重叠池化</h5><p>初步理解：假如池化单元为zxz</p><ul><li>步长s &#x3D; z，就是传统的池化</li><li>步长s &lt; z，就是重叠池化</li></ul><p>作者提到当s &#x3D; 2, z &#x3D; 3时，对比s &#x3D; z &#x3D; 2，错误率减少了0.4%(top-1) 和 0.3%(top-5)<strong>（使用重叠池化可以减少过拟合）</strong></p><h5 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h5><img src="/de804d7a/29.webp"><p>一共8层，前5层是卷积层，后3层是全连接层：</p><ul><li>卷积层除了第3层，其余卷积层直接连接在同GPU上。第3层会连接第2层所有输出。</li><li>局部响应规范化（Local Response Normalization）层在第1层和第2层卷积层后。</li><li>最大池化层在响应规范化层（第1层和第2层卷积）和第5层卷积后。</li><li>ReLU非线性激活函数应用在每个卷积和全连接层。</li></ul><img src="/de804d7a/30.webp"><hr><h3 id="pass-1"><a href="#pass-1" class="headerlink" title="pass 1"></a>pass 1</h3><h4 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h4><p>首先是论文标题，论文标题：ImageNet Classification with Deep Convolutional Neural Networks，中文意思是：使用深度卷积神经网络对ImageNet数据集进行分类。</p><p>第一次读论文标题时要注意下面几个关键词：</p><ul><li>ImageNet：论文使用的数据集是ImageNet，ImageNet数据集具体内容是什么样的？</li><li>Neural Networks ：神经网络，这篇文章使用了神经网络技术。</li><li>Deep Convolutional：卷积神经网络工作原理是什么? 同时作者为什么要使用深度的卷积神经网络。</li></ul><p>下面是论文的作者，论文一作是Alex，这也是本篇论文提出的网络被称为AlexNet的原因。可能对论文前两个作者不是很熟悉，但是论文通信作者是Hinton，长期研究神经网络和深度学习。</p><img src="/de804d7a/31.webp"><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>首先我们先读一读摘要：</p><img src="/de804d7a/1.webp"><p><strong>中文翻译：</strong></p><p><strong>摘要</strong></p><p><strong>我们训练了一个大型深度卷积神经网络来将ImageNet LSVRC-2010竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了top-1 37.5%和top-5 17.0%的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和65万个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元，并在进行卷积操作时使用了非常有效的GPU。为了减少全连接层的过拟合，我们采用了一个最近开发的名为dropout的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了ILSVRC-2012竞赛，赢得了冠军并且与第二名top-5 26.2%的错误率相比，我们取得了top-5 15.3%的错误率。</strong></p><ul><li>前2句话介绍了自己做的工作，训练了一个很大很深的卷积神经网络在LSVRC-2010 contest取得了很好的结果。</li><li>第3句话介绍了网络的模型，有6000万个参数和65万个神经元，模型包含5个卷积层，以及最大池化层和3个全连接层。</li><li>第4句和第5句介绍了如何训练网络，使用了不饱和的神经元和GPU实现，为了减少全连接层的过拟合，使用了dropout技术。</li><li>最后一句， 介绍在ILSVRC-2012比赛取得了第一名，错误率比第二名低了10.9%，可以看到本文设计的网络模型效果很好。</li></ul><p>很少有人会在第一句话写我做了什么、第二句话写我做得很好，这种写法十分少见。然后他说这个神经网络有6千万个参数和65万个神经元，在当时可能还不知道什么是神经元，但对比起SVM、线性模型等参数量还是很大的。接着他说神经网络有五层，一层max-pooling、三层全连接层、最后一层softmax，在当时第一次读应该也不清楚在干什么事情。之后他说为了加快训练使用了GPU，从2007年出了CUDA库之后，在2012年GPU的使用已经比较常见了，那时候机器学习用的大部分都是matlab的gpu加速包。另外，为了减少过拟合，也用了一个dropout的方法。最后说这个模型在刷榜比赛上比第二名领先很多。这里最后说的15.3%和前面的17.0%第一眼看起来有点奇怪，但我们最后再来看看这是怎么回事。</p><p>这个不算是一个很好的摘要，更像是一个技术报告，但结果在这里比别人好很多，有一定吸引力。</p><hr><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><img src="/de804d7a/2.webp"><p><strong>中文翻译：</strong></p><p><strong>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除任何一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</strong></p><p><strong>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</strong></p><p>这篇文章是没有结论的，而是一个讨论。讨论更多是吐吐槽、看看未来要干什么事情，结论很多时候是跟摘要的一一对应，所以说没有结论通常是比较少见的。</p><p>虽然第一段的结论说“深度是重要的”，并没有错，但不能因为移除一层网络就下降2个点就说明是深度影响了，也有可能是参数没设好、参数变少了等原因。但在现在的观点看来确实没什么问题，深度固然很重要，但宽度也很重要。不能说特别深特别窄，也不能说特别扁特别宽，就像拍照一样高宽比是很重要的。</p><p>训练神经网络的时候，可以先预训练一下，把参数大致调整一下再进行精细训练，因为深度神经网络在当时训练是很不容易的，所以很多时候会先用一些未标号的预热。但本文说我们没有先预训练，是针对与标注数据进行专一训练（监督学习），而不是说通过一个大网络得到一个万能通用的对未标注数据的解法（无监督学习）。所以说AlexNet也是引领了监督学习的一个浪潮，不过从GAN开始大家的目光又慢慢回到无监督学习。</p><p>论文讨论有两段，第一段说我们在有监督学习上取得了很好的效果，证明了网络的深度很重要。第二段说我们没有使用任何的无监督预训练，这直接导致了后面的深度学习研究都集中在有监督学习上；同时作者提出未来会研究视频数据（当前研究的一个很火方向），因为视频数据会提供时序信息，这是静态图像数据所缺失的。</p><p>文章也是说了如果模型、计算量更大的情况下还能更好，虽然跟人类比还是有差距，但在现在看来，深度学习已经比人类好很多了，开开车大概也不是问题了。最后也提出了说如果算力更好的话也想对视频这种时序性强的数据进行训练，但在现在看来，过去了这么多年训练video数据仍是很难的事情。虽然我们在图片上做了很多事情、在自然语言上做了很多事情，但视频数据走的路还是比较慢，因为数据量增加的不是一点点，而且大部分还是有版权的。</p><hr><h4 id="图和表"><a href="#图和表" class="headerlink" title="图和表"></a>图和表</h4><img src="/de804d7a/3.webp"><p>左边的图我们看到分类还是很准确的，而且比如第4个leopard豹子的第二预测jaguar美洲豹也是很接近的一个类别。但第7个dalmatian斑点狗和cherry樱桃同时在图里还是容易出错。第8个madagascar cat一般人也认不出来，也看得出深度学习中监督学习的优点。</p><p>右边的图比较有意思，他是把神经网络倒数第二层的图片拿出来，得到一个长的向量，根据余弦相似度得到相近的图片组，发现都很像。虽然论文没有讨论到这个东西有多重要，但实际上可能这才是最重要的结果。就是说深度神经网络最后训练出来的向量在语义空间里的表示特别好，相似的图片会把它放在一起，是一个非常好的特征，非常适合用之后机器学习的简单的分类器来分类，这也是深度学习的一大强项。</p><hr><img src="/de804d7a/4.webp"><p>这个表是本文的结果和别人的结果进行对比。前两个是那时候最好的在Top-1和Top-5上的结果，但可以看到本文的结果好很多。</p><hr><h4 id="pass-1-总结"><a href="#pass-1-总结" class="headerlink" title="pass 1 总结"></a>pass 1 总结</h4><p>第一遍读完大概知道这篇文章用深度学习的方法，效果特别好。具体为什么好、怎么做的还不清楚，之后再细读。</p><hr><h3 id="pass-2"><a href="#pass-2" class="headerlink" title="pass 2"></a>pass 2</h3><p>第二遍开始细读，对文章正文内容开始阅读。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><img src="/de804d7a/5.webp"><p><strong>中文翻译：</strong></p><p><strong>1 引言</strong></p><p><strong>当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们必须收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小——都在几万张图像的数量级上（例如，NORB[16]，Caltech-101&#x2F;256 [8, 9]和CIFAR-10&#x2F;100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，以及包含了22000个类别上的超过1500万张标注的高分辨率的图像ImageNet[6]。</strong></p><p>大数据集、大模型、更好的技术来防止过拟合，这基本是机器学习正常的途径。当时深度学习界认为，大模型+正则来防止过拟合，并且在之后几年内大家也是这么做的。但现在观点来看，好像正则又不是很重要，关键是整个神经网络的设计，使得很大的神经网络在没有很好的正则情况下也能训练的很好。这部分的理论和实践工作还在推进，坐等以后新的结论。后面吹了一下它这个ImageNet特别好，毕竟题目就说了本文用的是ImageNet，吹一下它数据量大、类别多。</p><hr><img src="/de804d7a/6.webp"><p><strong>中文翻译：</strong></p><p><strong>为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被特定化，即使通过像ImageNet这样足够大的数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一类这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的容量可以通过改变它们的深度和广度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络相比，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络稍微差一点。</strong></p><p>说完需要很大的模型，然后就直接讲CNN了，讲怎么把CNN做的特别大，所以这段的意思是大家应该用CNN来做，因为CNN是一个很好的模型。但是CNN做大不容易，因为做大很容易overfitting或者训练不动，但当时主流不用CNN，文章却没有提及其它模型和别人的算法，只提到CNN会显得视角比较狭窄，这个不能学。不能只提到自己的小领域，也要跟他人作对比。</p><hr><img src="/de804d7a/7.webp"><p><strong>中文翻译：</strong></p><p><strong>尽管CNN具有引人注目的质量，尽管它们的局部架构相对有效，但是将它们应用到大规模的高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣大量CNN的训练，以及最近的数据集例如ImageNet包含足够多的标注样本来训练这样的模型而没有严重的过拟合。</strong></p><p>第三段讲用GPU训练CNN，并且数据集ImageNet够大没有过拟合。所以前三段大概就是讲了个故事，讲做了什么东西。</p><hr><img src="/de804d7a/8.webp"><p><strong>中文翻译：</strong></p><p><strong>本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络固有的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。</strong></p><p>第四段讲的是作者这个paper的贡献。作者训练了当时为止最大的神经网络并且取得了特别好的成果，然后在GPU上实现了一个2D的卷积，网络也有新的特性，能够提升性能和降低训练时间，然后说第三节细说。第四节细说怎么防止这个大网络过拟合。最后说网络有5个卷积层、3个全连接层，发现深度好像很重要。</p><p>如果说在Imagenet上取得了特别好的结果，但只是把100个模型融合起来拿了个第一名，这就没什么意思了。就是说，在模型上没有太多创新，更多的是一些工程上、技术上的创新，这种论文大家就不太爱看。但本文说用了一些unusual features、新的技术来防止过拟合，新的技术是比较有意思的，如果用新技术又取得了较好的成果，大家更愿意往下去做、去钻研这个方向，更有启发性一点。</p><p>太大、太复杂、不容易复现，这样的模型对以后的研究工作不是很有用，作为研究工作者更希望看到新的思路、新的能够取得好效果的方向。（或许内涵GPT-3 or yolov4的叠buff操作）</p><p>反过来讲，本文如果只是取得了好的成果，没有什么新东西，也就不会成为奠基作了。</p><hr><img src="/de804d7a/9.webp"><p><strong>中文翻译：</strong></p><p><strong>最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。</strong></p><p>这一段介绍了一下设备。存下来的很多都是一些研究型的东西，少是工程性的细节，后面大家各种文章去更好的实现也没有这篇奠基石的开创性更重要。工程工作的本质还是脑力层面的体力劳动。</p><hr><h4 id="The-Dataset"><a href="#The-Dataset" class="headerlink" title="The Dataset"></a>The Dataset</h4><img src="/de804d7a/10.webp"><p><strong>中文翻译：</strong></p><p><strong>2 数据集</strong></p><p><strong>ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了亚马逊（Amazon）的Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。</strong></p><p><strong>ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：top-1和top-5，top-5错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中的分数。</strong></p><p>这两段话介绍了一下数据集，ImageNet很大、类别很多有22000类。这个模型参加过几次比赛，ILSVRC-2010有测试集所以看得出错误率，ILSVRC-2012就没有提供数据集，简单讲了一下摘要里那些错误率怎么来的。</p><hr><img src="/de804d7a/11.webp"><p><strong>中文翻译：</strong></p><p><strong>ImageNet包含各种分辨率的图像，而我们的系统要求固定的输入维度。因此，我们将图像进行下采样到固定的256×256分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256×256大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心化的）上训练我们的网络。</strong></p><p>这段讲ImageNet的预处理，保证短边为256然后中心裁剪，不做其它预处理。因为ImageNet图片的大小不一，要先处理一下。</p><p>当时的工作大部分都是把图片抽取特征，然后再进行训练，但这个是直接对原始像素上做的，虽然文章没有把这个作为卖点，但这个端对端的观点（end to end）在之后也得到了证实，模型会帮我们提取出特征，不用我们先提取。</p><p>SIFT在之前也是一个很流行的图像特征抽取算法，但从这开始，或许你不知道SIFT是怎么抽的，但你知道AlexNet是怎么做的，了解从原始图片到目标结果，不再需要了解那么多的专业知识，不再需要了解CV过去30年做特征是怎么做的。所以说，简单有效的东西是能够持久的。</p><hr><h4 id="The-Architecture"><a href="#The-Architecture" class="headerlink" title="The Architecture"></a>The Architecture</h4><img src="/de804d7a/12.webp"><p><strong>中文翻译：</strong></p><p><strong>3 架构</strong></p><p><strong>我们的网络架构概括为图2。它包含八个学习层——5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的或者不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的排在最前面。</strong></p><p>第三章是讲网络的架构，这也是文章的主要贡献之一，另外一个主要贡献是怎样避免过拟合在第四章。</p><hr><img src="/de804d7a/13.webp"><p><strong>中文翻译：</strong></p><p><strong>3.1 ReLU非线性</strong></p><p><strong>将神经元输出f建模为输入x的函数的标准方式是用f(x) &#x3D; tanh(x)或f(x) &#x3D; (1 + e−x)−1。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性f(x) &#x3D; max(0,x)更慢。根据Nair和Hinton[20]的说法，我们将这种非线性神经元称为修正线性单元(ReLU)。采用ReLU的深度卷积神经网络训练时间比等价的tanh单元要快好几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。</strong></p><p><strong>图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络一直比等价的饱和神经元快几倍。</strong></p><p><strong>我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数f(x) &#x3D; |tanh(x)|与对比归一化以及局部均值池化在Caltech-101数据集上表现甚好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响（预防过拟合）不同于我们使用ReLU拟合数据集时的反映的加速能力。更快的学习速率对大型模型在大型数据集上的性能有很大的影响。</strong></p><p>本小节标题是ReLU非线性激活函数，它说正常都是用tanh和sigmoid，但这些饱和的非线性激活函数会比非饱和的非线性激活函数慢（比如ReLU：max(0, x)）。可能有些人饱和和非饱和这些 saturating nonlinearities 、 non-saturating nonlinearity 没懂，但我们第二遍就不死挖到底了。</p><p>文章说用了ReLU特别好、特别快，图1也就表示出了它好的效果，随着epoch训练轮数的增加下降得更快。为什么非饱和更快，文章没有详细说明，有个引用标注，之后可以再看[20]的引用论文。</p><p>也分析了其他人的激活函数比如|tanh(x)|也不错，但还是不如ReLU快，毕竟数据集很大。在那时训练个ImageNet代价还是比较大的，快一点当然比较喜欢。不过在现在看来ReLU也没有说快多少，而且当时觉得ReLU快的原因在现在看来也不是那么正确，现在换个激活函数问题也不大。但大家还是用ReLU是单纯因为它简单。</p><hr><img src="/de804d7a/14.webp"><p><strong>中文翻译：</strong></p><p><strong>3.2 在多GPU上训练</strong></p><p><strong>单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。这种连接模式的选择有一个关于交叉验证的问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。</strong></p><p><strong>除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“柱状”CNN。与每个卷积层中只有一半的核在单GPU上训练的网络相比，这个方案分别降低了我们的top-1 1.7%，top-5 1.2%的错误率。双GPU网络比单GPU网络稍微减少了训练时间2。</strong></p><p><strong>图2：我们CNN架构图解，明确描述了两个GPU之间的职责。一个GPU运行图中上面部分的层，而另一个GPU运行图下面部分的层。两个GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440-186,624-64,896-64,896-43,264-4096-4096-1000。</strong></p><p>这几段主要是讲怎么用多个GPU并行训练，其实这些比较工程性细节的东西第二遍可以不看，除非是系统方向的论文，深度学习的可以等之后要复现再来细看。</p><hr><img src="/de804d7a/15.webp"><p><strong>中文翻译：</strong></p><p><strong>3.3 局部响应归一化（译者注：后来的研究表明，LRN几乎没有用处，因此读者可以跳过此部分）</strong></p><p><strong>ReLU具有让人满意的特性，即它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部归一化有助于泛化。 表示神经元激活，通过在(x,y)位置应用核i，然后应用ReLU非线性来计算，响应归一化激活 通过下式给定：</strong></p><p><strong>求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k&#x3D;2，n&#x3D;5，α&#x3D;0.0001，β&#x3D;0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。</strong></p><p><strong>这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因为我们没有减去均值。响应归一化分别减少了top-1 1.4%，top-5 1.2%的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：没有归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。</strong></p><p>“不需要通过输入归一化来防止饱和”，看不太懂，可以圈出来第三遍再看。它只讲了怎么定义怎么用这些细节，但没说为什么一定要用它以及它的效果。第二遍看不太懂，可以忽略一下，知道它是个normalization就可以。</p><p>如果以现在的眼光来看，这个东西已经不重要了，在之后的研究几乎没有人用到它。现在我们也有更好的normalization的技术。</p><hr><img src="/de804d7a/16.webp"><p><strong>中文翻译：</strong></p><p><strong>3.4 重叠池化</strong></p><p><strong>CNN中的池化层使用相同的核映射归纳了神经元相邻组的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为s个像素，每个网格归纳池化单元中心位置z × z大小的邻居。如果设置s &#x3D; z，我们会得到通常在CNN中采用的传统局部池化。如果设置s &lt; z，我们会得到重叠池化。这就是我们网络中使用的方法，设置s &#x3D; 2，z &#x3D; 3。这个方案与非重叠方案s &#x3D; 2, z &#x3D; 2相比，分别降低了top-1 0.4%，top-5 0.3%的错误率，两者的输出维度是相等的。我们在训练过程发现，采用重叠池化的模型更难以过拟合。</strong></p><p>一般来说pooling是不会overlapping的，但这说要重叠一下。也有可能不了解什么是pooling是干什么的，都第三遍之前先看一下。本文在传统池化方法上做了一点改动，虽然说改动不大，但文章说效果很好。</p><hr><img src="/de804d7a/17.webp"> <img src="/de804d7a/18.webp"><p><strong>中文翻译：</strong></p><p><strong>3.5 整体架构</strong></p><p><strong>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出喂给1000维的softmax层，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</strong></p><p><strong>第2，4，5卷积层的神经元只与位于同一GPU上的前一层的核映射相连接（见图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。第3.4节描述的这种最大池化层加在了响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</strong></p><p><strong>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波操作，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个大小为5 × 5 × 48核进行滤波。第3，4，5卷积层依次连接，中间没有接入任何池化层或归一化层。第3卷积层有384个大小为3 × 3 × 256的核，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个大小为3 × 3 × 192的核，第5卷积层有256个核大小为3 × 3 × 192的核。每个全连接层有4096个神经元。</strong></p><p>最后这一部分是整体的架构，讲了整体的8个层，因为用了两个GPU所以整个架构是比较麻烦的，第一个层的大小是224 × 224 × 3（为什么不是256之后会说）……</p><p>这篇文章看起来更像一篇技术报告，只讲作者做了什么，也不跟其他人做对比。</p><p>接下来看看图里的网络结构，不过这个图过于抠细节，有些繁琐，第一次看不一定理解。</p><p>框框是指每一层输入输出的数据的大小，进来的时候是一个224 × 224 × 3的图片。第一次卷积是11 × 11，stride跳4格，输出是55 × 55 × 48。在两个GPU上都有各自的5层卷积层，从第1层到第2层，是对当前GPU的层进行卷积；第2层到第3层，会到另一个GPU拿来卷积结果看一眼，在输出通道维度合并；第3到第4，第4到第5都是各搞各的，各自卷积；从卷积层到第1个全连接层又进行了一次通讯，后面的全连接层也是，其实就是表示成一个4096的向量，最后用一个线性分类器变成1000分类。</p><p>也就是说一张图片最后会表示成一个4096的维度，这个向量能很好的抓住语义信息，如果两个图片向量特别相近，那么很有可能是类似的图片。所以就是一张人能看得懂的像素图片，进行神经网络的特征提取，最后变成一个4096的机器能懂的向量，这个向量能够表示中间的语义信息。其实网络就是在压缩，这个向量机器可以拿来做搜索也好、分类也好。</p><p>但分成了两个GPU训练，导致模型较为复杂，其实在现在来看，其中的一块卡可能用caffe已经可以训练这个AlexNet了，没必要双卡，但Alex应该是花了很多时间搞这个双卡的模型，觉得是一个贡献点。如果三个卡、四个卡呢，网络岂不是更为复杂，所以模型其实也不用切。在六七年里，大家也都没有把模型进行切分，但在最近比如要训练GPT这种超级大的网络，可能训练不动了，又觉得模型切分好用了。（硬件再发展可能又不用切分了，所以这是一个机器算力和模型大小的矛盾）</p><hr><h4 id="Reducing-Overfitting"><a href="#Reducing-Overfitting" class="headerlink" title="Reducing Overfitting"></a>Reducing Overfitting</h4><img src="/de804d7a/19.webp"><p><strong>中文翻译：</strong></p><p><strong>4 减少过拟合</strong></p><p><strong>我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。</strong></p><p>第四节讲的是如何降低过拟合，因为训练了一个较大的网络，现在要防止过拟合。</p><hr><img src="/de804d7a/20.webp"> <img src="/de804d7a/21.webp"><p><strong>中文翻译：</strong></p><p><strong>4.1 数据增强</strong></p><p><strong>图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种不同的数据增强方式，这两种方式都是从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案没有消耗计算量。</strong></p><p><strong>第一种数据增强方式包括产生图像平移和水平翻转。我们从256 × 256图像上通过随机提取224 × 224的图像块（以及这些图像块的水平翻转）实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层的预测结果进行平均。</strong></p><p><strong>第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行主成分分析（PCA）。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素 ，我们加上下面的数量：</strong></p><p><strong>pi，λi分别是RGB像素值3 × 3协方差矩阵的第i个特征向量和特征值，αi是前面提到的随机变量。对于某个训练图像的所有像素，每个αi只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的强度和颜色发生变化时，物体本身没有发生变化。这个方案减少了top 1错误率1%以上。</strong></p><p>用了两种数据增强方式，当然也不是首创，也是前人的方法了。</p><p>第一种是尺寸上修改，通过平移与翻转进行抠图，从256 × 256变成224 × 224；第二种是颜色上修改，比如PCA改变一下图片的颜色，这个PCA不明白也可以留到后面细看。</p><hr><img src="/de804d7a/22.webp"><p><strong>中文翻译：</strong></p><p><strong>将许多不同模型的预测结果结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎将花费太长时间以至于无法训练。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近推出的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些用这种方式“丢弃”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</strong></p><p><strong>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</strong></p><p>虽然说模型融合很好用，但太久了不好用。所以Alex用了Hinton老爷子在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout，设置每次0.5的概率忽略神经元来防止过拟合，其实现在的眼光来看，dropout也不是模型融合，应该说是正则项。几年后他们也写了篇文章说dropout在线性模型中等价于一个L2的正则项，更复杂来说应该说是带来了一个正则的效果。</p><p>文章说把dropout放在了全连接层前，如果不放那么过拟合就很严重，但放了后训练时间大概会翻倍。所以AlexNet用了3个全连接，最后一个肯定是有的毕竟要输出，中间的两个很大的4096全连接是模型的一大瓶颈，这是该模型当时设计的一大缺陷，所以说导致整个模型特别大，放不进一个GPU里。因为用的是4096的全连接，dropout能防止过拟合，但现在CNN的设计通常不会使用那么大的全连接层，所以导致dropout也不那么重要，GPU的内存也没那么吃紧了。</p><p>反过来讲，dropout在全连接上还是很有用的，在RNN上、在Attention上dropout用得很多。</p><hr><h4 id="Details-of-learning"><a href="#Details-of-learning" class="headerlink" title="Details of learning"></a>Details of learning</h4><img src="/de804d7a/23.webp"> <img src="/de804d7a/24.webp"><p><strong>中文翻译：</strong></p><p><strong>我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减率为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：而且它减少了模型的训练误差。权重ww的更新规则是：</strong></p><p><strong>i是迭代索引，v是动量变量，ϵ是学习率， 是目标函数对w，在wi上的第i批微分Di的平均。</strong></p><p><strong>我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了早期阶段的学习。我们对剩下的层的神经元偏置初始化为0。</strong></p><p><strong>我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止改善时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。</strong></p><p>第五章讲的是模型是怎么样训练的，他说用SGD这个优化算法来训练，现在来看SGD当然是深度学习最常用的算法，但当年大家并不是这么觉得，因为SGD调参相对来说比较难调，那时候常用更稳定的算法比如L-BFGS、Coordinate Descent。后来大家发现SGD里面的噪音对模型其实是有好处的，所以现在深度学习大家都用这个了。</p><p>batch size是128，momentum是0.9，weight decay是0.0005。weight decay当时在机器学习界主流上应该叫作L2 regularization（L2正则项），但是这篇文章以及神经网络里面喜欢叫作weight decay，所以这个东西不是加在模型上，而是加在优化算法上，虽然说这两者其实是等价的关系，但深度学习的崛起大家也都叫做weight decay了。momentum也是防止在一段非极值较陡的区域上陷入局部最低点，而是保有一个冲量，冲过这一段，更好地找到最值。</p><p>函数里也可以看到新的momentum项等于0.9 × 旧momentum - 0.0005 × weight decay × 另外一个东西（这个东西其实就是学习率），最后再减去梯度。</p><p>然后他说权重是用一个均值为0、方差为0.01的高斯随机变量来初始化的。0.01这个值也是经验值，这个不大也不小，比较好用。当然了网络比较深需要更多的优化，对于这些简单的网络已经很ok了，现在那些大的网络有些也用0.02。</p><p>还说了在第2、4、5层卷积层和全连接层神经元的偏移量设置为1，其它设置为0，其实一般都是初始化为0，这个调参比较玄学，后来大家也没有跟进研究这个细节。毕竟全部为0效果也不差，就不整这个玄学了。（或许多训练一次结果又不一样了）</p><p>每个层都用的一样的学习率0.01，然后人工盯着不太动了就变为当前的十分之一，当时计算资源比较贵以及一段时间内大家都是这么干的（手动炼丹），但现在看来也没那么复杂，比如ResNet总共训练120轮每30轮就乘以0.1（动态学习率），也可以比如先训练60轮、100轮后面再下降。现在来说用一些更平滑的方法更常见，比如用cos之类的曲线。当然了学习率一开始也不能太大，太大了容易炸，太小了学不会训练不动，所以也有像yolo一样从0开始，先线性上升在cos下降。</p><p>训练了90个epochs，每一遍都是扫了训练集的120w张图片，用2张580训练了五六天。这个还是挺久的，调个参得五六天才出结果。（这也让N卡股价暴涨，要买好卡赶紧出结果）现在图像很快了，但即使用几千块的卡训练文本也要很多天，看看下次迭代是啥时候，让文本领域也发光发热。</p><hr><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><img src="/de804d7a/25.webp"> <img src="/de804d7a/26.webp"> <img src="/de804d7a/27.webp"><p><strong>6 结果</strong></p><p><strong>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了top-1 37.5%，top-5 17.0%的错误率5。在ILSVRC-2010竞赛中最佳结果是top-1错误率47.1%和top-5错误率28.2%，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，之后公布的最好结果是top-1错误率45.7%和top-5错误率25.7%，使用的方法是在Fisher向量（FV）上训练的两个分类器的预测结果取平均，Fisher向量是通过两种密集采样特征计算得到的[24]。</strong></p><p><strong>表1：ILSVRC-2010测试集上的结果对比。斜体表示的是其它人取得的最好结果。</strong></p><p><strong>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签没有公开，因此我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会将验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了top-5 18.2%的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“微调”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛团队取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</strong></p><p><strong>表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。</strong></p><p><strong>最后，我们也报告了我们在ImageNet 2009秋季数据集上的错误率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照文献中的惯例，用一半的图像来训练，一半的图像来测试。由于数据集上没有建立好的测试集，我们对数据集分割必然不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的top-1和top-5错误率最好结果是78.1%和60.9%[19]。</strong></p><p><strong>6.1 定性评估</strong></p><p><strong>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</strong></p><p><strong>图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。</strong></p><p><strong>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，照片的预期焦点确实存在的模糊性。</strong></p><p><strong>图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红色柱形表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。</strong></p><p><strong>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有不同的姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</strong></p><p><strong>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</strong></p><p>这部分也就是说模型的效果有多好，中间的过程也是现在深度学习的标准流程了。</p><p>我们读论文的很多时候，实验部分相对来说是不那么重要的，我们关心的是实验的一个效果，但是具体实验是怎么做的，很多时候除非我们是这个领域的专家，但专家一眼就大概知道了，如果是初入领域的新手也看不懂，除非你要重复这个实验、或者审论文等专家行为的时候才会大概去看他的实验。</p><p>最后文章也汇报了模型在完整的ImageNet上训练10184个种类的890w张图，但大家一般对ImageNet的印象也就是那1000类的120w张图，之后训练大家也不跑完整的890w，比较不解。</p><p>6.1也说了在两块GPU上的一个是无颜色的一个是颜色相关的，很奇怪，炼丹多次也是一样的结果。这里也是留下了一个疑惑，不过后来大家也都不拆分模型，所以也没有得到重视，毕竟是炼丹，过程奇怪也正常（？）。鄙人拙见可能是跟卷积核或者图片切块的方式有关。</p><p>有一些神经元还是很有对应性的，比如底层的神经元学到的是一些局部的信息，比如纹理、方向，上层的神经元学到的一些比如头、胳膊、动物等轮廓。也有人受启发去研究神经网络到底学什么，学形状呢还是学纹理，所以说这篇奠基石是很有启发性的。</p><p>相对于比较简单的机器学习模型来讲，现在大家还是不知道神经网络到底在学什么，它的可解释性还是比较低的。但这几年大家也开始慢慢研究它的公平性、神经网络的偏移等也是热议的重点，因为如果想用神经网络做决策，那么公平是很重要的。</p><hr><h3 id="他人总结"><a href="#他人总结" class="headerlink" title="他人总结"></a>他人总结</h3><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/cg129054036/article/details/120794416">https://blog.csdn.net/cg129054036/article/details/120794416</a></p></blockquote><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ih411J7Kz">https://www.bilibili.com/video/BV1ih411J7Kz</a><br><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/Jwenxue/article/details/89317880">https://blog.csdn.net/Jwenxue/article/details/89317880</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/a3fd2a72.html" rel="prev" title="脑机接口与混合智能-学习报告-博睿康脑电设备的试用报告"><i class="fa fa-chevron-left"></i> 脑机接口与混合智能-学习报告-博睿康脑电设备的试用报告</a></div><div class="post-nav-item"><a href="/b2f3cb6a.html" rel="next" title="第九艺术-微软Xbox科普指南（2021版）">第九艺术-微软Xbox科普指南（2021版） <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">原文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A5%A0%E5%9F%BA%E4%BD%9C%E4%B9%8B%E4%B8%80%EF%BC%9AAlexNet"><span class="nav-number">2.</span> <span class="nav-text">深度学习奠基作之一：AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="nav-number">2.1.</span> <span class="nav-text">基本信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="nav-number">2.2.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="nav-number">2.3.</span> <span class="nav-text">创新点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%A1%E7%8C%AE%E7%82%B9"><span class="nav-number">2.3.1.</span> <span class="nav-text">贡献点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.2.</span> <span class="nav-text">ReLU激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Local-Response-Normalization-%E5%B1%80%E9%83%A8%E5%93%8D%E5%BA%94%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88%E6%B2%A1%E5%95%A5%E7%94%A8%EF%BC%89"><span class="nav-number">2.3.3.</span> <span class="nav-text">Local Response Normalization 局部响应归一化（没啥用）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Overlapping-Pooling-%E9%87%8D%E5%8F%A0%E6%B1%A0%E5%8C%96"><span class="nav-number">2.3.4.</span> <span class="nav-text">Overlapping Pooling 重叠池化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">2.3.5.</span> <span class="nav-text">网络架构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pass-1"><span class="nav-number">3.</span> <span class="nav-text">pass 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E9%A2%98"><span class="nav-number">3.1.</span> <span class="nav-text">标题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">3.2.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">3.3.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%92%8C%E8%A1%A8"><span class="nav-number">3.4.</span> <span class="nav-text">图和表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pass-1-%E6%80%BB%E7%BB%93"><span class="nav-number">3.5.</span> <span class="nav-text">pass 1 总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pass-2"><span class="nav-number">4.</span> <span class="nav-text">pass 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Introduction"><span class="nav-number">4.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-Dataset"><span class="nav-number">4.2.</span> <span class="nav-text">The Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-Architecture"><span class="nav-number">4.3.</span> <span class="nav-text">The Architecture</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reducing-Overfitting"><span class="nav-number">4.4.</span> <span class="nav-text">Reducing Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Details-of-learning"><span class="nav-number">4.5.</span> <span class="nav-text">Details of learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Results"><span class="nav-number">5.</span> <span class="nav-text">Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%96%E4%BA%BA%E6%80%BB%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">他人总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">7.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">805</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">60</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">2.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">104:36</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>