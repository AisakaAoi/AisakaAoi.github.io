<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"example.com",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="原文  BERT 论文链接：https:&#x2F;&#x2F;aclanthology.org&#x2F;N19-1423.pdf BERT: 近 3 年 NLP 最火 CV: 大数据集上的训练好的 NN 模型，提升 CV 任务的性能 —— ImageNet 的 CNN 模型 NLP: BERT 简化了 NLP 任务的训练，提升了 NLP 任务的性能 BERT 如何站在巨人的肩膀上的？使用了哪些 NLP 已有的技术和思想？哪"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读-BERT论文逐段精读-《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》"><meta property="og:url" content="http://example.com/5f1c011a.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="原文  BERT 论文链接：https:&#x2F;&#x2F;aclanthology.org&#x2F;N19-1423.pdf BERT: 近 3 年 NLP 最火 CV: 大数据集上的训练好的 NN 模型，提升 CV 任务的性能 —— ImageNet 的 CNN 模型 NLP: BERT 简化了 NLP 任务的训练，提升了 NLP 任务的性能 BERT 如何站在巨人的肩膀上的？使用了哪些 NLP 已有的技术和思想？哪"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/5f1c011a/1.png"><meta property="og:image" content="http://example.com/5f1c011a/2.png"><meta property="og:image" content="http://example.com/5f1c011a/3.png"><meta property="article:published_time" content="2022-01-20T18:46:11.000Z"><meta property="article:modified_time" content="2023-06-09T22:25:14.764Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/5f1c011a/1.png"><link rel="canonical" href="http://example.com/5f1c011a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>论文阅读-BERT论文逐段精读-《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">逢坂葵葵</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">52</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">513</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://example.com/5f1c011a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文阅读-BERT论文逐段精读-《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-01-21 02:46:11" itemprop="dateCreated datePublished" datetime="2022-01-21T02:46:11+08:00">2022-01-21</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/5f1c011a.html" class="post-meta-item leancloud_visitors" data-flag-title="论文阅读-BERT论文逐段精读-《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/5f1c011a.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/5f1c011a.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>10k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>25 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><div class="pdfobject-container" data-target="./file/paper/2018-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding.pdf" data-height="500px"></div><p><strong>BERT</strong> 论文链接：<a target="_blank" rel="noopener" href="https://aclanthology.org/N19-1423.pdf">https://aclanthology.org/N19-1423.pdf</a></p><p>BERT: 近 3 年 NLP 最火</p><p>CV: 大数据集上的训练好的 NN 模型，提升 CV 任务的性能 —— ImageNet 的 CNN 模型</p><p>NLP: BERT 简化了 NLP 任务的训练，提升了 NLP 任务的性能</p><p>BERT 如何站在巨人的肩膀上的？使用了哪些 NLP 已有的技术和思想？哪些是 BERT 的创新？</p><span id="more"></span><hr><h3 id="标题-作者"><a href="#标题-作者" class="headerlink" title="标题 + 作者"></a>标题 + 作者</h3><p><strong>BERT</strong>: Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p>pre-training: 在一个大的数据集上训练好一个模型 pre-training，模型的主要任务是用在其它任务 training 上。</p><p>deep bidirectional transformers: 深的双向 transformers</p><p>language understanding: 更广义，transformer 主要用在机器翻译 MT</p><p>BERT: 用深的、双向的、transformer 来做预训练，用来做语言理解的任务。</p><p>作者：Google AI Language，写作时间短（几个月）</p><hr><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>新的语言表征模型 BERT: <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers，基于 ELMo<br>Transformers 模型的双向编码表示</p><p>与 ELMo 和 GPT 不同，BERT 从无标注的文本中（jointly conditioning 联合左右的上下文信息）预训练得到 无标注文本的 deep bidirectional representations</p><p>pre-trained BERT 可以通过加一个输出层来 fine-tune，在很多任务（问答、推理）有 SOTA 效果，而不需要对特定任务的做架构上的修改。</p><p>GPT unidirectional，使用左边的上下文信息 预测未来<br>BERT bidirectional，使用左右侧的上下文信息</p><p>ELMo based on RNNs, down-stream 任务需要调整一点点架构<br>BERT based on Transformers, down-stream 任务只需要调整最上层。<br>GPT, down-stream 任务 只需要改最上层。</p><p>摘要第一段：和哪两篇工作相关，区别是什么？<br>BERT 是在 GPT 和 ELMo 的基础上的改动。</p><p><strong>摘要第二段：BERT 的好处</strong><br>simple and empirically powerful, 11 NLP 任务的SOTA, 绝对精度 + 相对精度（比别人好多少）</p><p>摘要写法：<br>第一段：我和另外 2 篇相关工作的区别，改进在哪里？<br>第二段：我的结果特别好，好在什么地方？</p><p>Note: BERT 论文写作好 –&gt; 经典<br>工作质量：创新性、效果好 –&gt; 经典</p><hr><h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>导言第一段：本篇论文关注的研究方向的一些上下文关系<br>Language model pre-training 可以提升 NLP 任务的性能<br>NLP任务分两类：sentence-level tasks 句子情绪识别、两个句子的关系； token-level tasks NER (人名、街道名) 需要 fine-grained output</p><p>NLP 预训练很早之前存在，BERT 使 NLP 预训练 出圈了。</p><p>导言第二段：摘要第一段的扩充</p><p>pre-trained language representations 两类策略：<br><strong>基于特征的 ELMo</strong> (构建和每一个下游任务相关的 NN 架构；训练好的特征（作为额外的特征） 和 输入 一起放进模型)</p><p><strong>基于微调参数的 GPT</strong><br>所有的权重参数根据新的数据集进行微调。</p><p>介绍别人工作的目的：铺垫自己方法的好</p><p>ELMo 和 GPT 预训练时 使用 unidirectional langugage model，使用相同的目标函数<br>语言模型是单向的、预测未来。不是给第 一句、第三句，预测第二句</p><p>导言第三段：<br>当前技术的局限性：标准语言模型是 unidirectional 单向的，限制了模型架构的选择。</p><p>GPT 从左到右的架构，只能将输入的一个句子从左看到右。句子情感分类任务：从左看到右、从右看到左 都应该是合法的。</p><p>token-level tasks：问答 qa 看完整个句子选答案，不是从左往右一步一步看。</p><p>如果能 incorporate context from both directions 看两方向的信息，能提升 任务性能。</p><p>相关工作的局限性，+ 解决局限性的想法 – &gt; 导言第四段： 如何解决？</p><p>BERT 通过 MLM 带掩码的语言模型 作为预训练的目标，来减轻 语言模型的单向约束。inspired by the Close task 1953</p><p>MLM 带掩码的语言模型做什么呢？<br>每次随机选输入的词源 tokens, 然后 mask 它们，目标函数是预测被 masked 的词；类似挖空填词、完形填空。</p><p>MLM 和 standard language model （只看左边的信息）有什么区别？<br>MLM 可以看 左右的上下文信息, pre-train deep bidirectional transformer 的基础。</p><p>BERT 除了 MLM 还有什么？<br>NSP: next sentence prediction<br>判断两个句子是随机采样的 or 原文相邻，学习 sentence-level 的信息。</p><p><strong>文章 3点 贡献：</strong></p><ol><li><p>bidirectional 双向信息的重要性<br>GPT 只用了 unidirectional 信息；另外 Peter 2018 把从左看到右 和 从右看到左的模型独立训练 + shallow concatenation 拼在一起；BERT 在 bidirectional pre-training 的应用更好</p></li><li><p>BERT 首个 微调模型，在 sentence-level and token-level task效果好<br>好的预训练模型，不用对特定任务做一些模型架构的改动</p></li><li><p>BERT 开源，随便用。</p></li></ol><hr><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>近期实验表明，非监督的预训练模型很好，low-resource 任务也能享受 benefit from 深的神经网络。<br>本文贡献：拓展前任的结果到 deep bidirectional architectures，使同样的预训练模型能够处理大量的 NLP 任务</p><p><strong>本文故事：</strong></p><p>2个相关工作：ELMo 用了 bidirectional 信息，但架构 RNN 老；GPT 架构 Transformer 新，但只用了 unidirectional 信息。</p><p>BERT &#x3D; ELMo 的 bidirectional 信息 + GPT 的新架构 transformer</p><p>How?<br>Language model 任务：不是预测未来，而是完形填空。</p><p>写作：两个算法的结合，主要工作 – 证明 双向有用</p><p>A + B 缝合工作 or C 技术解决 D 领域的问题，不要觉得想法小、不值得写出来；简单朴实的写出来。简单好用 说不定会出圈</p><hr><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>2.1 Unsupervised Feature-based approaches<br>非监督的基于特征表示的工作：词嵌入、ELMo等</p><p>2.2 Unsupervised Fine-tuning approaches<br>非监督的基于微调的工作：GPT等</p><p>2.3 Transfer Learning from Supervised Data<br>在有标号的数据上做迁移学习。</p><p>NLP 有标号 的大数据集：natural language inference and machine translation</p><p>CV做的还不错，ImageNet 训练好、再做迁移。</p><p>NLP 表现不那么好：CV 和 NLP 任务的区别，NLP 数据的不足。</p><p><strong>BERT 的作用：</strong><br>NLP 中，在无标号的大量数据集上训练的模型效果 &gt; 有标号、但数据量少一些的数据集上训练效果</p><p>CV 采用 BERT 的想法嘛？<br>Yes，在大量无标号的图片上训练的模型，可能比 有标号的 ImageNet 百万图片 效果更好。</p><hr><h3 id="BERT-模型"><a href="#BERT-模型" class="headerlink" title="BERT 模型"></a>BERT 模型</h3><p>BERT 有哪两步？预训练 + 微调<br>pre-training: 使用 unlabeled data 训练<br>fine-tuning: 微调的 BERT 使用 预训练的参数 初始化，所有的权重参数通过 下游任务的 labeled data 进行微调。<br>每一个下游任务会创建一个 新的 BERT 模型，（由预训练参数初始化），但每一个下游任务会根据自己任务的 labeled data 来微调自己的 BERT 模型。</p><p>预训练和微调不是 BERT 的创新，CV里用的比较多。</p><p><strong>作者关于预训练和微调的介绍 好吗？</strong><br>好！如果假设读者都知道论文的技术，而只一笔带过（给Ref），不太好。论文写作要自洽，简单的说明就好，避免读者不知道预训练和微调，增加理解文章的障碍。</p><img src="/5f1c011a/1.png"><p>预训练的输入：unlabelled sentence pair<br>训练 BERT 的权重</p><p>下游任务：创建同样的 BERT 的模型，权重的初始化值来自于 预训练好 的权重。<br>MNLI, NER, SQuAD 下游任务有 自己的 labeled data, 对 BERT 继续训练，得到各个下游任务自己的的 BERT 版本。</p><p><strong>Model Architecture</strong></p><p>multi-layer bidirectional Transformer encoder<br>一个多层双向 Transformer 的解码器，基于 transfomer 的论文和代码。</p><p>写作：第三章这里不讲可以；在第二章相关工作做一定的介绍, i.e., L H</p><p>模型调了哪 3 个参数?<br>L: transform blocks的个数<br>H: hidden size 隐藏层大小<br>A: 自注意力机制 multi-head 中 head 头的个数</p><p>调了 BERT_BASE （1亿参数）和 BERT_LARGE （3.4亿参数）</p><p>Large 模型 层数 L 翻倍 12 – 24；宽度 H 768 – 1024<br>BERT 模型复杂度和层数 L 是 linear, 和宽度 H 是 平方关系。<br>因为 深度 变成了 以前的两倍，在宽度上面也选择一个值，使得这个增加的平方大概是之前的两倍。</p><p>H &#x3D; 16，因为每个 head 的维度都固定在了64。因为你的宽度增加了，所以 head 数也增加了。</p><p>BERT_base 的参数选取 和 GPT 差不多，比较模型；BERT_large 刷榜。</p><p><strong>超参数换算成可学习参数的大小，transformer架构的回顾</strong></p><p>可学习参数的来源：嵌入层 30k * H、transformer块 L * H^2 * 12</p><p>嵌入层： 输入是词的字典大小 30k，输出是 H<br>参数：30k （字典大小） * H （hidden size）</p><p>嵌入层的输出会进入 transformer 块。</p><p>transformer blocks（H^2 * 12）: self-attention mechanism （H^2 * 4）+ MLP（H^2 * 8）</p><p>self-attention mechanism 本身无可学习参数; multi-head self-attention mechanism 要对 q, k, v 做投影，每一次投影维度&#x3D;64 –&gt; A * 64 &#x3D; H。<br>每一个 q, k, v 都有自己的投影矩阵，合并每个 head 的投影矩阵 –&gt; q, k, v 分别的 H * H 矩阵。</p><p>得到输出后还会有一次 H * H 的投影。</p><p>Transformer block 里的 self-attention 可学习参数 &#x3D; H^ 2 * 4</p><p>MLP 的 2个全连接层：<br>第一个全连接层输入是 H，输出是 4 * H；<br>第二个全连接层输入是 4 * H，输出是 H。</p><p>每一个参数矩阵大小 H * 4H，MLP 中的可学习参数 H^2 * 8</p><p>一个 transformer block 的参数量 H^2 * 12，L 个 blocks，L * H^2 * 12</p><p><strong>Input&#x2F;Output Representations</strong></p><p>下游任务有处理一个句子 or 处理 2 个句子，BERT 能处理不同句子数量的下游任务，使输入可以是 a single sentence and a pair of sentences (Question answer)</p><p>a single sentence: 一段连续的文字，不一定是真正上的语义上的一段句子，它是我的输入叫做一个序列 sequence。</p><p>A “sequence” 序列可以是一个句子，也可以是两个句子。</p><p>BERT 的输入和 transformer 区别？<br>transformer 预训练时候的输入是一个序列对。编码器和解码器分别会输入一个序列。<br>BERT 只有一个编码器，为了使 BERT 能处理两个句子的情况，需要把两个句子并成一个序列。</p><p><strong>BERT 如何切词？</strong></p><p>WordPiece, 把一个出现概率低的词切开，只保留一个词出现频率高的子序列，30k token 经常出现的词（子序列）的字典。<br>否则，空格切词 –&gt; 一个词是一个 token。数据量打的时候，词典会特别大，到百万级别。可学习的参数基本都在嵌入层了。</p><p>BERT 的输入序列如何构成？ [ CLS ] + [ SEP ]</p><p>序列开始: [ CLS ] 输出的是句子层面的信息 sequence representation<br>BERT 使用的是 transformer 的 encoder，self-attention layer 会看输入的每个词和其它所有词的关系。<br>就算 [ CLS ] 这个词放在我的第一个的位置，他也是有办法能看到之后所有的词。所以他放在第一个是没关系的，不一定要放在最后。</p><p>区分 两个合在一起的句子 的方法：</p><ul><li>每个句子后 + [ SEP ] 表示 seperate</li><li>学一个嵌入层 来表示 整个句子是第一句还是第二句</li></ul><p>[ CLS ] [Token1] …… [Token n] [SEP] [Token1’] …… [Token m]</p><p>每一个 token 进入 BERT 得到 这个 token 的embedding 表示。<br>对于 BERT，输入一个序列，输出一个序列。</p><p>最后一个 transformer 块的输出，表示 这个词源 token 的 BERT 的表示。在后面再添加额外的输出层，来得到想要的结果。</p><img src="/5f1c011a/2.png"><p>For a given token, 进入 BERT 的表示 &#x3D; token 本身的表示 + segment 句子的表示 + position embedding 位置表示</p><p>BERT 嵌入层：一个词源的序列 –&gt; 一个向量的序列 –&gt; 进入 transformer 块</p><p>Token embeddings: 词源的embedding层，整成的embedding层， 每一个 token 有对应的词向量。<br>Segement embeddings: 这个 token 属于第一句话 A还是第二句话 B。<br>Position embeddings: 输入的大小 &#x3D; 这个序列最长有多长？ i.e., 1024<br>Position embedding 的输入是 token 词源在这个序列 sequence 中的位置信息。从0开始 1 2 3 4 –&gt; 1024</p><img src="/5f1c011a/3.png"><p>BERT input representation &#x3D; token embeddings + segment embeddings + position embeddings</p><p>BERT 的 segment embedding （属于哪个句子）和 position embedding （位置在哪里）是学习得来的，transformer 的 position embedding 是给定的。</p><p>BERT 关于 pre-train 和 fine-tune 同样的部分 &#x3D;&#x3D; end</p><p><strong>3.1 Pre-training BERT</strong></p><p>预训练的 key factors: 目标函数，预训练的数据</p><p><strong>Task 1 MLM</strong><br>为什么 bidirectional 好？ MLM 是什么？完形填空</p><p>由 WordPiece 生成的词源序列中的词源，它有 15% 的概率会随机替换成一个掩码。但是对于特殊的词源不做替换，i.e., 第一个词源 [ CLS ] 和中间的分割词源 [SEP]。</p><p>如果输入序列长度是 1000 的话，要预测 150 个词。</p><p>MLM 带来的问题：<strong>预训练和微调看到的数据不一样</strong>。预训练的输入序列有 15% [MASK]，微调时的数据没有 [MASK].</p><p>15% 计划被 masked 的词: 80% 的概率被替换为 [MASK], 10% 换成 random token,10% 不改变原 token。但 T_i 还是被用来做预测。</p><p>80%, 10%, 10% 的选择，有 ablation study in appendix</p><p>unchanged 和 微调中的数据应该是一样的。</p><p><strong>Task 2 NSP Next Sentence Prediction</strong></p><p>在问答和自然语言推理里都是<strong>句子对</strong>。<br>如果 BERT 能学习到 sentence-level 信息，很棒。</p><p>输入序列有 2 个句子 A 和 B，50% 正例，50%反例<br>50% B 在 A 之后，50% 是 a random sentence 随机采样的。</p><p>正例：这个人要去一个商店，然后他买了一加仑的牛奶。IsNext<br>反例：这个人去了商店，然后企鹅是一种不能飞的鸟。NotNext</p><p>flight ## less, flightless 出现概率不高，WordPiece 分成了 2 个出现频率高的子序列，## 表示 less 是 flightless 的一部分。</p><p><strong>Pre-training data</strong></p><p>2 个数据集：BooksCorpus (800 M) + English Wikipedia (2500 M)<br>使用一篇一篇文章，而不是随机打断的句子。 a document-level corpus rather than a shuffled sentence-level corpus</p><p>transformer 可以处理较长的序列，一整个文本的输入，效果会好一些。</p><p><strong>3.2 Fine-tuning BERT</strong></p><p>用 BERT 做微调的一般化的介绍。</p><p>BERT 和一些基于encoder-decoder的架构为什么不一样？transformer 是encoder-decoder。</p><p>整个句子对被放在一起输入 BERT，self-attention 能够在两个句子之间相互看。BERT 更好，但代价是 不能像 transformer 做机器翻译。</p><p>在encoder-decoder的架构，编码器看不到解码器的东西。</p><p><strong>BERT 做 下游任务</strong></p><p>根据下游任务，设计我们任务相关的输入和输出。</p><p>好处：模型不怎么变，加一个输出层 softmax 得到 标号 label</p><p><strong>怎么样把输入改成想要的句子对？</strong></p><ul><li>有两个句子的话，当然就是句子 A 和 B。</li><li>只有一个句子的话，要做句子分类的话， B 没有。根据下游任务的要求，要么是 [CLS] representation is fed into an output layer for classification 拿到第一个词源 [CLS] 对应的输出做分类 such as entailment or sentiment analysis，或者是 the token representations are fed into an output layer for token-level tasks 拿到对应那些词源的输出做 sequence tagging or question answering 输出。</li></ul><p>微调比预训练便宜。TPU 1 hour, GPU a few hours.</p><p><strong>Section 4 具体对每一个下游任务是怎么样构造输入输出</strong></p><hr><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><strong>4.1 GLUE General Language Understanding Evaluation</strong></p><ul><li>多个数据集</li><li>sentence-level tasks</li></ul><p>[CLS] 的 BERT 输出表示 + 一个输出层 W，softmax 分类得到 label<br>log(softmax(CW^T)</p><p><strong>4.2 SQuAD v1.1</strong><br>Standford Question Answering Dataset</p><p>QA 问答：给一段文字，问一个问题，摘录答案。–&gt; 判断答案的开始和结尾。<br>对每个词源 token，判断是不是答案的开始or结尾</p><p>学 2 个向量 S 和 E，分别对应这个词源 token 是答案开始词的概率 和 是答案结尾词的概率。</p><p>具体计算 每个 token 是答案开始的概率，结尾词类似 E。<br>S 和 第二句话的每个词源 token 相乘 + softmax，得到归一化的概率。<br>P_i &#x3D; e ^ ( S * T_i ) &#x2F; \sigma_j ( e ^ ( S * T_j ) )</p><p>本文微调时，数据扫三遍，epochs &#x3D; 3, lr &#x3D; 5e-5, batch_size &#x3D; 32</p><p>大家实验发现：用 BERT 做微调的时候，结果非常不稳定。同样的参数，同样的数据集，训练 10 遍，variance 方差特别大。</p><p>其实很简单，epochs 不够，3 太小了，可能要多学习几遍会好一点。</p><p>adam 的不完全版 在长时间训练的 BERT 没问题，训练时间不够，需要 adam 的完全版。</p><p><strong>4.3 SQuAD v2.0 表现也很不错</strong></p><p><strong>4.4 SWAG</strong></p><p>Situations With Adversarial Generations 判断两个句子之间的关系，BERT 和之前的训练没多大区别，效果好。</p><p>总结：BERT 在不一样的数据集上，用起来很方便，效果很好。<br>输入表示成“一对句子的形式”，最后拿到 BERT 对应的输出，然后加一个输出层 softmax，完事了。</p><p>BERT 对 NLP 整个领域的贡献非常大，有大量的任务用一个相对简单、只改数据输入形式和最后加一个输出层，就可以效果很不错。</p><p><strong>5 Ablation studies</strong></p><p>看 BERT 每一个组成部分的贡献。</p><p>没有 NSP<br>LTR 从左看到右（无 MLM ） &amp; 没有 NSP<br>LTR 从左看到右（无 MLM ） &amp; 没有 NSP + BiLSTM （从ELMo来的想法）</p><p>去掉任何一个组成部分，BERT的效果都会有打折，特别是 MRPC。</p><p><strong>5.2 Effect of Model Size</strong></p><p>BERT_base 110 M 可学习参数<br>BERT_large 340 M 可学习参数</p><p>NLP界认为 模型越大，效果越好。BERT 首先证明了大力出奇迹，引发了模型“大”战</p><p>现在：GPT-3 1000 亿可学习参数</p><p><strong>5.3 Feature-based Approach with BERT</strong></p><p>没有微调的 BERT，将pre-trained 得到的 BERT 特征作为一个静态的特征输入，效果没有 + 微调好</p><p>卖点：用 BERT 需要微调。</p><hr><h3 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h3><p>写作：</p><ul><li>先写 BERT 和 ELMo (bidirectional + RNN)、GPT (unidirectional + transformer) 的区别</li><li>介绍 BERT 模型</li><li>BERT 实验设置、效果好</li><li>结论突出 ‘bidirectional’ 贡献</li><li>文章 1个卖点，容易记。</li></ul><p><strong>但 BERT 是否要选择 ‘bidirectional’ 双向性呢？</strong><br>可以写，但也要写 双向性带来的不足是什么？</p><p>选择有得有失。<br>GPT 用的是 decoder<br>BERT 用的是 encoder，不好做generative tasks：机器翻译、文本摘要。</p><p>分类问题在 NLP 更常见。<br>NLP 研究者喜欢 BERT，较容易的应用在 NLP 中自己想解决的问题。</p><p>BERT，完整的解决问题的思路 —- 大家对 DL 的期望<br>训练一个很深、很宽的模型，在一个很大的数据集上预训练好；训练好的模型参数可以解决很多小的问题，通过微调提升小数据集上的性能。</p><p>这个模型拿出来之后可以用在很多小的问题上，能够通过微调来全面提升这些小数据上的性能。这个在计算机视觉里面我们用了很多年了。</p><p>BERT 把 CV 的套路搬到了 NLP，1个3亿参数的模型，展示：模型越大、效果越好。大力出奇迹。</p><p>为什么 BERT 被记住？<br>BERT 用了 ELMo, GPT 更大的训练数据集，效果更好；BERE 也被更大的训练数据集和更大的模型超越。<br>BERT 的引用率是 GPT 的 10 倍，影响力 ✔</p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/6c3d1ed6.html" rel="prev" title="机器学习-什么是SVM支持向量机？"><i class="fa fa-chevron-left"></i> 机器学习-什么是SVM支持向量机？</a></div><div class="post-nav-item"><a href="/e921b18a.html" rel="next" title="软件硬件-固态硬盘SSD是如何存储数据的？">软件硬件-固态硬盘SSD是如何存储数据的？ <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">原文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%A2%98-%E4%BD%9C%E8%80%85"><span class="nav-number">2.</span> <span class="nav-text">标题 + 作者</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">3.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E8%A8%80"><span class="nav-number">4.</span> <span class="nav-text">导言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">5.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">6.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.</span> <span class="nav-text">BERT 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">8.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E8%AE%BA"><span class="nav-number">9.</span> <span class="nav-text">评论</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">513</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">66:17</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>