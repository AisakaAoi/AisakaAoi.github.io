<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"example.com",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="英国赫特福德大学与 GBG Plc 的研究者近日发布了一篇综述论文，对人脸识别方法进行了全面的梳理和总结，其中涵盖各种传统方法和如今风头正盛的深度学习方法。机器之心重点编译介绍了其中的深度学习方法部分，更多有关传统人脸识别方法的内容请参阅原论文。   论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.00116 自七十年代以来，人脸识别已经成为了计算机视觉和生物识别领域被研究最多的主"><meta property="og:type" content="article"><meta property="og:title" content="人脸识别-人脸识别技术全面总结：从传统方法到深度学习"><meta property="og:url" content="http://example.com/3373677a.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="英国赫特福德大学与 GBG Plc 的研究者近日发布了一篇综述论文，对人脸识别方法进行了全面的梳理和总结，其中涵盖各种传统方法和如今风头正盛的深度学习方法。机器之心重点编译介绍了其中的深度学习方法部分，更多有关传统人脸识别方法的内容请参阅原论文。   论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.00116 自七十年代以来，人脸识别已经成为了计算机视觉和生物识别领域被研究最多的主"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/3373677a/1.jpg"><meta property="og:image" content="http://example.com/3373677a/2.jpg"><meta property="og:image" content="http://example.com/3373677a/3.png"><meta property="og:image" content="http://example.com/3373677a/4.jpg"><meta property="og:image" content="http://example.com/3373677a/5.jpg"><meta property="og:image" content="http://example.com/3373677a/6.jpg"><meta property="og:image" content="http://example.com/3373677a/7.jpg"><meta property="og:image" content="http://example.com/3373677a/8.jpg"><meta property="og:image" content="http://example.com/3373677a/9.jpg"><meta property="og:image" content="http://example.com/3373677a/10.jpg"><meta property="og:image" content="http://example.com/3373677a/11.jpg"><meta property="article:published_time" content="2022-04-13T21:15:39.000Z"><meta property="article:modified_time" content="2023-06-07T23:41:23.919Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/3373677a/1.jpg"><link rel="canonical" href="http://example.com/3373677a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>人脸识别-人脸识别技术全面总结：从传统方法到深度学习 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">逢坂葵葵</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">52</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">507</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://example.com/3373677a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">人脸识别-人脸识别技术全面总结：从传统方法到深度学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-04-14 05:15:39" itemprop="dateCreated datePublished" datetime="2022-04-14T05:15:39+08:00">2022-04-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/%F0%9F%92%AB%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">💫计算机视觉 Computer Vision</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/%F0%9F%92%AB%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Computer-Vision/%F0%9F%9B%B0%EF%B8%8F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-Face-Recognition/" itemprop="url" rel="index"><span itemprop="name">🛰️人脸识别 Face Recognition</span></a> </span></span><span id="/3373677a.html" class="post-meta-item leancloud_visitors" data-flag-title="人脸识别-人脸识别技术全面总结：从传统方法到深度学习" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/3373677a.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/3373677a.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>7.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>18 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>英国赫特福德大学与 GBG Plc 的研究者近日发布了一篇综述论文，对人脸识别方法进行了全面的梳理和总结，其中涵盖各种传统方法和如今风头正盛的深度学习方法。机器之心重点编译介绍了其中的深度学习方法部分，更多有关传统人脸识别方法的内容请参阅原论文。</p><img src="/3373677a/1.jpg"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.00116">https://arxiv.org/abs/1811.00116</a></p><p>自七十年代以来，人脸识别已经成为了计算机视觉和生物识别领域被研究最多的主题之一。基于人工设计的特征和传统机器学习技术的传统方法近来已被使用非常大型的数据集训练的深度神经网络取代。在这篇论文中，我们对流行的人脸识别方法进行了全面且最新的文献总结，其中既包括传统方法（基于几何的方法、整体方法、基于特征的方法和混合方法），也有深度学习方法。</p><span id="more"></span><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>人脸识别是指能够识别或验证图像或视频中的主体的身份的技术。首个人脸识别算法诞生于七十年代初 [1,2]。自那以后，它们的准确度已经大幅提升，现在相比于指纹或虹膜识别 [3] 等传统上被认为更加稳健的生物识别方法，人们往往更偏爱人脸识别。让人脸识别比其它生物识别方法更受欢迎的一大不同之处是人脸识别本质上是非侵入性的。比如，指纹识别需要用户将手指按在传感器上，虹膜识别需要用户与相机靠得很近，语音识别则需要用户大声说话。相对而言，现代人脸识别系统仅需要用户处于相机的视野内（假设他们与相机的距离也合理）。这使得人脸识别成为了对用户最友好的生物识别方法。这也意味着人脸识别的潜在应用范围更广，因为它也可被部署在用户不期望与系统合作的环境中，比如监控系统中。人脸识别的其它常见应用还包括访问控制、欺诈检测、身份认证和社交媒体。</p><p>当被部署在无约束条件的环境中时，由于人脸图像在现实世界中的呈现具有高度的可变性（这类人脸图像通常被称为自然人脸（faces in-the-wild）），所以人脸识别也是最有挑战性的生物识别方法之一。人脸图像可变的地方包括头部姿势、年龄、遮挡、光照条件和人脸表情。图 1 给出了这些情况的示例。</p><img src="/3373677a/2.jpg"><div align="center">图 1：在自然人脸图像中找到的典型变化。（a）头部姿势，（b）年龄，（c）光照，（d）面部表情，（e）遮挡。</div><p>人脸识别技术这些年已经发生了重大的变化。传统方法依赖于人工设计的特征（比如边和纹理描述量）与机器学习技术（比如主成分分析、线性判别分析或支持向量机）的组合。人工设计在无约束环境中对不同变化情况稳健的特征是很困难的，这使得过去的研究者侧重研究针对每种变化类型的专用方法，比如能应对不同年龄的方法 [4,5]、能应对不同姿势的方法 [6]、能应对不同光照条件的方法 [7,8] 等。近段时间，传统的人脸识别方法已经被基于卷积神经网络（CNN）的深度学习方法接替。深度学习方法的主要优势是它们可用非常大型的数据集进行训练，从而学习到表征这些数据的最佳特征。网络上可用的大量自然人脸图像已让研究者可收集到大规模的人脸数据集 [9-15]，这些图像包含了真实世界中的各种变化情况。使用这些数据集训练的基于 CNN 的人脸识别方法已经实现了非常高的准确度，因为它们能够学到人脸图像中稳健的特征，从而能够应对在训练过程中使用的人脸图像所呈现出的真实世界变化情况。此外，深度学习方法在计算机视觉方面的不断普及也在加速人脸识别研究的发展，因为 CNN 也正被用于解决许多其它计算机视觉任务，比如目标检测和识别、分割、光学字符识别、面部表情分析、年龄估计等。</p><p>人脸识别系统通常由以下构建模块组成：</p><ul><li><strong>人脸检测</strong>。人脸检测器用于寻找图像中人脸的位置，如果有人脸，就返回包含每张人脸的边界框的坐标。如图 3a 所示。</li><li><strong>人脸对齐</strong>。人脸对齐的目标是使用一组位于图像中固定位置的参考点来缩放和裁剪人脸图像。这个过程通常需要使用一个特征点检测器来寻找一组人脸特征点，在简单的 2D 对齐情况中，即为寻找最适合参考点的最佳仿射变换。图 3b 和 3c 展示了两张使用了同一组参考点对齐后的人脸图像。更复杂的 3D 对齐算法（如 [16]）还能实现人脸正面化，即将人脸的姿势调整到正面向前。</li><li><strong>人脸表征</strong>。在人脸表征阶段，人脸图像的像素值会被转换成紧凑且可判别的特征向量，这也被称为模板（template）。理想情况下，同一个主体的所有人脸都应该映射到相似的特征向量。</li><li><strong>人脸匹配</strong>。在人脸匹配构建模块中，两个模板会进行比较，从而得到一个相似度分数，该分数给出了两者属于同一个主体的可能性。</li></ul><img src="/3373677a/3.png"><div align="center">图 2：人脸识别的构建模块。。</div><p>很多人认为人脸表征是人脸识别系统中最重要的组件，这也是本论文第二节所关注的重点。</p><img src="/3373677a/4.jpg"><div align="center">图 3：（a）人脸检测器找到的边界框。（b）和（c）：对齐后的人脸和参考点。</div><hr><h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><p>卷积神经网络（CNN）是人脸识别方面最常用的一类深度学习方法。深度学习方法的主要优势是可用大量数据来训练，从而学到对训练数据中出现的变化情况稳健的人脸表征。这种方法不需要设计对不同类型的类内差异（比如光照、姿势、面部表情、年龄等）稳健的特定特征，而是可以从训练数据中学到它们。深度学习方法的主要短板是它们需要使用非常大的数据集来训练，而且这些数据集中需要包含足够的变化，从而可以泛化到未曾见过的样本上。幸运的是，一些包含自然人脸图像的大规模人脸数据集已被公开 [9-15]，可被用来训练 CNN 模型。除了学习判别特征，神经网络还可以降维，并可被训练成分类器或使用度量学习方法。CNN 被认为是端到端可训练的系统，无需与任何其它特定方法结合。</p><p>用于人脸识别的 CNN 模型可以使用不同的方法来训练。其中之一是将该问题当作是一个分类问题，训练集中的每个主体都对应一个类别。训练完之后，可以通过去除分类层并将之前层的特征用作人脸表征而将该模型用于识别不存在于训练集中的主体 [99]。在深度学习文献中，这些特征通常被称为瓶颈特征（bottleneck features）。在这第一个训练阶段之后，该模型可以使用其它技术来进一步训练，以为目标应用优化瓶颈特征（比如使用联合贝叶斯 [9] 或使用一个不同的损失函数来微调该 CNN 模型 [10]）。另一种学习人脸表征的常用方法是通过优化配对的人脸 [100,101] 或人脸三元组 [102] 之间的距离度量来直接学习瓶颈特征。</p><p>使用神经网络来做人脸识别并不是什么新思想。1997 年就有研究者为人脸检测、眼部定位和人脸识别提出了一种名为「基于概率决策的神经网络（PBDNN）」[103] 的早期方法。这种人脸识别 PDBNN 被分成了每一个训练主体一个全连接子网络，以降低隐藏单元的数量和避免过拟合。研究者使用密度和边特征分别训练了两个 PBDNN，然后将它们的输出组合起来得到最终分类决定。另一种早期方法 [104] 则组合使用了自组织映射（SOM）和卷积神经网络。自组织映射 [105] 是一类以无监督方式训练的神经网络，可将输入数据映射到更低维的空间，同时也能保留输入空间的拓扑性质（即在原始空间中相近的输入在输出空间中也相近）。注意，这两种早期方法都不是以端到端的方式训练的（[103] 中使用了边特征，[104] 中使用了 SOM），而且提出的神经网络架构也都很浅。[100] 中提出了一种端到端的人脸识别 CNN。这种方法使用了一种孪生式架构，并使用了一个对比损失函数 [106] 来进行训练。这个对比损失使用了一种度量学习流程，其目标是最小化对应同一主体的特征向量对之间的距离，同时最大化对应不同主体的特征向量对之间的距离。该方法中使用的 CNN 架构也很浅，且训练数据集也较小。</p><p>上面提到的方法都未能取得突破性的成果，主要原因是使用了能力不足的网络，且训练时能用的数据集也相对较小。直到这些模型得到扩展并使用大量数据 [107] 训练后，用于人脸识别的首个深度学习方法 [99,9] 才达到了当前最佳水平。尤其值得一提的是 Facebook 的 DeepFace [99]，这是最早的用于人脸识别的 CNN 方法之一，其使用了一个能力很强的模型，在 LFW 基准上实现了 97.35% 的准确度，将之前最佳表现的错误率降低了 27%。研究者使用 softmax 损失和一个包含 440 万张人脸（来自 4030 个主体）的数据集训练了一个 CNN。本论文有两个全新的贡献：（1）一个基于明确的 3D 人脸建模的高效的人脸对齐系统；（2）一个包含局部连接的层的 CNN 架构 [108,109]，这些层不同于常规的卷积层，可以从图像中的每个区域学到不同的特征。在那同时，DeepID 系统 [9] 通过在图块（patch）上训练 60 个不同的 CNN 而得到了相近的结果，这些图块包含十个区域、三种比例以及 RGB 或灰度通道。在测试阶段，会从每个图块提取出 160 个瓶颈特征，加上其水平翻转后的情况，可形成一个 19200 维的特征向量（160×2×60）。类似于 [99]，新提出的 CNN 架构也使用了局部连接的层。其验证结果是通过在这种由 CNN 提取出的 19200 维特征向量上训练一个联合贝叶斯分类器 [48] 得到的。训练该系统所使用的数据集包含 202599 张人脸图像，来自 10177 位名人 [9]。</p><p>对于基于 CNN 的人脸识别方法，影响准确度的因素主要有三个：训练数据、CNN 架构和损失函数。因为在大多数深度学习应用中，都需要大训练集来防止过拟合。一般而言，为分类任务训练的 CNN 的准确度会随每类的样本数量的增长而提升。这是因为当类内差异更多时，CNN 模型能够学习到更稳健的特征。但是，对于人脸识别，我们感兴趣的是提取出能够泛化到训练集中未曾出现过的主体上的特征。因此，用于人脸识别的数据集还需要包含大量主体，这样模型也能学习到更多类间差异。[110] 研究了数据集中主体的数量对人脸识别准确度的影响。在这项研究中，首先以降序形式按照每个主体的图像数量对一个大数据集进行了排序。然后，研究者通过逐渐增大主体数量而使用训练数据的不同子集训练了一个 CNN。当使用了图像数量最多的 10000 个主体进行训练时，得到的准确度是最高的。增加更多主体会降低准确度，因为每个额外主体可用的图像非常少。另一项研究 [111] 研究了更宽度的数据集更好，还是更深度的数据集更好（如果一个数据集包含更多主体，则认为它更宽；类似地，如果每个主体包含的图像更多，则认为它更深）。这项研究总结到：如果图像数量相等，则更宽的数据集能得到更好的准确度。研究者认为这是因为更宽度的数据集包含更多类间差异，因而能更好地泛化到未曾见过的主体上。表 1 展示了某些最常用于训练人脸识别 CNN 的公开数据集。</p><img src="/3373677a/5.jpg"><div align="center">表 1：公开的大规模人脸数据集。</div><p>用于人脸识别的 CNN 架构从那些在 ImageNet 大规模视觉识别挑战赛（ILSVRC）上表现优异的架构上取得了很多灵感。举个例子，[11] 中使用了一个带有 16 层的 VGG 网络 [112] 版本，[10] 中则使用了一个相似但更小的网络。[102] 中探索了两种不同类型的 CNN 架构：VGG 风格的网络 [112] 和 GoogleNet 风格的网络 [113]。即使这两种网络实现了相当的准确度，但 GoogleNet 风格的网络的参数数量少 20 倍。更近段时间，残差网络（ResNet）[114] 已经成为了很多目标识别任务的最受偏爱的选择，其中包括人脸识别 [115-121]。ResNet 的主要创新点是引入了一种使用捷径连接的构建模块来学习残差映射，如图 7 所示。捷径连接的使用能让研究者训练更深度的架构，因为它们有助于跨层的信息流动。[121] 对不同的 CNN 架构进行了全面的研究。在准确度、速度和模型大小之间的最佳权衡是使用带有一个残差模块（类似于 [122] 中提出的那种）的 100 层 ResNet 得到的。</p><img src="/3373677a/6.jpg"><div align="center">图 7：[114] 中提出的原始的残差模块。</div><p>选择用于训练 CNN 方法的损失函数已经成为近来人脸识别最活跃的研究领域。即使使用 softmax 损失训练的 CNN 已经非常成功 [99,9,10,123]，但也有研究者认为使用这种损失函数无法很好地泛化到训练集中未出现过的主体上。这是因为 softmax 损失有助于学习能增大类间差异的特征（以便在训练集中区别不同的类），但不一定会降低类内差异。研究者已经提出了一些能缓解这一问题的方法。优化瓶颈特征的一种简单方法是使用判别式子空间方法，比如联合贝叶斯 [48]，就像 [9,124,125,126,10,127] 中所做的那样。另一种方法是使用度量学习。比如，[100,101] 中使用了配对的对比损失来作为唯一的监督信号，[124-126] 中还结合使用了分类损失。人脸识别方面最常用的度量学习方法是三元组损失函数 [128]，最早在 [102] 中被用于人脸识别任务。三元组损失的目标是以一定余量分开正例对之间的距离和负例对之间的距离。从数学形式上讲，对于每个三元组 i，需要满足以下条件 [102]：</p><img src="/3373677a/7.jpg"><p>其中 x_a 是锚图像，x_p 是同一主体的图像，x_n 是另一个不同主体的图像，f 是模型学习到的映射关系，α 施加在正例对和负例对距离之间的余量。在实践中，使用三元组损失训练的 CNN 的收敛速度比使用 softmax 的慢，这是因为需要大量三元组（或对比损失中的配对）才能覆盖整个训练集。尽管这个问题可以通过在训练阶段选择困难的三元组（即违反余量条件的三元组）来缓解 [102]，但常见的做法是在第一个训练阶段使用 softmax 损失训练，在第二个训练阶段使用三元组损失来对瓶颈特征进行调整 [11,129,130]。研究者们已经提出了三元组损失的一些变体。比如 [129] 中使用了点积作为相似度度量，而不是欧几里德距离；[130] 中提出了一种概率式三元组损失；[131,132] 中提出了一种修改版的三元组损失，它也能最小化正例和负例分数分布的标准差。用于学习判别特征的另一种损失函数是 [133] 中提出的中心损失（centre loss）。中心损失的目标是最小化瓶颈特征与它们对应类别的中心之间的距离。通过使用 softmax 损失和中心损失进行联合训练，结果表明 CNN 学习到的特征能够有效增大类间差异（softmax 损失）和降低类内个体差异（中心损失）。相比于对比损失和三元组损失，中心损失的优点是更高效和更容易实现，因为它不需要在训练过程中构建配对或三元组。另一种相关的度量学习方法是 [134] 中提出的范围损失（range loss），这是为改善使用不平衡数据集的训练而提出的。范围损失有两个组件。类内的损失组件是最小化同一类样本之间的 k-最大距离，而类间的损失组件是最大化每个训练批中最近的两个类中心之间的距离。通过使用这些极端案例，范围损失为每个类都使用同样的信息，而不管每个类别中有多少样本可用。类似于中心损失，范围损失需要与 softmax 损失结合起来以避免损失降至零 [133]。</p><p>当结合不同的损失函数时，会出现一个困难，即寻找每一项之间的正确平衡。最近一段时间，已有研究者提出了几种修改 softmax 损失的方法，这样它无需与其它损失结合也能学习判别特征。一种已被证明可以增加瓶颈特征的判别能力的方法是特征归一化 [115,118]。比如，[115] 提出归一化特征以具有单位 L2 范数，[118] 提出归一化特征以具有零均值和单位方差。一个成功的方法已经在 softmax 损失中每类之间的决策边界中引入了一个余量 [135]。为了简单，我们介绍一下使用 softmax 损失进行二元分类的情况。在这种情况下，每类之间的决策边界（如果偏置为零）可由下式给定：</p><img src="/3373677a/8.jpg"><p>其中 x 是特征向量，W_1 和 W_2 是对应每类的权重，θ_1 和 θ_2 是 x 分别与 W_1 和 W_2 之间的角度。通过在上式中引入一个乘法余量，这两个决策边界可以变得更加严格：</p><img src="/3373677a/9.jpg"><p>如图 8 所示，这个余量可以有效地增大类别之间的区分程度以及各自类别之内的紧凑性。根据将该余量整合进损失的方式，研究者们已经提出了多种可用方法 [116,119-121]。比如 [116] 中对权重向量进行了归一化以具有单位范数，这样使得决策边界仅取决于角度 θ_1 和 θ_2。[119,120] 中则提出了一种加性余弦余量。相比于乘法余量 [135,116]，加性余量更容易实现和优化。在这项工作中，除了归一化权重向量，特征向量也如 [115] 中一样进行了归一化和比例调整。[121] 中提出了另一种加性余量，它既有 [119,120] 那样的优点，还有更好的几何解释方式，因为这个余量是加在角度上的，而不是余弦上。表 2 总结了有余量的 softmax 损失的不同变体的决策边界。这些方法是人脸识别领域的当前最佳。</p><img src="/3373677a/10.jpg"><div align="center">图 8：在两个类别之间的决策边界中引入一个余量 m 的效果。（a）softmax 损失，（b）有余量的 softmax 损失。</div><img src="/3373677a/11.jpg"><div align="center">表 2：有余量的 softmax 损失的不同变体的决策边界。注意这些决策边界针对的是二元分类案例中的类别 1。</div><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247591532&amp;idx=2&amp;sn=e6f351783723a84ee055c7a606815871">https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247591532&amp;idx=2&amp;sn=e6f351783723a84ee055c7a606815871</a></p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/bfcd8a7b.html" rel="prev" title="脑机接口与混合智能-新闻-睡眠纺锤波可作为神经退行性疾病检测的生物标志物"><i class="fa fa-chevron-left"></i> 脑机接口与混合智能-新闻-睡眠纺锤波可作为神经退行性疾病检测的生物标志物</a></div><div class="post-nav-item"><a href="/a5c1ffe6.html" rel="next" title="论文阅读-ViT论文再整理">论文阅读-ViT论文再整理 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">深度学习方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">3.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">507</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/AisakaManatsu" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaManatsu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">65:45</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>