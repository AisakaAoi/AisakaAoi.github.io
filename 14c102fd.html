<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="对比学习综述发展历程大概可以分为四个阶段：  百花齐放  InstDisc（instance discrimination） CPC CMC 在这个阶段中，方法、模型、目标函数、代理任务都还没有统一，所以说是一个百花齐放的时代   CV双雄  MoCo v1 SimCLR v1 MoCo v2 SimCLR v2 CPC、CMC的延伸工作 SwAV 这个阶段发展非常迅速，上述工作有的间隔一两个月，"><meta property="og:type" content="article"><meta property="og:title" content="对比学习论文综述-1百花齐放"><meta property="og:url" content="https://aisakaaoi.github.io/14c102fd.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="对比学习综述发展历程大概可以分为四个阶段：  百花齐放  InstDisc（instance discrimination） CPC CMC 在这个阶段中，方法、模型、目标函数、代理任务都还没有统一，所以说是一个百花齐放的时代   CV双雄  MoCo v1 SimCLR v1 MoCo v2 SimCLR v2 CPC、CMC的延伸工作 SwAV 这个阶段发展非常迅速，上述工作有的间隔一两个月，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/14c102fd/9.webp"><meta property="article:published_time" content="2021-12-31T06:19:25.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:39.735Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/14c102fd/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/14c102fd.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>对比学习论文综述-1百花齐放 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1026</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/14c102fd.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">对比学习论文综述-1百花齐放</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-12-31 14:19:25" itemprop="dateCreated datePublished" datetime="2021-12-31T14:19:25+08:00">2021-12-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">⭐论文带读</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E8%AE%BA%E6%96%87%E5%B8%A6%E8%AF%BB/%F0%9F%92%AB%E7%B2%BE%E8%AF%BB%E7%BB%8F%E5%85%B8/" itemprop="url" rel="index"><span itemprop="name">💫精读经典</span></a> </span></span><span id="/14c102fd.html" class="post-meta-item leancloud_visitors" data-flag-title="对比学习论文综述-1百花齐放" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/14c102fd.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/14c102fd.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>15 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="对比学习综述"><a href="#对比学习综述" class="headerlink" title="对比学习综述"></a>对比学习综述</h3><p>发展历程大概可以分为四个阶段：</p><ol><li><p>百花齐放</p><ul><li>InstDisc（instance discrimination）</li><li>CPC</li><li>CMC</li><li>在这个阶段中，方法、模型、目标函数、代理任务都还没有统一，所以说是一个百花齐放的时代</li></ul></li><li><p>CV双雄</p><ul><li>MoCo v1</li><li>SimCLR v1</li><li>MoCo v2</li><li>SimCLR v2</li><li>CPC、CMC的延伸工作</li><li>SwAV</li><li>这个阶段发展非常迅速，上述工作有的间隔一两个月，有的间隔甚至不到一个月，ImageNet上的成绩基本上每个月都在被刷新</li></ul></li><li><p>不用负样本</p><ul><li>BYOL以及它后续的一些改进</li><li>最后SimSiam将所有的方法都归纳总结了一下，融入到了SimSiam的框架中，基本上是用卷积神经网络做对比学习的一个总结性工作</li></ul></li><li><p>transformer</p><ul><li>MoCo v3</li><li>DINO</li><li>对于自监督学习来说，无论是对比学习还是最新的掩码学习，都是用Vision Transformer做的</li></ul></li></ol><p>这里只是把最有联系的一些工作串到一起，讲述他们的相似之处和不同之处</p><span id="more"></span><hr><iframe src="//player.bilibili.com/player.html?aid=680170801&bvid=BV19S4y1M7hm&cid=472587940&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen></iframe><hr><h3 id="百花齐放"><a href="#百花齐放" class="headerlink" title="百花齐放"></a>百花齐放</h3><h4 id="《Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination》"><a href="#《Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination》" class="headerlink" title="《Unsupervised Feature Learning via Non-Parametric Instance Discrimination》"></a>《Unsupervised Feature Learning via Non-Parametric Instance Discrimination》</h4><p>InstDisc（instance discrimination）</p><p>这篇论文提出了个体判别任务以及memory bank</p><img src="/14c102fd/1.webp"><p>图1：本文方法受到有监督学习结果的启发，如果将一张豹子的图片喂给一个已经用有监督学习方式训练好的分类器，会发现他给出来的分类结果排名前几的全都是跟豹子相关的，有猎豹、雪豹，总之从图片来看这些物体都是长非常相近的；而排名靠后的那些判断往往是跟豹子一点关系都没有的类别</p><p>通过很多这样的现象，作者觉得让这些图片聚集在一起的原因并不是因为它们有相似的语义标签，而是因为这些照片长得太像了，某一些 object 就是很相似，它们跟另外一些 object 就是不相似，所以才会导致有些分类分数都很高，而有些分数非常低</p><p>最后作者根据这个观察提出了个体判别任务：无监督的学习方式就是把按照类别走的有监督信号推到了极致，现在把每一个 instance 都看成是一个类别，也就是每一张图片都看作是一个类别，目标是能学一种特征能把每一个图片都区分开来</p><p>所以图一画的很好，起到了一石二鸟的作用：不仅很简单的介绍了研究动机，自然而然地引入了问题，而且用一句话的形式引入了个体判别这个代理任务</p><img src="/14c102fd/2.webp"><p>图二讲述了文章中的方法</p><ul><li>通过一个卷积神经网络把所有的图片都编码成一个特征（这些特征在最后的特征空间里能够尽可能的分开，因为对于个体判别任务来说每个图片都是自己的类，所以说每个图片都应该和别的图片尽量的分开）</li><li>训练这个卷积神经网络使用的是对比学习，所以需要有正样本和负样本，根据个体判别这个任务，正样本就是这个图片本身（可能经过一些数据增强），负样本就是数据集里所有其它的图片</li><li>做对比学习，大量的负样本特征到底应该存在哪呢？本文用了 memory bank 的形式：就是说把所有图片的特征全都存到 memory bank 里，也就是一个字典（ImageNet数据集有128万的图片，也就是说memory bank里要存128万行，也就意味着每个特征的维度不能太高，否则存储代价太大了，本文用的是128维）</li></ul><p>前向过程：</p><ul><li>假如batch size是256，也就是说有256个图片进入到编码器中，通过一个 Res 50，最后的特征维度是2048维，然后把它降维降到128维，这就是每个图片的特征大小</li><li>batch size 是 256 的话也就意味着有256个正样本，那负样本从哪来呢？自然是从 memory bank 里随机地抽一些负样本出来。本文抽了4,096个负样本出来</li><li>有了正样本也有了负样本，就可以用NCE loss 计算对比学习的目标函数</li><li>一旦更新完这个网络，就可以把 mini batch里的数据样本所对应的那些特征，在 memory bank 里更换掉，这样 memory bank 就得到了更新</li><li>接下来就是反复这个过程，不停的去更新这个编码 t，不停的更新这个 memory bank，最后学到这个特征尽可能的有区分性</li></ul><p>本文的方法还有很多细节都设计的非常巧妙</p><ul><li>比如说 proximal regularization：它给模型的训练加了一个约束，从而能让 memory bank 里的那些特征进行动量式的更新，跟 MoCo 的想法是非常一致的</li></ul><img src="/14c102fd/3.webp"><p>另外设置里面超参数的设定，比如说算 loss 的时候温度的设置是0.07，选了4,000个负样本，训练是200个epochs，batch size 是256，起始的 learning rate 是0.03，之后其它论文（尤其是 MoCo）所有的这些实验细节，MoCo 都是严格按照 Inst Disc 来的，这些超参数都没有进行更改。<strong>所以说 Inst Disc 这篇论文也是一个里程碑式的工作：它不仅提出了个体判别这个代理任务，而且用这个代理任务和 NCE loss 做对比学习，从而取得了不错的无监督表征学习的结果，同时它还提出了用别的数据结构存储这种大量的负样本，以及如何对特征进行动量的更新，所以真的是对后来对比学习的工作起到了至关重要的推进作用。</strong></p><img src="/14c102fd/4.webp"><hr><h4 id="《Unsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature》"><a href="#《Unsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature》" class="headerlink" title="《Unsupervised Embedding Learning via Invariant and Spreading Instance Feature》"></a>《Unsupervised Embedding Learning via Invariant and Spreading Instance Feature》</h4><img src="/14c102fd/5.webp"><p>这是一篇 CVPR 19 的论文，跟其它论文相比，它的影响力可能不是那么大，之所以提一下这篇论文，是因为它可以被理解成是 SimCLR 的一个前身。它没有使用额外的数据结构去存储大量的负样本，正负样本就是来自于同一个 mini-batch，而且它只用一个编码器进行端到端的学习。</p><p><strong>这篇文章也没有给自己起名字，所以就像 Inst Disc 一样就叫它 Inva Spread 好了，所以写论文的时候，最好还是给自己的方法起个名字，而不是叫 ours，这样方便别人记住也方便别人引用，也是一个双赢的事情。</strong></p><p>本文的想法其实就是最基本的对比学习</p><img src="/14c102fd/6.webp"><p>如图1所示，同样的图片通过编码器以后它的特征应该很类似，不同的图片它的特征出来就应该不类似，这就是题目中说的 invariant 和 spreading，就是说对于相似的图片、物体，特征应该保持不变性，但是对于不相似的物体或者完全不沾边的物体，特征应该尽可能的分散开。</p><p>具体做法：</p><p>代理任务也是选取了个体判别这个任务</p><img src="/14c102fd/7.webp"><p>前向过程：</p><ul><li>如果 batch size 是256，也就是说一共有256个图片，经过数据增强，又得到了256张图片</li><li>对于 x1 这张图片来说， x1’ 就是它的正样本，它的负样本是所有剩下的这些图片（包括原始的图片以及经过数据增强后的图片），也就是说正样本是256，负样本是(256 - 1) * 2，就是除去样本本身之外 mini-batch 剩下的所有样本以及它经过数据增强后的样本</li><li>它和 Inst Disc 的区别：Inst Disc中，正样本虽然是256，它的负样本是从一个 memory bank 里抽出来的，它用的负样本是4096甚至还可以更大</li><li>本文为什么要从同一个 mini-batch 里去选正负样本？因为这样就可以用一个编码器做端到端的训练了，这也就是MoCo里讲过的端到端的学习方式</li><li>剩下的前向过程都是差不多的，就是过完编码器以后，再过一层全连接层就把这个特征的维度降的很低，就变成128，正样本比如说上图中绿色的球在最后的特征空间上应该尽可能的接近，但是这个绿色的球跟别的颜色的特征应该尽可能的拉远</li><li>本文所用的目标函数也是 NCE loss 的一个变体</li><li>这篇论文，刚好属于另一个流派，也就是端到端的学习，而且只用一个编码器，不需要借助外部的数据结构去存储大量的负样本，它的正负样本都来自于同一个 mini-batch</li><li>既然它跟 SimCLR 这么像，为什么它没有取得那么好的结果呢？就是之前在 MoCo 那篇论文里反复强调过的，就是这个字典必须足够大，也就是说在做对比学习的时候，负样本最好是足够多，而本文的作者是没有 TPU 的，所以说它的 batch size 就是256，也就意味着它的负样本只有500多个，再加上它还缺少像 SimCLR 那样那么强大的数据增广以及最后提出的那个 mlp projector，所以说呢这篇论文的结果没有那么炸裂，自然也就没有吸引大量的关注，但事实上它是可以理解成 SimCLR 的前身</li></ul><hr><h4 id="《Representation-Learning-with-Contrastive-Predictive-Coding》"><a href="#《Representation-Learning-with-Contrastive-Predictive-Coding》" class="headerlink" title="《Representation Learning with Contrastive Predictive Coding》"></a>《Representation Learning with Contrastive Predictive Coding》</h4><p>CPC(contrastive predictive coding)</p><p>一般机器学习分为判别式模型和生成式模型，个体判别显然是属于判别式范畴的，那肯定就会有一些生成式的代理任务，比如最常见的预测型的任务</p><p>cpc 这篇论文其实非常厉害，因为它是一个很通用的结构</p><img src="/14c102fd/8.webp"><p>图1中描述的是CPC不仅可以处理音频，还可以处理图片、文字以及在强化学习里使用，这里为了简单，它用的是一个音频的信号作为输入。</p><p>本文的想法：假如说有一个输入 x（一个持续的序列），t 表示当前时刻，t-i 表示过去的时刻，t+i 表示未来的时刻。把之前时刻的输入全都扔给一个编码器，这个编码器就会返回一些特征，然后把这些特征喂给一个自回归的模型（gar，auto regressive），一般常见的自回归模型，就是 RNN 或者 LSTM 的模型，所以每一步最后的输出，就会得到图中红色的方块（ct，context representation，代表上下文的一个特征表示），如果这个上下文的特征表示足够好（它真的包含了当前和之前所有的这些信息），那它应该可以做出一些合理的预测，所以就可以用 ct 预测未来时刻的这个zt + 1、zt + 2（未来时刻的特征输出）</p><p>对比学习在哪里体现的呢？正样本其实就是未来的输入通过编码器以后得到的未来时刻的特征输出，这相当于做的预测是 query，而真正未来时刻的输出是由输入决定的，也就是说它们相对于预测来说是正样本；负样本的定义其实很广泛，比如可以任意选取输入通过这个编码器得到输出，它都应该跟预测是不相似的，这就是cpc定义正负样本的方式</p><p>这套思想是很朴实的，把输入序列换成一个句子，也可以说用前面的单词来预测后面的单词的特征输出；如果把这个序列想象成一个图片的 patch 块从左上到右下形成一个序列，就可以用上半部分的图片特征去预测后半部分的图片特征，总之是非常灵活</p><hr><h4 id="《Contrastive-Multiview-Coding》"><a href="#《Contrastive-Multiview-Coding》" class="headerlink" title="《Contrastive Multiview Coding》"></a>《Contrastive Multiview Coding》</h4><p>CMC(contrastive multiview coding)</p><p>cpc是用预测的代理任务做对比学习，cmc这篇论文定义正样本的方式就更为广泛了：一个物体的很多个视角都可以被当做正样本</p><p>cmc 的摘要写的非常好：</p><ul><li>人观察这个世界是通过很多个传感器，比如说眼睛或者耳朵都充当着不同的传感器来给大脑提供不同的信号</li><li>每一个视角都是带有噪声的，而且有可能是不完整的，但是最重要的那些信息其实是在所有的这些视角中间共享，比如说基础的物理定律、几何形状或者说它们的语音信息都是共享的</li><li>在这里举了个很好的例子：比如一个狗既可以被看见，也可以被听到或者被感受到<br>-基于这个现象作者就提出：他想要学一个非常强大的特征，它具有视角的不变性（不管看哪个视角，到底是看到了一只狗，还是听到了狗叫声，都能判断出这是个狗）</li><li>cmc工作的目的就是去增大互信息（所有的视角之间的互信息）</li><li>如果能学到一种特征能够抓住所有视角下的关键的因素，那这个特征就很好了，至少解决分类问题不在话下</li></ul><img src="/14c102fd/9.webp"><p>cmc到底是怎么样去形成正样本和负样本从而去做对比学习的呢？如图一所示，它选取的是 NYU RGBD 这个数据集（这个数据集有同时4个view，也就是有四个视角：原始的图像、这个图像对应的深度信息（每个物体离观察者到底有多远）、SwAV ace normal、这个物体的分割图像）</p><p>cmc 的意思是说，虽然这些不同的输入来自于不同的传感器或者说不同的模态，但是所有的这些输入其实对应的都是一整图片，都是一个东西，那它们就应该互为正样本，也就是说，当有一个特征空间的时候，比如图中圆圈所示的特征空间，这四个绿色的点在这个特征空间里就应该非常的接近。这时候如果随机再去挑一张图片，不论是用图片还是用风格的图像（总之属于一个不配对的视角）的话，这个特征就应该跟这些绿色的特征远离</p><p>这就是 cmc 定义正负样本的方式，它的正样本来自于多个视角，一旦定义好了正负样本，剩下的工作就大差不差了</p><p>cmc 是第一个或者说比较早的工作去做这种多视角的对比学习，它不仅证明了对比学习的灵活性，而且证明了这种多视角、多模态的这种可行性。所以说接下来 open AI，很快就出了 clip 模型：也就是说如果有一个图片，还有一个描述这个图片的文本，那这个图像和文本就可以当成是一个正样本对，就可以拿来做多模态的对比学习</p><p>cmc 原班作者人马用对比学习的思想做了一篇蒸馏的工作：不论用什么网络，不论这个网络是好是坏是大是小，只要你的输入是同一张图片，那得到的这个特征就应该尽可能的类似，也就意味着想让 teacher 模型的输出跟 student 模型的输出尽可能的相似，它就通过这种方式把 teacher 和 student 做成了一个正样本对，从而可以做对比学习</p><p>所以说让大家意识到对比学习如此灵活，可以应用到不同的领域，cmc 功不可没</p><p>一个小小的局限性：当处理不同的视角或者说不同的模态时候，可能需要不同的编码器，因为不同的输入可能长得很不一样，这就有可能会导致使用几个视角，有可能就得配几个编码器，在训练的时候这个计算代价就有点高（比如说在 clip 这篇论文里，它的文本端就是用一个大型的语言模型，比如说 bert，它的图像端就是用一个 vit，就需要有两个编码器），这样其实又回到了刚开始讲ViT时候所说的说这个 Transformer 的好处————Transformer有可能可以同时处理不同模态的数据</p><p>事实上现在已经有人这么做了，2021的 ICLR 就有一篇 ma clip，它就用一个 Transformer 去同时处理两个输入模态，效果反而更好，所以说这可能才是 Transformer 真正吸引人的地方：一个网络能处理很多类型的数据，而不用做针对每个数据特有的改进。</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>第一阶段大概讲了这四篇论文，可以看到：</p><ul><li>它们使用的代理任务是不一样的，有个体判别，有预测未来，还有多视角多模态</li><li>它们使用的目标函数也不尽相同，有 NCE，有 infoNCE，还有 NCE 的其它变体</li><li>它们使用的模型也都不一样，比如说 invariant spread 用了一个编码器；Inst Disc 用一个编码器和 memory bank； cpc有一个编码器，还有一个自回归模型；cmc 可能有两个甚至多个编码器</li><li>它们做的任务从图像到视频到音频到文字到强化学习，非常的丰富多彩</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/20bd4067.html" rel="prev" title="SFFAI 133 | 对抗机器学习专题《杨卓林：TRS：Transferability Reduced Ensemble via Encouraging...》"><i class="fa fa-chevron-left"></i> SFFAI 133 | 对抗机器学习专题《杨卓林：TRS：Transferability Reduced Ensemble via Encouraging...》</a></div><div class="post-nav-item"><a href="/a8634df8.html" rel="next" title="对比学习论文综述-2CV双雄">对比学习论文综述-2CV双雄 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">对比学习综述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%99%BE%E8%8A%B1%E9%BD%90%E6%94%BE"><span class="nav-number">2.</span> <span class="nav-text">百花齐放</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AUnsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination%E3%80%8B"><span class="nav-number">2.1.</span> <span class="nav-text">《Unsupervised Feature Learning via Non-Parametric Instance Discrimination》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AUnsupervised-Embedding-Learning-via-Invariant-and-Spreading-Instance-Feature%E3%80%8B"><span class="nav-number">2.2.</span> <span class="nav-text">《Unsupervised Embedding Learning via Invariant and Spreading Instance Feature》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8ARepresentation-Learning-with-Contrastive-Predictive-Coding%E3%80%8B"><span class="nav-number">2.3.</span> <span class="nav-text">《Representation Learning with Contrastive Predictive Coding》</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%8AContrastive-Multiview-Coding%E3%80%8B"><span class="nav-number">2.4.</span> <span class="nav-text">《Contrastive Multiview Coding》</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1026</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">4m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">167:37</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>