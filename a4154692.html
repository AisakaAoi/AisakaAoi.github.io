<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><meta name="baidu-site-verification" content="code-KCMz4b3cnd"><meta name="google-site-verification" content="MTp8U7dJ1uzrfz8Mu6rgqX1CIm3HjqPWd0xaRcv1tFg"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"aisakaaoi.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="导读生成对抗网络 (Generative Adversarial Networks, GANs) 在过去几年中被广泛地研究，其在图像生成、图像转换和超分辨率等领域取得了显著的进步。到目前为止，已经提出了大量基于GANs的相关工作和综述。本文基于柏林圣三一大学计算机科学与统计学院的王正蔚博士与字节跳动AI实验室联合发表的一篇综述[1]为基础，详细的解读GANs的来龙去脉，同时为大家介绍近期一些相关工"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-一文看尽深度学习中的生成对抗网络 | CVHub带你看一看GANs架构发展的8年"><meta property="og:url" content="https://aisakaaoi.github.io/a4154692.html"><meta property="og:site_name" content="逢坂葵的个人博客"><meta property="og:description" content="导读生成对抗网络 (Generative Adversarial Networks, GANs) 在过去几年中被广泛地研究，其在图像生成、图像转换和超分辨率等领域取得了显著的进步。到目前为止，已经提出了大量基于GANs的相关工作和综述。本文基于柏林圣三一大学计算机科学与统计学院的王正蔚博士与字节跳动AI实验室联合发表的一篇综述[1]为基础，详细的解读GANs的来龙去脉，同时为大家介绍近期一些相关工"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/1.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/2.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/3.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/4.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/5.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/6.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/7.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/8.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/9.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/10.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/11.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/12.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/13.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/14.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/15.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/16.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/17.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/18.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/19.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/20.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/21.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/22.webp"><meta property="og:image" content="https://aisakaaoi.github.io/a4154692/23.webp"><meta property="article:published_time" content="2021-06-29T15:04:58.000Z"><meta property="article:modified_time" content="2026-01-23T11:02:36.594Z"><meta property="article:author" content="Aisaka Aoi"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://aisakaaoi.github.io/a4154692/1.webp"><link rel="canonical" href="https://aisakaaoi.github.io/a4154692.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度学习-一文看尽深度学习中的生成对抗网络 | CVHub带你看一看GANs架构发展的8年 | 逢坂葵的个人博客</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?7308ed05421777c301eefa3754da1b42",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="逢坂葵的个人博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">aoi学院</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Aisaka's Blog, School of Aoi, Aisaka University</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">1026</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><a href="https://github.com/AisakaAoi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://aisakaaoi.github.io/a4154692.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/manatsu.jpg"><meta itemprop="name" content="Aisaka Aoi"><meta itemprop="description" content="Aisaka's Blog, School of Aoi, Aisaka University"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逢坂葵的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习-一文看尽深度学习中的生成对抗网络 | CVHub带你看一看GANs架构发展的8年</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-06-29 23:04:58" itemprop="dateCreated datePublished" datetime="2021-06-29T23:04:58+08:00">2021-06-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">🌙进阶学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%F0%9F%8C%99%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/%E2%AD%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">⭐人工智能 Artificial Intelligence</span></a> </span></span><span id="/a4154692.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-一文看尽深度学习中的生成对抗网络 | CVHub带你看一看GANs架构发展的8年" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/a4154692.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/a4154692.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>14k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>35 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h3><p>生成对抗网络 (Generative Adversarial Networks, GANs) 在过去几年中被广泛地研究，其在图像生成、图像转换和超分辨率等领域取得了显著的进步。到目前为止，已经提出了大量基于GANs的相关工作和综述。本文基于柏林圣三一大学计算机科学与统计学院的王正蔚博士与字节跳动AI实验室联合发表的一篇综述[1]为基础，详细的解读GANs的来龙去脉，同时为大家介绍近期一些相关工作，中间也会穿插一些笔者的见解。最后，本文也将列出一些可探索的未来研究方向，希望能给予读者一些启发。</p><span id="more"></span><hr><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>生成对抗网络已应用在各种领域，例如计算机视觉（Computer Vision）、自然语言处理（Natural Language Processing）、时间序列合成（Time-series Synthesis）和语义分割（Semantic Segmentation）等。GANs本质上属于机器学习中的生成模型系列，与其他生成模型（如变分自动编码器）相比，GANs能够有效地生成所需要的样本，消除确定性偏差，并且与内部神经网络结构具有良好的兼容性。这些特性使GANs在计算机视觉领域获得了巨大的成功。尽管GANs至今已取得了巨大的成功，但将其应用于现实世界仍存在许多挑战，最主要的难点为：</p><ol><li>高质量图像的生成</li><li>生成图像的多样性</li><li>训练的不稳定性</li></ol><p>为了改善如上所述的问题，可以从体系结构角度或从损失函数角度对GANs进行修改。下图展示了2014-2020年提出的具有代表性的GANs分类。我们将当前GANs分为两类：<strong>体系结构变体（architecture-variant）和 损失变体（loss-variant）。在体系结构变体中，我们分为三类，分别为网络结构（network architecture）、隐空间（latent space） 以及 应用研究（application-focus）</strong>。网络结构类别表示对整个GAN结构进行改进或修改，例如**Progressive GAN[2]<strong>使用渐进增大的训练方式。潜在空间类别表示基于潜在空间的不同表示的结构修改，例如</strong>Conditional GAN[3]<strong>涉及向生成器和判别器提供标签信息。最后一类应用研究是指根据不同的应用进行修改，例如</strong>CycleGAN[4]**有一个特定结构用于图像风格转换。对于损失变体，我们将其分成两类，损失类型和正则化。损失类型是指可以针对GANs优化的不同损失函数，而正则化则是一种附加惩罚，被设计在损失函数或是归一化操作中。本文先对基于体系结构改进的GANs进行介绍。</p><img src="/a4154692/1.webp"><div align="center">GANs分类</div><hr><h3 id="Original-GAN"><a href="#Original-GAN" class="headerlink" title="Original GAN"></a>Original GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/goodfeli/adversarial">https://github.com/goodfeli/adversarial</a></p><img src="/a4154692/2.webp"><div align="center">Original GAN结构图</div><p>如上图所示，原始**GAN[5]**分为两个组件，一个为判别器(D)，用于区分真实样本与生成样本；另一个为生成器(G)，用于生成假样本来欺骗判别器。给定一个分布z~pz，G将概率分布定义为样本G(z)的分布。GAN的目的是要学习近似实际数据分布pr的生成器分布pg。用于优化GAN的损失函数如下：</p><img src="/a4154692/3.webp"><p>GANs作为深度生成模型（DGM）家族的一员，由于与传统DGMs相比具有以下优势：</p><ol><li>GANs性能更强<br>与变分自动编码器（VAE）相比，GAN能够计算任何类型的概率密度，从而生成更清晰的图像。</li><li>GAN框架能训练任何类型的生成器网络<br>其他的DGMs可能对生成器有要求，例如需要生成器的输出层为高斯。</li><li>数据大小没有限制<br>这些优势促使GANs在生成合成数据（尤其是图像数据）时达到SOTA性能。</li></ol><hr><h3 id="Energy-based-GAN"><a href="#Energy-based-GAN" class="headerlink" title="Energy-based GAN"></a>Energy-based GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.03126">https://arxiv.org/abs/1609.03126</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a></p><img src="/a4154692/4.webp"><div align="center">Energy-based GAN结构图</div><p><strong>Energy-based GAN</strong>[6]（EBGAN）通过利用auto-encoder作为判别器，使其鉴别对象不再是真假样本，而是鉴别样本的重构性。即不再对真实样本与生成样本之间的差异进行打分，而是利用一种“记忆”（通过auto-encoder实现）让判别器记住真实样本分布，若输入x与“记忆”相似则给高分，否则给低分。该方法应用于一些简单的图像数据集（<strong>MNIST</strong>[7]、<strong>CIFAR-10</strong>[8]和Toronto face）中取得了不错的效果，对比原始GAN，EBGAN做了如下工作：</p><ol><li>由于EBGAN的判别器是经过预训练的，因此生成器在一开始便能获得比较大的“能量驱动”（energy besed），有效地较低了模型的训练成本。</li><li>该文章提出k step优化判别器以及1 setp优化生成器，以避免判别器过拟合。</li><li>采用最大化 logD(G(z)) 来训练生成器，避免优化生成器时出现梯度消失的问题。此修改等效于利用反向KL散度来度量 p_r 和 p_g 之间的差异，会引起非对称的问题。</li><li>将<strong>maxout</strong>[9]应用在判别器，而ReLU和sigmoid激活函数应用于生成器。而然设置对于复杂图像并没有表现出良好的泛化性能。</li></ol><hr><h3 id="Semi-supervised-GAN"><a href="#Semi-supervised-GAN" class="headerlink" title="Semi-supervised GAN"></a>Semi-supervised GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.01583">https://arxiv.org/abs/1606.01583</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a></p><img src="/a4154692/5.webp"><div align="center">Semi-supervised GAN结构图</div><p><strong>Semi-supervised GAN</strong><a href="SGAN">10</a> 是在半监督学习的背景下提出的。半监督学习是介于监督学习和无监督学习之间的一个很有前景的研究领域。与监督学习（每个样本需要提供标签）和无监督学习（不提供标签）不同，半监督学习为一小部分样本提供标签。与原始GAN不同，SGAN 的判别器是多头的，如上图所示，它具有 softmax 和 sigmoid 分别用于对真实数据进行分类并区分真假样本。SGAN在MNIST数据集上进行了大量的实验，结果表明SGAN中的判别器和生成器的都比原始GAN表现更好。这种多头判别器的架构比较简单，也许会限制模型的多样性，结合更复杂的鉴别器架构可能会进一步提高模型的性能。</p><hr><h3 id="Bidirectional-GAN"><a href="#Bidirectional-GAN" class="headerlink" title="Bidirectional GAN"></a>Bidirectional GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1605.09782">https://arxiv.org/abs/1605.09782</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a></p><img src="/a4154692/6.webp"><div align="center">Bidirectional GAN结构图</div><p>原始GAN无法学习逆映射，例如将数据投影回隐空间。<strong>Bidirectional GAN</strong>[11]（BiGAN）正是为解决此问题而设计的。如上图所示，BiGAN的网络结构包含一个编码器 (E)、一个生成器 (G) 以及一个判别器 (D)。E 将真实样本数据编码为 E(x)，而 G 则将 z 解码为G(z)。因此，D 旨在评估每对 (E(x), x) 与 (G(z), z) 之间的差异，E 和 G 并不会直接交互。通过加入编码器与解码器之间的相互反转结构，BiGAN的性能得到了进一步地提升。如果将这种模型应用于处理对抗样本将会很有趣。</p><hr><h3 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.1784">https://arxiv.org/abs/1411.1784</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a></p><img src="/a4154692/7.webp"><div align="center">Conditional GAN结构图</div><p><strong>Conditional GAN</strong>[12]（CGAN）的创新在于通过提供类标签（class label）来进一步优化生成器和判别器。如上图所示，CGAN提供了额外的信息 y（可以是类标签或者其它模态的数据）给生成器和判别器。值得注意的是，y 通常是先独自进行编码，然后再与已编码的 z 和 x 进行拼接。例如，在MNIST数据集的实验中， z 和 y 分别被映射到层大小为200和1000的隐藏层，然后在生成器中再进行结合（200+1000&#x3D;1200）。通过这种做法，CGAN能提升判别器的判别能力。CGAN的损失函数与原始GAN略有不同，如以下等式所示：</p><img src="/a4154692/8.webp"><p>其中 x 和 y 受限于。得益于额外的 y 信息，CGAN不仅能够处理单一模态的图像数据，而且还可以处理多模态的数据。例如，包含标记图像数据及其相关用户生成元数据的Flickr数据集，这将GAN带到了多模态数据生成领域。</p><hr><h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.03657">https://arxiv.org/abs/1606.03657</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a></p><img src="/a4154692/9.webp"><div align="center">InfoGAN结构图</div><p><strong>InfoGAN</strong>[13]被提出作为一种超越CGAN的模型，它通过利用无监督的方式，最大化条件变量和生成数据之间的互信息，以学习可解释的特征表示。如上图所示，InfoGan引入了另一个分类器 Q 来预测由 G(z|y) 生成的 y。G 和 Q 的结合在这里可以理解为自动编码器，其中我们旨在找出最小化 y 和 y’ 之间交叉熵的 G(z|y)。然而，D 和原始GAN都是干一样的事情，即区分真实样本和生成样本。为了减少计算开销，Q 和 D 共享所有卷积层除了最后的全连接层，这使得判别器拥有判别真假样本的能力并且恢复信息 y。与原始GAN结构相比，这使得InfoGAN具有更强的判别能力。InfoGAN使用的损失是CGAN损失的正则化：</p><img src="/a4154692/10.webp"><p>其中 V(D, G) 为CGAN的优化目标除了判别器不需要额外的 y 信息作为输入，并且 I(·) 为互信息。</p><hr><h3 id="Auxiliary-Classifier-GAN"><a href="#Auxiliary-Classifier-GAN" class="headerlink" title="Auxiliary Classifier GAN"></a>Auxiliary Classifier GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v70/odena17a.html">http://proceedings.mlr.press/v70/odena17a.html</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/buriburisuri/ac-gan">https://github.com/buriburisuri/ac-gan</a></p><img src="/a4154692/11.webp"><div align="center">Auxiliary Classifier GAN结构图</div><p><strong>Auxiliary Classifier GAN</strong>[14]（AC-GAN）与CGAN和InfoGAN非常相似，如上图所示，它包含了一个辅助分类器。每一个生成样本都一个对应类标签 c 和噪声 z。需要注意的是，AC-GAN与前两个架构变体（CGAN 和 InfoGAN）的区别在于附加信息，这里仅限于类标签，而前两个可以是其他的领域数据。因此，上图中使用 c 和 c’ 来标记以突显其与前两种方法的差异。AC-GAN的判别器由一个判别器 D 以及一个分类器 Q 组成，与InfoGAN类似，判别器与分类器共享参数除了最后一层。AC-GAN的损失函数可以写成以下形式：</p><img src="/a4154692/12.webp"> <img src="/a4154692/13.webp"><p>其中 D 通过最大化 L_S + L_C 训练，G 通过最大化 L_C - L_S 训练。AC-GAN 提高了生成图像的视觉质量以及模型多样性。但是，这些改进需要依赖大规模的带标签数据集，这在实际应用中可能会带来挑战。</p><hr><h3 id="Laplacian-Pyramid-of-Adversarial-Networks"><a href="#Laplacian-Pyramid-of-Adversarial-Networks" class="headerlink" title="Laplacian Pyramid of Adversarial Networks"></a>Laplacian Pyramid of Adversarial Networks</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.05751">https://arxiv.org/abs/1506.05751</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch">https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch</a></p><img src="/a4154692/14.webp"><div align="center">LAPGAN中生成器的上采样过程</div><p><strong>Laplacian Pyramid of Adversarial Networks</strong>[15]（LAPGAN）被设计用于将低分辨率图像作为输入，生成高分辨率图像。拉普拉斯金字塔[16]是一种图像编码方法，它使用多个尺度但形状相同的局部算子作为基础函数。LAPGAN 在拉普拉斯金字塔框架 [16] 中利用级联 CNN 来生成高质量图像，如上图所示。LAPGAN使用拉普拉斯金字塔对图像进行上采样，而不是使用转置卷积。首先，LAPGAN使用第一个生成器生成一张非常小的图像，这可以提升生成器的稳定性，接着通过使用拉普拉斯金字塔对该图像进行上采样。然后，将上采样的图像喂给下一个生成器以生成图像差异以及图像差异的总和。如上图所示，我们将 G_3 的生成图像作为输入图像，虽然图像尺度非常小，但有利于训练的稳定性。对于较大的图像，生成器用于生成图像差异，这比相同大小的原始图像要简单得多。</p><hr><h3 id="Deep-Convolutional-GAN"><a href="#Deep-Convolutional-GAN" class="headerlink" title="Deep Convolutional GAN"></a>Deep Convolutional GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.06434">https://arxiv.org/abs/1511.06434</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/carpedm20/DCGAN-tensorflow">https://github.com/carpedm20/DCGAN-tensorflow</a></p><img src="/a4154692/15.webp"><div align="center">Deep Convolutional GAN的生成器结构图</div><p><strong>Deep Convolutional GAN</strong>[16] (DCGAN)是第一个将转置卷积神经网络架构应用于生成器的方法。转置卷积能够有效地将CNN特征进行<strong>可视化</strong>[17]。DCGAN 的生成器使用转置卷积操作对图像进行上采样，这能够提升其生成高分辨率图像的质量。与原始GAN 相比，DCGAN 的架构有一些关键的修改，这有利于高分辨率建模以及稳定训练：</p><ol><li>DCGAN使用跨步卷积（strided convolution）代替结构中所有的池化层。</li><li>判别器和生成器都使用批归一化（batch normalization），这有助于网络更有效地进行训练。</li><li>除了最后的输出层使用Tanh，生成器的其他层全部使用ReLU激活函数；判别器的所有层均使用Leaky ReLU激活函数。</li></ol><p>DCGAN是GANs历史上非常重要的里程碑，转置卷积操作被后续提出的GANs广泛应用。由于模型容量的限制，DCGAN只能在低分辨率和多样性较少的图像上取得较好的效果。</p><hr><h3 id="Boundary-Equilibrium-GAN"><a href="#Boundary-Equilibrium-GAN" class="headerlink" title="Boundary Equilibrium GAN"></a>Boundary Equilibrium GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.10717">https://arxiv.org/abs/1703.10717</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/carpedm20/BEGAN-tensorflow">https://github.com/carpedm20/BEGAN-tensorflow</a></p><img src="/a4154692/16.webp"><div align="center">Boundary Equilibrium GAN结构图</div><p>GANs可以生成非常逼真的图像，比使用像素损失的自编码器生成的图像更锐利清晰。然而，GANs仍然面临许多未解决的困难：超参数选择困难、模式坍塌和平衡生成器和判别器的收敛程度困难。<strong>Boundary Equilibrium GAN</strong>[18]（BEGAN）借鉴了EBGAN和WGAN各自的一些优点，提出了一种新的评价生成器生成质量的方式，使得网络能有效地避免模式崩溃（model collapse）和训练不稳定的问题。</p><p>通常，如果两个分布越接近，则可以认为它们越相似，当生成数据分布非常接近于真实数据分布的时候，生成器就具备了足够的生成能力。BEGAN代替了这种估计概率分布方法，它不直接去估计生成分布 P_g 与真实分布 P_r 的差距，而是估计分布的误差分布之间的差距，即只要分布之间的误差分布相近的话，也可以认为这些分布是相近的。</p><p>总的来说，Boundary Equilibrium GAN作了如下贡献：</p><ol><li>提出了一个简单强大的GAN结构，使用标准的训练步骤实现了快速且稳定的收敛。</li><li>对于GAN中的 G 和 D 的平衡提出了一种均衡的概念。</li><li>提供了一个超参数，用于平衡图像的多样性和生成质量。</li><li>提出了一种收敛程度的估计（灵感来自于WGAN）。</li></ol><p>BEGAN网络结构简洁且无需调整大量的超参数，在CeleB数据集上达到了优异的性能效果。</p><hr><h3 id="Progressive-GAN"><a href="#Progressive-GAN" class="headerlink" title="Progressive GAN"></a>Progressive GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10196">https://arxiv.org/abs/1710.10196</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/akanimax/pro_gan_pytorch">https://github.com/akanimax/pro_gan_pytorch</a></p><img src="/a4154692/17.webp"><div align="center">Progressive GAN的逐步增长训练过程</div><p><strong>Progressive GAN</strong>[19]（ProGAN）也是一种应用于图像生成的生成对抗网络。不同于先前的GANs的训练策略，它们都是先搭建好网络再进行训练，而ProGAN则是采用逐步增长的训练策略，如上图所示。具体来说，生成器 G 和判别器数 D 最初只有一层，前者用于生成分辨率极低（4×4）的图像，后者再对生成的图像与真实图像（同等分辨率）进行编码得到输出结果。训练若干轮次。接着，给 G 和 D 均加一层，使得生成的图像分辨率更高（8×8），并且将其与真实图像一起输入至判别器，获得输出结果。再次训练若干轮次。重复上述过程，直至生成图像分辨率达到指定大小。</p><p>ProGAN解决了生成高分辨率图像的问题。该方法最主要的贡献为提出了一种渐进式训练方式。然而，ProGAN与多数GAN一样，控制生成图像的特定特征的能力非常有限。图像特征属性相互纠缠，即使略微调整输入，会同时影响生成图像的多个属性。因此如何将ProGAN改为条件生成模型，或者增强其调整单个属性的能力，是一个不错的研究方向。</p><hr><h3 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/NVlabs/stylegan">https://github.com/NVlabs/stylegan</a></p><img src="/a4154692/18.webp"><div align="center">(a)传统生成器结构；(b)基于样式的生成器结构</div><p>由于ProGAN是逐级生成图片的，我们没有办法对其增添控制，这导致ProGAN无法生成具有特定特征的图像。因此，在ProGAN的基础上，<strong>StyleGAN</strong>[20]对生成器做出了进一步的改进和提升，其主要通过分别修改每一层级的输入，在不影响其他层级的情况下以控制该层所表示的视觉特征。StyleGAN的主要贡献如下：</p><ul><li>借鉴了风格迁移，提出新的生成器结构（基于样式的生成器）<ul><li>实现无监督地分离高级属性（姿态，身份）与随机变化（头发，雀斑）</li><li>实现可直观地，按特定尺度控制合成</li></ul></li><li>实现了更高的生成质量，更好的解耦能力<ul><li>通过映射网络f将隐向量z映射至中间的隐空间W。输入隐向量z必须遵循训练数据的概率密度，这会导致某种程度的不可避免的纠缠，而中间的隐空间W则不受限制。</li></ul></li><li>提出了两种新的量化隐空间解耦程度的方法<ul><li>感知路径长度和线性可分性。与传统的生成器体系结构相比，新的生成器允许更线性和解耦地表示不同的变化因素。</li></ul></li><li>提出了FFHQ人脸数据集<ul><li>高质量、大规模的人脸数据集，7万张1024×1024图片</li></ul></li></ul><p>不得不说，StyleGAN不论是在理论上，还是工程实践上，都是具有突破性的，它不仅可以生成高质量的图像，而且还可以对生成图像进行有效的控制和理解。</p><hr><h3 id="Self-attention-GAN"><a href="#Self-attention-GAN" class="headerlink" title="Self-attention GAN"></a>Self-attention GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf">http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/heykeetae/Self-Attention-GAN">https://github.com/heykeetae/Self-Attention-GAN</a></p><img src="/a4154692/19.webp"><div align="center">自注意力机制的计算流程</div><p>传统CNNs只能捕获局部空间信息并且感受野会受到一定的限制，这导致了基于传统CNN的GANs学习多类别的复杂数据集会非常困难以及生成图像中的关键语义可能会发生偏移，例如生成的脸部图像中鼻子并不一定在正确的位置。自注意力机制能够捕获大感受野的同时无需牺牲太多的计算效率。<strong>Self-attention GAN</strong>[21]（SAGAN）生成器和判别器都运用了自注意力机制，如上图所示。受益于自注意力机制，SAGAN能够学习空间像素的全局依赖关系来生成图像，其在ImageNet数据集的多类别图像生成任务中取得了很好的性能表现。</p><hr><h3 id="BigGAN"><a href="#BigGAN" class="headerlink" title="BigGAN"></a>BigGAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.11096">https://arxiv.org/abs/1809.11096</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/ajbrock/BigGAN-PyTorch">https://github.com/ajbrock/BigGAN-PyTorch</a></p><img src="/a4154692/20.webp"><div align="center">(a)生成器；(b)生成器中Residual Block的展开细节；(c)判别器中Residual Block的展开细节</div><p><strong>BigGAN</strong>[22]作为GAN发展史上重要的里程碑，在生成图像精度和多样性均作出了跨越式提升。在ImageNet训练下，将Inception Score（IS，生成图像的质量）和Frechet Inception Distance（FID，生成图像的多样性）分别从52.52和18.65提升到166.3和9.6。BigGAN主要有以下贡献点：</p><ul><li>通过增加模型参数量（扩展channel）以及增大batch size<ul><li>batch size提高分别2、4、8，通道数增加1.5倍，性能均获得不同程度的提升。</li></ul></li><li>结合了各种新颖trick<ul><li>采用共享嵌入（shared embeddings）将类别信息投影至生成器的每个BN层，有效利用了类别信息且不增加参数量；</li><li>通过层次化隐空间将随机噪声向量输入到生成器的每一层中，使得随机噪声向量能直接作用在不同层级特征；</li><li>使用正交正则化来改善截断技术引起的饱和伪影问题，使得更有效利用整个随机噪声向量空间合成更高质量的样本。</li></ul></li></ul><p>通过利用许多新颖的方法和技巧，BigGAN的性能可以得到有效的提升，然而网络在训练时也容易崩溃，因此在实际使用中有必须采取提前停止训练的措施。此外，BigGAN也探索了一套能够指示训练崩溃的度量指标，使得能针对性地分析并解决网络训练不稳定的问题。</p><hr><h3 id="Label-noise-rGANs"><a href="#Label-noise-rGANs" class="headerlink" title="Label-noise rGANs"></a>Label-noise rGANs</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kaneko_Label-Noise_Robust_Generative_Adversarial_Networks_CVPR_2019_paper.pdf">http://openaccess.thecvf.com/content_CVPR_2019/papers/Kaneko_Label-Noise_Robust_Generative_Adversarial_Networks_CVPR_2019_paper.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/takuhirok/rGAN">https://github.com/takuhirok/rGAN</a></p><img src="/a4154692/21.webp"><div align="center">(a)AC-GAN；(b)rAC-GAN；(c)CGAN以及(d)rCGAN的判别器。黄色矩形为噪声过渡模型</div><p>我们在上文已经分别讨论过CGAN和AC-GAN，它们都具有学习解耦表示（disentangled representation）以及提高GANs的判别能力。然而，训练模型需要大规模标记的数据集，这在实际场景中具有很大的挑战性。<strong>label-noise rGANs</strong>[23]通过结合噪声过渡模型，能够从具有噪声的训练标签中学习到干净的标签条件生成分布。上图展示了基于AC-GAN和CGAN的扩展，这些rGANs的核心部分为噪声过度模块引入到判别器中。rGAN在CIFAR-10 和 CIFAR-100数据集上进行了对比实验，其中在CIFAR-10数据集上的性能优于原始架构，并且还表现出对噪声标签的鲁棒性。但是，在 CIFAR-100 中，当向标签引入高噪声时，rGAN的性能会下降。结果表明rGAN在遇到更复杂的数据集时仍然有些局限性，未来需要进一步研究。</p><hr><h3 id="Your-Local-GAN"><a href="#Your-Local-GAN" class="headerlink" title="Your Local GAN"></a>Your Local GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Daras_Your_Local_GAN_Designing_Two_Dimensional_Local_Attention_Mechanisms_for_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Daras_Your_Local_GAN_Designing_Two_Dimensional_Local_Attention_Mechanisms_for_CVPR_2020_paper.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/giannisdaras/ylg">https://github.com/giannisdaras/ylg</a></p><p>注意力机制通过建模像素间关系，能有效地处理复杂的几何形状以及捕获长距离依赖，以进一步提高网络性能。然而，注意力也存在以下限制。首先计算开销大。标准的密集注意力需要的空间和时间成倍增加。其次，计算注意力时将二维的空间结构展开为一维向量会损失空间特性。针对以上的问题，<strong>Your Local GAN</strong>[24]（YLG）主要做了以下贡献：</p><ol><li>引入了局部稀疏注意力层，该层保留了二维图像的局部性，并且可以通过attention steps来支持良好的信息流。</li><li>使用了信息流图的信息理论框架，以量化信息流的步骤并保持二维局部性。</li><li>基于SAGAN结构提出了YLG-SAGAN，使得网络的性能和训练时间均得到大幅优化。</li><li>提出了一种解决GANs反演问题的新方法，能对更大模型的损失进行梯度下降的自然反演过程。</li></ol><p>YLG通过应用稀疏注意力与信息流的相关技术以有效提高网络性能。然而，该工作存在两个相互矛盾的目标。一方面，YLG旨在使网络尽可能稀疏以提高计算和统计效率，另一方面它们仍然需要支持良好和完整的信息流。</p><hr><h3 id="AutoGAN"><a href="#AutoGAN" class="headerlink" title="AutoGAN"></a>AutoGAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/VITA-Group/AutoGAN">https://github.com/VITA-Group/AutoGAN</a></p><p><strong>AutoGAN</strong>[25]将神经架构搜索(NAS)算法引入生成对抗网络(GANs)。生成器架构变体的搜索空间通过RNN、参数共享和动态重置来引导以加速该过程。AutoGAN使用Inception Score（IS）作为奖励，并引入多级搜索策略以渐进方式执行 NAS。在谱归一化 GAN (SN-GAN) 的训练设置之后，作者使用hinge loss来训练共享GAN。</p><p>AutoGAN方法的设计非常具有创新性，同时也提出了关于NAS和GANs有效结合的新挑战。目前，NAS仍然有待对标准分类问题进行优化，更不用说GANs固有的训练不稳定问题。对比手动设计的GAN架构，虽然AutoGAN是非常新颖的并且展示出了不错的性能，但是还有以下问题需要解决：</p><ol><li>生成器的搜索空间是有限的，并且没有讨论判别器的搜索策略。</li><li>AutoGAN并未在高分辨率图像生成数据集上进行测试，其适用性不能被直观的评估。</li></ol><hr><h3 id="MSG-GAN"><a href="#MSG-GAN" class="headerlink" title="MSG-GAN"></a>MSG-GAN</h3><p><strong>论文链接：</strong><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Karnewar_MSG-GAN_Multi-Scale_Gradients_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf">http://openaccess.thecvf.com/content_CVPR_2020/papers/Karnewar_MSG-GAN_Multi-Scale_Gradients_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf</a><br><strong>代码链接：</strong><a target="_blank" rel="noopener" href="https://github.com/HelenMao/MSGAN">https://github.com/HelenMao/MSGAN</a></p><img src="/a4154692/22.webp"><div align="center">MSG-GAN结构图。应用了类似ProGAN的渐进式训练策略。MSG-GAN包括了从生成器的中间层到判别器的中间层的连接</div><p>众所周知，GANs很难适应不同的数据集。研究者认为造成这种情况的原因之一是当真实分布和虚假分布没有足够的重叠时，梯度的信息量无法有效地从判别器传递到生成器。<strong>MSG-GAN</strong>[26]被提出作为处理这类问题的一种手段。如上图所示，生成器和判别器的隐空间相连，以便它们共享更多的信息。更具体地说，在生成器的每个转置卷积的激活值通过1×1卷积映射到不同尺度的图像上。相似地，映射图像被 r’ 进一步编码为激活值，然后与真实图像的激活值拼接起来，这种连接使得判别器与生成器之间能共享更多的信息，并且实验结果也验证了有效性。</p><p>尽管 MSG-GAN 在多个图像数据集上取得了非常好的结果，但 MSG-GAN 生成不同图像的能力尚未经过测试并且我们发现 CIFAR10 上的结果不如其他数据集。这可能是由生成器和判别器 之间的连接引起的，导致生成器的多样性受到限制。图像的多样性可能会引起不一致的匹配激活，这会对训练产生负面影响，可以尝试添加自注意力模块来改善这个问题。</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="/a4154692/23.webp"><div align="center">(a)GAN结构的变体发展概述；(b)GAN结构的变体在解决三个挑战上的效果对比</div><p>本文对不同结构的GANs进行了总结，重点介绍了在图像质量、模式多样性和训练不稳定这三个关键挑战上他们是如何提高性能的。上图（a）展示了从2014到2020的GANs在架构改进上的发展，可以发现，在不同架构的GANs中存在许多联系。上图（b）展示了GANs对于三大挑战的相对性能表现。有兴趣的读者可以查阅原论文以更深入地了解每个GAN变体的原理和性能表现。接下来本文将简要回顾一下不同架构的GANs是如何解决三大挑战的。</p><h4 id="图像质量"><a href="#图像质量" class="headerlink" title="图像质量"></a>图像质量</h4><p>GANs 的基本目标之一是生成具有高质量的逼真图像。由于网络的容量有限，原始 GAN (FCGAN) 仅应用于 MNIST、Toronto face 和 CIFAR-10 数据集。DCGAN 和 LAPGAN 引入了转置卷积和上采样过程，这使得模型能生成更高分辨率的图像。其他的结构变体GANs（如BEGAN、ProGAN、SAGAN和BigGAN）在损失函数上也进行了改进以进一步提升图像的质量。仅就架构组件而言，BEGAN使用自动编码器作为判别器，在像素级别上比较生成图像和真实图像，这有助于生成器生成易于重建的数据。ProGAN 采用更深层次的架构，模型随着训练的进行而扩展。这种渐进式训练策略提高了判别器和生成器的学习稳定性，因此模型更容易学习如何生成高分辨率图像。SAGAN 主要通过谱归一化提升性能，BigGAN验证了更大的batch size和更深的模型有助于高分辨率图像的生成。</p><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><p>改变损失函数是目前解决此类问题的唯一方法。本文介绍的一些GANs避免了梯度消失问题，但这主要是因为它们使用了不同的损失函数。</p><h4 id="模式多样性"><a href="#模式多样性" class="headerlink" title="模式多样性"></a>模式多样性</h4><p>这是 GANs 最具挑战性的问题。GANs很难生成逼真的多样化图像（例如自然图像）。在架构变体GANs中，只有SAGAN和BigGAN较好地解决了这个问题。受益于自注意力机制，SAGAN和BigGAN 中的 CNN 可以捕获大的感受野，克服生成图像中的shifting components问题，这使得此类 GANs 能够生成不同的图像。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1MzY0MDI2NA==&amp;mid=2247490757&amp;idx=1&amp;sn=a1d56de8f8966d524f23c79b11ff4482">https://mp.weixin.qq.com/s?__biz=MzU1MzY0MDI2NA==&amp;mid=2247490757&amp;idx=1&amp;sn=a1d56de8f8966d524f23c79b11ff4482</a><br>[1] Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy<br>[2] Progressive growing of gans for improved quality, stability, and variation<br>[3] Conditional Generative Adversarial Nets<br>[4] Unpaired Image-to-image Translation Using Cycle-consistent Adversarial Networks<br>[5] Generative adversarial networks<br>[6] Energy-based generative adversarial network<br>[7] Gradient-based learning applied to document recognition<br>[8] Learning Multiple Layers of Features from Tiny Images<br>[9] Deep sparse rectifier neural networks<br>[10] Semi-supervised Learning with Generative Adversarial Networks<br>[11] Adversarial Feature Learning<br>[12] Conditional Generative Adversarial Nets<br>[13] InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets<br>[14] Conditional image synthesis with auxiliary classifier gans<br>[15] Deep generative image models using a laplacian pyramid of adversarial networks<br>[16] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks<br>[17] Generative Adversarial Text to Image Synthesis<br>[18] Began: Boundary equilibrium generative adversarial networks<br>[19] Progressive Growing of GANs for Improved Quality, Stability, and Variation<br>[20] A style-based generator architecture for generative adversarial networks<br>[21] Self-attention Generative Adversarial Networks<br>[22] Large Scale GAN Training for High Fidelity Natural Image Synthesis<br>[23] Label-noise robust generative adversarial networks<br>[24] Your local GAN: Designing two dimensional local attention mechanisms for generative models<br>[25] AutoGAN: Neural architecture search for generative adversarial networks<br>[26] Multi-scale gradients for generative adversarial network</p></blockquote></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/e0d395a6.html" rel="prev" title="软件过程管理-知识点快速复习"><i class="fa fa-chevron-left"></i> 软件过程管理-知识点快速复习</a></div><div class="post-nav-item"><a href="/1181719b.html" rel="next" title="SFFAI 113 | 算法优化专题《王振楠：深度学习模型提升泛化性能的正则化新方法》">SFFAI 113 | 算法优化专题《王振楠：深度学习模型提升泛化性能的正则化新方法》 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E8%AF%BB"><span class="nav-number">1.</span> <span class="nav-text">导读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">2.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Original-GAN"><span class="nav-number">3.</span> <span class="nav-text">Original GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Energy-based-GAN"><span class="nav-number">4.</span> <span class="nav-text">Energy-based GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semi-supervised-GAN"><span class="nav-number">5.</span> <span class="nav-text">Semi-supervised GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bidirectional-GAN"><span class="nav-number">6.</span> <span class="nav-text">Bidirectional GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conditional-GAN"><span class="nav-number">7.</span> <span class="nav-text">Conditional GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#InfoGAN"><span class="nav-number">8.</span> <span class="nav-text">InfoGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Auxiliary-Classifier-GAN"><span class="nav-number">9.</span> <span class="nav-text">Auxiliary Classifier GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Laplacian-Pyramid-of-Adversarial-Networks"><span class="nav-number">10.</span> <span class="nav-text">Laplacian Pyramid of Adversarial Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Convolutional-GAN"><span class="nav-number">11.</span> <span class="nav-text">Deep Convolutional GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Boundary-Equilibrium-GAN"><span class="nav-number">12.</span> <span class="nav-text">Boundary Equilibrium GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Progressive-GAN"><span class="nav-number">13.</span> <span class="nav-text">Progressive GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StyleGAN"><span class="nav-number">14.</span> <span class="nav-text">StyleGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-attention-GAN"><span class="nav-number">15.</span> <span class="nav-text">Self-attention GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BigGAN"><span class="nav-number">16.</span> <span class="nav-text">BigGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Label-noise-rGANs"><span class="nav-number">17.</span> <span class="nav-text">Label-noise rGANs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Your-Local-GAN"><span class="nav-number">18.</span> <span class="nav-text">Your Local GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AutoGAN"><span class="nav-number">19.</span> <span class="nav-text">AutoGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MSG-GAN"><span class="nav-number">20.</span> <span class="nav-text">MSG-GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">21.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F"><span class="nav-number">21.1.</span> <span class="nav-text">图像质量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">21.2.</span> <span class="nav-text">梯度消失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%BC%8F%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="nav-number">21.3.</span> <span class="nav-text">模式多样性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">22.</span> <span class="nav-text">参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Aisaka Aoi" src="/images/manatsu.jpg"><p class="site-author-name" itemprop="name">Aisaka Aoi</p><div class="site-description" itemprop="description">逢坂葵的个人博客</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1026</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AisakaAoi" title="GitHub 👨‍💻 → https:&#x2F;&#x2F;github.com&#x2F;AisakaAoi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👨‍💻</a> </span><span class="links-of-author-item"><a href="https://github.com/Aisakaorz" title="GitHub 👩‍💻 → https:&#x2F;&#x2F;github.com&#x2F;Aisakaorz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub 👩‍💻</a> </span><span class="links-of-author-item"><a href="mailto:aisakaaoi@qq.com" title="E-Mail 📧 → mailto:aisakaaoi@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 📧</a> </span><span class="links-of-author-item"><a href="mailto:chenzongnan@m.scnu.edu.cn" title="E-Mail 🏫 → mailto:chenzongnan@m.scnu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail 🏫</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/91560309" title="Bilibili 📺 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;91560309" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 📺</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/198562921" title="Bilibili 🎮 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;198562921" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>Bilibili 🎮</a> </span><span class="links-of-author-item"><a href="https://www.youtube.com/channel/UCALvyn5Cl76GCotO9pczvjg" title="YouTube 📺 → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCALvyn5Cl76GCotO9pczvjg" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube 📺</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2026</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Aisaka Aoi</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">4m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">167:37</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll(".pdfobject-container").forEach(e=>{var t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>e+"="+encodeURIComponent(t)).join("&"),r="/lib/pdf/web/viewer.html?file="+encodeURIComponent(t)+a;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${r}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!0,notify:!0,appId:"UqjWdRYbIUEUQRXhBUUIh1QE-gzGzoHsz",appKey:"gj89JXC485PFbpdHLKVkz6dm",placeholder:"这里可以发送评论~（上面可以输入昵称、邮箱）",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/assets/haru02.model.json"},display:{position:"right",width:208,height:520},mobile:{show:!1},log:!1})</script></body></html>